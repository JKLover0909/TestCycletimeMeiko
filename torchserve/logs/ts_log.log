2025-06-05T03:50:20,631 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T03:50:20,631 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T03:50:20,635 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T03:50:20,635 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T03:50:20,673 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T03:50:20,673 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T03:50:20,889 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T03:50:20,889 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T03:50:20,900 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T03:50:20,900 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T03:50:20,916 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T03:50:20,916 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T03:50:20,962 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T03:50:20,962 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T03:50:20,962 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T03:50:20,962 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T03:50:20,963 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T03:50:20,963 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T03:50:20,963 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T03:50:20,963 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T03:50:20,963 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T03:50:20,963 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T03:50:26,673 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:59280 "GET / HTTP/1.1" 405 2
2025-06-05T03:50:26,674 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749095426
2025-06-05T03:50:27,624 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:59282 "GET /favicon.ico HTTP/1.1" 404 0
2025-06-05T03:50:27,625 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749095427
2025-06-05T03:52:57,985 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:48702 "GET /models HTTP/1.1" 200 2
2025-06-05T03:52:57,985 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749095577
2025-06-05T04:09:43,420 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:09:43,420 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:09:43,422 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:09:43,422 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:09:43,458 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:09:43,458 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:09:43,700 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:09:43,700 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:09:43,707 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:09:43,707 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:09:43,718 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T04:09:43,718 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T04:09:43,824 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:09:43,824 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:09:43,824 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:09:43,824 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:09:43,825 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:09:43,825 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:09:43,825 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:09:43,825 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:09:43,828 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T04:09:43,828 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T04:09:43,829 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:43,829 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:43,898 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:09:43,898 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:09:43,898 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:09:43,898 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:09:43,898 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:09:43,898 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:09:43,899 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:09:43,899 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:09:43,899 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T04:09:43,899 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T04:09:43,900 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:43,900 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:43,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:09:43,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:09:43,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:09:43,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:09:43,974 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:09:43,974 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:09:43,974 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:09:43,974 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:09:43,974 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T04:09:43,974 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T04:09:43,975 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:43,975 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:44,044 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:09:44,044 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:09:44,044 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:09:44,044 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:09:44,044 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:09:44,044 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:09:44,044 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:09:44,044 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:09:44,045 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T04:09:44,045 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T04:09:44,045 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:44,045 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:44,119 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:09:44,119 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:09:44,120 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:09:44,120 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:09:44,120 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:09:44,120 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:09:44,120 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:09:44,120 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:09:44,120 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T04:09:44,120 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T04:09:44,121 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:44,121 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:44,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:09:44,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:09:44,190 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:09:44,190 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:09:44,190 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:09:44,190 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:09:44,190 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:09:44,190 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:09:44,191 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:44,191 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:44,194 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:09:44,194 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:09:44,324 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,328 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - A module that was compiled using NumPy 1.x cannot be run in
2025-06-05T04:09:44,324 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,328 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
2025-06-05T04:09:44,329 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - A module that was compiled using NumPy 1.x cannot be run in
2025-06-05T04:09:44,329 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - versions of NumPy, modules must be compiled with NumPy 2.0.
2025-06-05T04:09:44,329 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
2025-06-05T04:09:44,329 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - versions of NumPy, modules must be compiled with NumPy 2.0.
2025-06-05T04:09:44,329 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
2025-06-05T04:09:44,329 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,330 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
2025-06-05T04:09:44,330 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,330 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - If you are a user of the module, the easiest solution will be to
2025-06-05T04:09:44,330 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - If you are a user of the module, the easiest solution will be to
2025-06-05T04:09:44,330 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - downgrade to 'numpy<2' or try to upgrade the affected module.
2025-06-05T04:09:44,330 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - downgrade to 'numpy<2' or try to upgrade the affected module.
2025-06-05T04:09:44,331 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - We expect that some modules will need time to support NumPy 2.
2025-06-05T04:09:44,331 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - We expect that some modules will need time to support NumPy 2.
2025-06-05T04:09:44,331 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,331 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - Traceback (most recent call last):  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 17, in <module>
2025-06-05T04:09:44,331 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,331 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - Traceback (most recent call last):  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 17, in <module>
2025-06-05T04:09:44,331 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2025-06-05T04:09:44,332 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 14, in <module>
2025-06-05T04:09:44,332 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2025-06-05T04:09:44,332 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     from ts.service import Service
2025-06-05T04:09:44,332 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 11, in <module>
2025-06-05T04:09:44,332 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 14, in <module>
2025-06-05T04:09:44,332 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-06-05T04:09:44,332 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/protocol/otf_message_handler.py", line 14, in <module>
2025-06-05T04:09:44,332 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     from ts.service import Service
2025-06-05T04:09:44,332 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     import torch
2025-06-05T04:09:44,333 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/__init__.py", line 1477, in <module>
2025-06-05T04:09:44,333 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 11, in <module>
2025-06-05T04:09:44,333 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     from .functional import *  # noqa: F403
2025-06-05T04:09:44,333 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/functional.py", line 9, in <module>
2025-06-05T04:09:44,333 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-06-05T04:09:44,333 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     import torch.nn.functional as F
2025-06-05T04:09:44,334 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/protocol/otf_message_handler.py", line 14, in <module>
2025-06-05T04:09:44,334 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     import torch
2025-06-05T04:09:44,334 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
2025-06-05T04:09:44,334 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     from .modules import *  # noqa: F403
2025-06-05T04:09:44,334 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/__init__.py", line 1477, in <module>
2025-06-05T04:09:44,334 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
2025-06-05T04:09:44,334 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     from .functional import *  # noqa: F403
2025-06-05T04:09:44,335 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/functional.py", line 9, in <module>
2025-06-05T04:09:44,335 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     from .transformer import TransformerEncoder, TransformerDecoder, \
2025-06-05T04:09:44,335 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
2025-06-05T04:09:44,335 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     import torch.nn.functional as F
2025-06-05T04:09:44,335 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -     device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,335 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
2025-06-05T04:09:44,335 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     from .modules import *  # noqa: F403
2025-06-05T04:09:44,336 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
2025-06-05T04:09:44,336 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/torch/csrc/utils/tensor_numpy.cpp:84.)
2025-06-05T04:09:44,336 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     from .transformer import TransformerEncoder, TransformerDecoder, \
2025-06-05T04:09:44,336 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,336 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
2025-06-05T04:09:44,337 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -     device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,337 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/torch/csrc/utils/tensor_numpy.cpp:84.)
2025-06-05T04:09:44,337 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,385 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,386 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - A module that was compiled using NumPy 1.x cannot be run in
2025-06-05T04:09:44,386 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
2025-06-05T04:09:44,386 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - versions of NumPy, modules must be compiled with NumPy 2.0.
2025-06-05T04:09:44,386 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
2025-06-05T04:09:44,386 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,386 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - If you are a user of the module, the easiest solution will be to
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - downgrade to 'numpy<2' or try to upgrade the affected module.
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - We expect that some modules will need time to support NumPy 2.
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - Traceback (most recent call last):  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 17, in <module>
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 14, in <module>
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     from ts.service import Service
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 11, in <module>
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/protocol/otf_message_handler.py", line 14, in <module>
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     import torch
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/__init__.py", line 1477, in <module>
2025-06-05T04:09:44,387 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     from .functional import *  # noqa: F403
2025-06-05T04:09:44,388 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/functional.py", line 9, in <module>
2025-06-05T04:09:44,388 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     import torch.nn.functional as F
2025-06-05T04:09:44,388 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
2025-06-05T04:09:44,388 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     from .modules import *  # noqa: F403
2025-06-05T04:09:44,388 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
2025-06-05T04:09:44,388 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     from .transformer import TransformerEncoder, TransformerDecoder, \
2025-06-05T04:09:44,388 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
2025-06-05T04:09:44,389 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -     device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,389 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/torch/csrc/utils/tensor_numpy.cpp:84.)
2025-06-05T04:09:44,389 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,440 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,440 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - A module that was compiled using NumPy 1.x cannot be run in
2025-06-05T04:09:44,440 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
2025-06-05T04:09:44,440 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - versions of NumPy, modules must be compiled with NumPy 2.0.
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - If you are a user of the module, the easiest solution will be to
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - downgrade to 'numpy<2' or try to upgrade the affected module.
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - We expect that some modules will need time to support NumPy 2.
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - Traceback (most recent call last):  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 17, in <module>
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 14, in <module>
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     from ts.service import Service
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 11, in <module>
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-06-05T04:09:44,441 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/protocol/otf_message_handler.py", line 14, in <module>
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     import torch
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/__init__.py", line 1477, in <module>
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     from .functional import *  # noqa: F403
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/functional.py", line 9, in <module>
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     import torch.nn.functional as F
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     from .modules import *  # noqa: F403
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     from .transformer import TransformerEncoder, TransformerDecoder, \
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -     device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/torch/csrc/utils/tensor_numpy.cpp:84.)
2025-06-05T04:09:44,442 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,573 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,573 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - A module that was compiled using NumPy 1.x cannot be run in
2025-06-05T04:09:44,573 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
2025-06-05T04:09:44,573 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - versions of NumPy, modules must be compiled with NumPy 2.0.
2025-06-05T04:09:44,574 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
2025-06-05T04:09:44,574 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,574 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - If you are a user of the module, the easiest solution will be to
2025-06-05T04:09:44,574 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - downgrade to 'numpy<2' or try to upgrade the affected module.
2025-06-05T04:09:44,574 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - We expect that some modules will need time to support NumPy 2.
2025-06-05T04:09:44,575 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,575 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - Traceback (most recent call last):  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 17, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 14, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     from ts.service import Service
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 11, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/protocol/otf_message_handler.py", line 14, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     import torch
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/__init__.py", line 1477, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     from .functional import *  # noqa: F403
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/functional.py", line 9, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     import torch.nn.functional as F
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     from .modules import *  # noqa: F403
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     from .transformer import TransformerEncoder, TransformerDecoder, \
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -     device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/torch/csrc/utils/tensor_numpy.cpp:84.)
2025-06-05T04:09:44,576 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,576 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - A module that was compiled using NumPy 1.x cannot be run in
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - versions of NumPy, modules must be compiled with NumPy 2.0.
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - If you are a user of the module, the easiest solution will be to
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - downgrade to 'numpy<2' or try to upgrade the affected module.
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - We expect that some modules will need time to support NumPy 2.
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - 
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - Traceback (most recent call last):  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 17, in <module>
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2025-06-05T04:09:44,577 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 14, in <module>
2025-06-05T04:09:44,578 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     from ts.service import Service
2025-06-05T04:09:44,578 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 11, in <module>
2025-06-05T04:09:44,578 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-06-05T04:09:44,578 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/protocol/otf_message_handler.py", line 14, in <module>
2025-06-05T04:09:44,578 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     import torch
2025-06-05T04:09:44,578 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/__init__.py", line 1477, in <module>
2025-06-05T04:09:44,578 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     from .functional import *  # noqa: F403
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/functional.py", line 9, in <module>
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     import torch.nn.functional as F
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     from .modules import *  # noqa: F403
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     from .transformer import TransformerEncoder, TransformerDecoder, \
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -     device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/torch/csrc/utils/tensor_numpy.cpp:84.)
2025-06-05T04:09:44,579 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-06-05T04:09:44,699 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=38925
2025-06-05T04:09:44,699 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:09:44,701 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=38915
2025-06-05T04:09:44,702 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:09:44,703 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:09:44,703 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]38925
2025-06-05T04:09:44,703 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:09:44,703 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:09:44,703 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,703 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,705 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:09:44,706 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]38915
2025-06-05T04:09:44,706 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:09:44,706 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,706 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:09:44,706 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,706 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:09:44,706 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:09:44,706 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:09:44,706 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:09:44,711 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:09:44,711 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:09:44,713 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584712
2025-06-05T04:09:44,713 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584712
2025-06-05T04:09:44,713 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584712
2025-06-05T04:09:44,713 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584712
2025-06-05T04:09:44,714 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584714
2025-06-05T04:09:44,714 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584714
2025-06-05T04:09:44,714 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584714
2025-06-05T04:09:44,714 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584714
2025-06-05T04:09:44,728 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=38920
2025-06-05T04:09:44,729 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:09:44,729 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:09:44,729 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]38920
2025-06-05T04:09:44,729 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:09:44,729 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:09:44,729 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,729 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,730 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:09:44,730 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:09:44,730 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=38902
2025-06-05T04:09:44,730 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:09:44,731 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:09:44,731 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584731
2025-06-05T04:09:44,731 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584731
2025-06-05T04:09:44,732 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584732
2025-06-05T04:09:44,732 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584732
2025-06-05T04:09:44,733 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:09:44,733 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]38902
2025-06-05T04:09:44,733 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:09:44,734 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,734 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:09:44,734 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,734 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:09:44,734 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:09:44,738 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584738
2025-06-05T04:09:44,738 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584738
2025-06-05T04:09:44,738 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:09:44,740 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584740
2025-06-05T04:09:44,740 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584740
2025-06-05T04:09:44,751 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:09:44,751 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:09:44,751 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:09:44,752 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:09:44,781 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:09:44,782 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:09:44,781 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:09:44,781 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:09:44,782 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:09:44,781 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:09:44,782 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:09:44,782 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:09:44,845 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=38935
2025-06-05T04:09:44,846 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:09:44,850 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:09:44,850 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]38935
2025-06-05T04:09:44,850 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:09:44,850 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:09:44,850 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,850 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,850 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:09:44,850 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:09:44,850 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=38930
2025-06-05T04:09:44,850 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:09:44,851 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:09:44,851 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584851
2025-06-05T04:09:44,851 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584851
2025-06-05T04:09:44,851 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584851
2025-06-05T04:09:44,851 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584851
2025-06-05T04:09:44,854 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:09:44,854 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]38930
2025-06-05T04:09:44,854 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:09:44,854 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:09:44,854 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,854 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:09:44,854 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:09:44,854 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:09:44,857 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:09:44,857 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584857
2025-06-05T04:09:44,857 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096584857
2025-06-05T04:09:44,857 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584857
2025-06-05T04:09:44,857 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096584857
2025-06-05T04:09:44,868 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:09:44,873 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:09:44,884 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:09:44,884 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:09:44,889 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:09:44,889 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:09:44,899 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:09:44,899 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:09:44,899 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:09:44,899 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:09:44,899 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:09:44,899 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:09:44,899 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:09:44,900 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:09:44,915 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:09:44,915 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:09:44,915 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:09:44,915 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:09:44,915 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:09:44,915 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:09:44,915 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:09:44,915 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:09:44,915 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:09:44,915 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:09:44,915 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:09:44,915 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:09:44,916 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:09:44,916 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:09:44,916 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:09:44,916 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:09:44,916 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:09:44,916 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:09:44,916 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:09:44,916 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:09:44,916 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:09:44,916 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:09:44,916 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:09:44,916 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:09:44,916 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:09:44,916 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/0f4f119b775248f6af72bad0de494bd1/onnx_handler.py", line 11, in initialize
2025-06-05T04:09:44,916 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:09:44,916 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:09:44,916 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:09:44,916 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:09:44,916 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:09:44,916 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:09:44,916 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:09:44,917 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:09:44,917 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:09:44,917 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:09:44,917 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:09:44,917 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:09:44,917 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:09:44,917 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:09:44,917 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:09:44,917 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:09:44,917 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/81883d3de4d4495db3a7abf44d5f6600/onnx_handler.py", line 11, in initialize
2025-06-05T04:09:44,917 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/96f63eab27bd4c968b17abe12d355c5d/onnx_handler.py", line 11, in initialize
2025-06-05T04:09:44,917 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,917 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,917 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,917 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,917 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/227b799c2f89462fbb022ef5ffa8e00b/onnx_handler.py", line 11, in initialize
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:09:44,917 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:09:44,917 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,917 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:09:44,917 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:09:44,917 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,917 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,918 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:09:44,918 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:09:44,918 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,918 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,917 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,917 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,917 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,918 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,918 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,917 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,929 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:09:44,929 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:09:44,929 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:09:44,929 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:09:44,929 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:09:44,929 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:09:44,929 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,929 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:09:44,929 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,929 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,929 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:09:44,929 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,929 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,929 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,929 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584929
2025-06-05T04:09:44,929 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584929
2025-06-05T04:09:44,929 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584929
2025-06-05T04:09:44,929 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,929 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584929
2025-06-05T04:09:44,929 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,929 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584929
2025-06-05T04:09:44,929 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584929
2025-06-05T04:09:44,929 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584929
2025-06-05T04:09:44,929 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584929
2025-06-05T04:09:44,930 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:09:44,930 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:09:44,930 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:09:44,930 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:09:44,930 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:09:44,930 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:09:44,931 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:09:44,931 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:09:44,942 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:09:44,942 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:09:44,942 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:09:44,942 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:09:44,944 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:09:44,944 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:09:44,945 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:09:44,945 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:09:44,945 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:09:44,945 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:09:44,946 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:09:44,946 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:09:44,946 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:09:44,946 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:09:44,947 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:09:44,947 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:09:44,947 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:09:44,947 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:09:44,952 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:09:44,952 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:09:44,956 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:09:44,956 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:09:44,956 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:09:44,956 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e73e927528004f819b824cd33084e334/onnx_handler.py", line 11, in initialize
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:09:44,957 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,957 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:09:44,957 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,957 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,957 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,957 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,957 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,958 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:09:44,958 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:09:44,958 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,958 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,958 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584958
2025-06-05T04:09:44,958 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584958
2025-06-05T04:09:44,958 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:09:44,958 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:09:44,964 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:09:44,964 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:09:44,964 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/3c9a8fa383ec4f04821a74be18f6a9eb/onnx_handler.py", line 11, in initialize
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:09:44,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:09:44,965 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,965 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:09:44,966 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,966 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:09:44,966 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,966 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:09:44,967 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:09:44,967 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:09:44,967 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,967 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:09:44,967 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584967
2025-06-05T04:09:44,967 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096584967
2025-06-05T04:09:44,967 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:09:44,967 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:09:44,968 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:09:44,968 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:09:44,968 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:09:44,968 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:09:44,976 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:09:44,976 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:09:44,976 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:09:44,976 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:09:45,931 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,931 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,931 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,931 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,931 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,931 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,931 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,931 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,958 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,958 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,968 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:45,968 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:09:46,258 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-06-05T04:09:46,258 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-06-05T04:14:10,562 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:14:10,562 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:14:10,564 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:14:10,564 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:14:10,597 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:14:10,597 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:14:10,768 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:14:10,768 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:14:10,778 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:14:10,778 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:14:10,788 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T04:14:10,788 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T04:14:10,896 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:14:10,896 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:14:10,896 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:14:10,896 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:14:10,896 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:14:10,896 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:14:10,897 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:14:10,897 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:14:10,901 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T04:14:10,901 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T04:14:10,901 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:10,901 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:10,976 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:14:10,976 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:14:10,977 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:14:10,977 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:14:10,977 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:14:10,977 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:14:10,977 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:14:10,977 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:14:10,977 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T04:14:10,977 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T04:14:10,979 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:10,979 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,054 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:14:11,054 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:14:11,054 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:14:11,054 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:14:11,054 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:14:11,054 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:14:11,054 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:14:11,054 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:14:11,055 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T04:14:11,055 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T04:14:11,055 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,055 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,128 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:14:11,128 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:14:11,129 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:14:11,129 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:14:11,129 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:14:11,129 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:14:11,129 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:14:11,129 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:14:11,129 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T04:14:11,129 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T04:14:11,130 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,130 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,203 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:14:11,203 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:14:11,204 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:14:11,204 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:14:11,204 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:14:11,204 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:14:11,204 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:14:11,204 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:14:11,204 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T04:14:11,204 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T04:14:11,205 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,205 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,305 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:14:11,305 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:14:11,306 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:14:11,306 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:14:11,306 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:14:11,306 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:14:11,307 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:14:11,307 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:14:11,308 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,308 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:11,312 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:14:11,312 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:14:11,372 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T04:14:11,372 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T04:14:11,372 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T04:14:11,372 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T04:14:11,373 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T04:14:11,373 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T04:14:11,374 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T04:14:11,374 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T04:14:11,374 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T04:14:11,374 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T04:14:11,865 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=40347
2025-06-05T04:14:11,866 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:14:11,870 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:11,870 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]40347
2025-06-05T04:14:11,871 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:11,871 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,871 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,871 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:11,874 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:11,874 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:11,881 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:14:11,883 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096851883
2025-06-05T04:14:11,883 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096851883
2025-06-05T04:14:11,884 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096851884
2025-06-05T04:14:11,884 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096851884
2025-06-05T04:14:11,899 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=40352
2025-06-05T04:14:11,899 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:14:11,902 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:11,902 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]40352
2025-06-05T04:14:11,903 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:11,903 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,903 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,903 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:11,903 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:11,903 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:11,904 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=40343
2025-06-05T04:14:11,904 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:14:11,906 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096851906
2025-06-05T04:14:11,906 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096851906
2025-06-05T04:14:11,906 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:14:11,907 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:14:11,907 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096851907
2025-06-05T04:14:11,907 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096851907
2025-06-05T04:14:11,908 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:11,908 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]40343
2025-06-05T04:14:11,908 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:11,908 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,908 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:11,908 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,908 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:11,908 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:11,911 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096851911
2025-06-05T04:14:11,911 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096851911
2025-06-05T04:14:11,911 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096851911
2025-06-05T04:14:11,911 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:14:11,911 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096851911
2025-06-05T04:14:11,923 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:14:11,927 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:14:11,943 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:11,943 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:11,945 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:11,946 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:11,949 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:11,950 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:11,982 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=40371
2025-06-05T04:14:11,982 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:14:11,986 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:11,986 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]40371
2025-06-05T04:14:11,986 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:11,986 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:11,986 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,986 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,987 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:11,987 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:11,988 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096851988
2025-06-05T04:14:11,988 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:14:11,988 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096851988
2025-06-05T04:14:11,988 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096851988
2025-06-05T04:14:11,988 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096851988
2025-06-05T04:14:11,995 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=40357
2025-06-05T04:14:11,995 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:14:11,998 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:11,999 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]40357
2025-06-05T04:14:11,999 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:11,999 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:11,999 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,999 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:11,999 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:11,999 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:12,000 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:14:12,001 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096852001
2025-06-05T04:14:12,001 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:14:12,001 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096852001
2025-06-05T04:14:12,001 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096852001
2025-06-05T04:14:12,001 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096852001
2025-06-05T04:14:12,012 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:14:12,018 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:12,018 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:12,031 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:12,032 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:12,043 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:12,043 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:12,043 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:12,043 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:12,043 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:12,043 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:12,059 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:12,059 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:12,059 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:12,059 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:12,059 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:12,060 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:12,059 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:12,060 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:12,060 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:12,060 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:12,060 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:12,060 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:12,060 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:12,060 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:12,060 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:12,060 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,060 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,061 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:12,060 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,060 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:12,060 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:12,061 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:12,060 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,060 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,061 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:12,061 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:12,060 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:12,061 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,061 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:12,061 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,061 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,061 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,061 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:12,061 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:12,061 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:12,061 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,061 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:12,062 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:12,062 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:12,062 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:12,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:12,062 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:12,062 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:12,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:12,062 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:12,062 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:12,062 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:12,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:12,063 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:12,063 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:12,063 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:12,063 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:12,063 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:12,063 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:12,063 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:12,063 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:12,063 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:12,063 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:12,063 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:12,063 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:12,063 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:12,061 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,061 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,062 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,061 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,062 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,061 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,074 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:12,074 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:12,074 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:12,074 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:12,074 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,074 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:12,074 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,074 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:12,075 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,075 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852075
2025-06-05T04:14:12,075 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,075 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852075
2025-06-05T04:14:12,075 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852075
2025-06-05T04:14:12,075 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852075
2025-06-05T04:14:12,076 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,076 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,076 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852076
2025-06-05T04:14:12,076 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852076
2025-06-05T04:14:12,076 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:14:12,076 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:14:12,076 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:14:12,076 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:14:12,077 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:14:12,077 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:14:12,086 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:12,086 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:12,088 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:12,088 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:12,088 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:12,088 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:12,088 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:12,088 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:12,088 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:12,088 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:12,088 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:12,088 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:12,088 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:12,088 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:12,096 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:12,096 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:12,099 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:12,100 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:12,100 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:12,100 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:12,100 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:12,100 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,100 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:12,100 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:12,100 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,100 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:12,100 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:12,100 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,100 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,100 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,100 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,100 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:12,100 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:12,101 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,101 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,101 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852101
2025-06-05T04:14:12,101 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852101
2025-06-05T04:14:12,101 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:14:12,101 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:14:12,107 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:12,108 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:12,108 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:12,108 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,108 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:12,108 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,109 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:12,109 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,109 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:12,109 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,109 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:12,109 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:12,109 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:12,109 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:12,109 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:12,109 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,109 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:12,109 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,109 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852109
2025-06-05T04:14:12,109 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852109
2025-06-05T04:14:12,110 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:14:12,110 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:14:12,119 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:12,119 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:12,119 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:12,119 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:12,166 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=40430
2025-06-05T04:14:12,167 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:14:12,170 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:12,170 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]40430
2025-06-05T04:14:12,170 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:12,170 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:14:12,170 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:12,170 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:12,170 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:12,170 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:12,179 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096852179
2025-06-05T04:14:12,179 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096852179
2025-06-05T04:14:12,179 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:14:12,179 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096852179
2025-06-05T04:14:12,179 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096852179
2025-06-05T04:14:12,188 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:14:12,206 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:12,207 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:12,313 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:12,313 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:12,323 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:12,323 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:12,323 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:12,323 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:12,323 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:12,323 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:12,324 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:12,324 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:12,324 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:12,324 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:12,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:12,324 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,324 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:12,324 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:12,324 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:12,325 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,325 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:12,325 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852325
2025-06-05T04:14:12,325 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749096852325
2025-06-05T04:14:12,325 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:14:12,325 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:14:12,333 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:12,333 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:12,333 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:12,333 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:13,064 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,064 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,064 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,064 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,064 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,064 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,089 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,089 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,097 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,097 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,312 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,312 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:13,784 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=40602
2025-06-05T04:14:13,785 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:14:13,788 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:13,788 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]40602
2025-06-05T04:14:13,788 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:13,788 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:13,788 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,788 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,788 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:13,788 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:13,789 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:14:13,790 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853790
2025-06-05T04:14:13,790 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853790
2025-06-05T04:14:13,790 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853790
2025-06-05T04:14:13,790 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853790
2025-06-05T04:14:13,799 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=40603
2025-06-05T04:14:13,799 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:14:13,798 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=40601
2025-06-05T04:14:13,799 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:14:13,803 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:13,803 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]40601
2025-06-05T04:14:13,804 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:13,804 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,804 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:14:13,804 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:13,804 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,804 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:13,804 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:13,805 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:13,805 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]40603
2025-06-05T04:14:13,805 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:13,805 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:13,805 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,805 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,805 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:13,805 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:13,805 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853805
2025-06-05T04:14:13,805 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853805
2025-06-05T04:14:13,805 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853805
2025-06-05T04:14:13,805 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853805
2025-06-05T04:14:13,805 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:14:13,813 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:14:13,813 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=40613
2025-06-05T04:14:13,814 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853814
2025-06-05T04:14:13,814 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:14:13,814 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853814
2025-06-05T04:14:13,814 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853814
2025-06-05T04:14:13,814 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853814
2025-06-05T04:14:13,818 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=40610
2025-06-05T04:14:13,818 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:14:13,818 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:13,818 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]40613
2025-06-05T04:14:13,818 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:13,818 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:13,818 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,818 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,819 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:13,819 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:13,822 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:13,822 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]40610
2025-06-05T04:14:13,822 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:13,822 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:13,823 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,823 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:13,823 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:13,823 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:13,823 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853823
2025-06-05T04:14:13,823 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853823
2025-06-05T04:14:13,824 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:14:13,828 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853828
2025-06-05T04:14:13,828 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853828
2025-06-05T04:14:13,828 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:13,829 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:13,829 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853829
2025-06-05T04:14:13,829 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:14:13,829 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096853829
2025-06-05T04:14:13,832 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853832
2025-06-05T04:14:13,832 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096853832
2025-06-05T04:14:13,833 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:14:13,834 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:14:13,849 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:14:13,853 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:14:13,854 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:13,854 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:13,855 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:13,856 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:13,868 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:13,869 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:13,871 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:13,872 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:13,929 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:13,929 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:13,929 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:13,929 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:13,929 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:13,930 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:13,930 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:13,930 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:13,932 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:13,932 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:13,942 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:13,942 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:13,943 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:13,943 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:13,943 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:13,943 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:13,943 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:13,943 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:13,943 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:13,943 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,943 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:13,943 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:13,943 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:13,943 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,944 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,944 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:13,944 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,944 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:13,944 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:13,944 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:13,944 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:13,944 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:13,944 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,944 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:13,944 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:13,944 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,944 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,944 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:13,944 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,944 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:13,944 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,945 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:13,945 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:13,944 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,945 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:13,945 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:13,945 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:13,945 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,945 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:13,945 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:13,945 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:13,945 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,945 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:13,945 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,945 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,945 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:13,945 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,945 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,945 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:13,945 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:13,945 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:13,945 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:13,945 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,945 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:13,945 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:13,945 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,946 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,946 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:13,946 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:14:13,946 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:14:13,946 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,946 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:14:13,946 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,946 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:13,946 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,946 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:14:13,946 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:13,946 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:13,946 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:13,946 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:13,946 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,946 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:13,946 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:13,947 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:13,947 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:13,947 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,947 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:13,947 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:13,947 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:13,947 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,947 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:13,947 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:13,947 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:13,947 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:13,947 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,947 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:13,947 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:13,947 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:13,947 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,947 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,947 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,947 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:13,947 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,947 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,948 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:13,948 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,948 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,948 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:14:13,948 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:13,948 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:14:13,948 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,948 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:13,948 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,948 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:13,948 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:13,948 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:13,948 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,948 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,948 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,948 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,948 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:13,949 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:13,949 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:14:13,949 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:13,949 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:14:13,949 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:13,949 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:13,949 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:13,951 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:13,952 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,952 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:13,952 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,952 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:13,952 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,952 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:13,952 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:13,952 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:13,953 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,953 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:13,953 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,953 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:13,953 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:14:13,953 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:14:13,958 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:13,958 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:13,958 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:13,958 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:13,959 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:13,959 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:13,960 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:13,960 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:13,960 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:13,960 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:13,961 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:13,961 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:13,965 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:13,965 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:13,965 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:13,965 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:14,086 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=40627
2025-06-05T04:14:14,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:14:14,090 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:14,090 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]40627
2025-06-05T04:14:14,090 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:14,090 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:14,091 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:14,091 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:14,091 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:14,091 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:14,092 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:14:14,092 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096854092
2025-06-05T04:14:14,092 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096854092
2025-06-05T04:14:14,092 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096854092
2025-06-05T04:14:14,092 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096854092
2025-06-05T04:14:14,100 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:14:14,117 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:14,118 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:14,197 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:14,197 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:14,211 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:14,211 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:14,211 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:14,212 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:14,212 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:14,212 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:14,212 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:14,212 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:14,212 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:14,212 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:14,212 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:14,212 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:14,213 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:14,213 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:14,213 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:14,213 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:14,213 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:14:14,213 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:14:14,222 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:14,222 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:14,222 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:14,222 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:14,947 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,947 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,947 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,947 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,949 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,949 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,949 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,949 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,953 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:14,953 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:15,214 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:15,214 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:15,656 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=40833
2025-06-05T04:14:15,657 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:14:15,663 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:15,663 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]40833
2025-06-05T04:14:15,663 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:15,664 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,664 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,664 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:15,664 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:15,664 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:15,666 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:14:15,666 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855666
2025-06-05T04:14:15,666 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855666
2025-06-05T04:14:15,666 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855666
2025-06-05T04:14:15,666 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855666
2025-06-05T04:14:15,669 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=40826
2025-06-05T04:14:15,669 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:14:15,669 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=40827
2025-06-05T04:14:15,670 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:14:15,674 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:15,675 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]40826
2025-06-05T04:14:15,675 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,675 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:15,675 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,676 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:15,676 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:15,676 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:15,677 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:15,677 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]40827
2025-06-05T04:14:15,677 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:15,677 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,677 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:15,677 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,678 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:15,678 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:15,678 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:14:15,678 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855678
2025-06-05T04:14:15,678 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855678
2025-06-05T04:14:15,679 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855679
2025-06-05T04:14:15,679 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855679
2025-06-05T04:14:15,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=40832
2025-06-05T04:14:15,686 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=40838
2025-06-05T04:14:15,686 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:14:15,686 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:15,686 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:14:15,686 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]40832
2025-06-05T04:14:15,687 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:14:15,687 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:15,687 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:15,687 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,687 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,687 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:15,687 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:15,687 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:15,687 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:14:15,687 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855687
2025-06-05T04:14:15,687 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]40838
2025-06-05T04:14:15,687 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855687
2025-06-05T04:14:15,687 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:15,687 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:15,687 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855687
2025-06-05T04:14:15,687 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,687 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855687
2025-06-05T04:14:15,687 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,688 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:15,688 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:15,696 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:14:15,696 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:14:15,697 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855697
2025-06-05T04:14:15,697 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855697
2025-06-05T04:14:15,697 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855697
2025-06-05T04:14:15,697 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855697
2025-06-05T04:14:15,697 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855697
2025-06-05T04:14:15,697 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855697
2025-06-05T04:14:15,697 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855697
2025-06-05T04:14:15,697 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855697
2025-06-05T04:14:15,697 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:14:15,716 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:15,717 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:14:15,717 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:14:15,717 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:15,717 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:14:15,722 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:15,722 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:15,735 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:15,735 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:15,735 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:15,736 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:15,736 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:15,736 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:15,827 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:15,827 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:15,827 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:15,827 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:15,827 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:15,827 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:15,827 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:15,827 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:15,827 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:15,827 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:15,839 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:15,839 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:15,839 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:15,839 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:15,839 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:15,839 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:15,839 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:15,839 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:15,839 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:15,839 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:15,840 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:15,840 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:15,840 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:15,840 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:15,840 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:15,840 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:15,840 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:15,840 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,840 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,840 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,840 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:15,840 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:15,840 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:15,841 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:15,840 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,841 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:15,840 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,841 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:15,841 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:15,841 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:15,841 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:15,841 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,840 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,841 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:15,841 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,841 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,841 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,841 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,841 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:15,841 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:15,841 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,841 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:15,841 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:15,841 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:15,841 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,841 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,841 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,841 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,841 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:15,841 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:15,841 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:15,841 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,841 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:15,841 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,841 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:15,841 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:15,841 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,841 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:15,841 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,842 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,842 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,842 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:15,841 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,841 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:15,842 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:15,841 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,842 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,842 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:15,842 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:15,842 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:15,842 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:15,842 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-06-05T04:14:15,842 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:15,842 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:15,842 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:15,842 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-06-05T04:14:15,842 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:15,842 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,842 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,842 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,842 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:15,842 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,842 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,842 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:15,843 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2025-06-05T04:14:15,843 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:15,843 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2025-06-05T04:14:15,843 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,843 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:15,843 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:15,843 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,843 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:15,843 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,843 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,843 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,844 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:15,844 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:15,844 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,844 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,844 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,844 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,844 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:15,844 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2025-06-05T04:14:15,844 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2025-06-05T04:14:15,844 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:15,845 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:15,845 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:15,845 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:15,845 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,845 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:15,845 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,845 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:15,845 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:15,845 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:15,845 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,845 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:15,846 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,846 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:15,846 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2025-06-05T04:14:15,846 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2025-06-05T04:14:15,853 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:15,853 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:15,853 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:15,853 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:15,854 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:15,854 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:15,854 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:15,854 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:15,855 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:15,855 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:15,855 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:15,855 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:15,856 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:15,856 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:15,856 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:15,856 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:15,857 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:15,857 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:15,857 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:15,857 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:15,971 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=40841
2025-06-05T04:14:15,971 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:14:15,974 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:15,974 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]40841
2025-06-05T04:14:15,974 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:15,974 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:15,974 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,974 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:15,975 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:15,975 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:15,975 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:14:15,975 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855975
2025-06-05T04:14:15,975 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096855975
2025-06-05T04:14:15,975 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855975
2025-06-05T04:14:15,975 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096855975
2025-06-05T04:14:15,986 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:14:16,000 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:16,001 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:16,064 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:16,064 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:16,075 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:16,075 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:16,075 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:16,075 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:16,075 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:16,076 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:16,076 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:16,076 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:16,076 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:16,076 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:16,076 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:16,076 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:16,076 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:16,076 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2025-06-05T04:14:16,076 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2025-06-05T04:14:16,083 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:16,083 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:16,083 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:16,083 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:17,850 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,850 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,851 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,851 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,851 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,851 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,853 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,854 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,853 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:17,854 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:18,084 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:18,084 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:18,526 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=41037
2025-06-05T04:14:18,526 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:14:18,532 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:18,532 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]41037
2025-06-05T04:14:18,532 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,532 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:18,532 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:18,532 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,533 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:18,533 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:18,537 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:14:18,538 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858538
2025-06-05T04:14:18,538 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858538
2025-06-05T04:14:18,538 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858538
2025-06-05T04:14:18,538 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858538
2025-06-05T04:14:18,562 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:14:18,563 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=41040
2025-06-05T04:14:18,563 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:14:18,569 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:18,570 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]41040
2025-06-05T04:14:18,570 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:18,570 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,570 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:18,570 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,570 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:18,570 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:18,571 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:14:18,571 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858571
2025-06-05T04:14:18,571 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858571
2025-06-05T04:14:18,571 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858571
2025-06-05T04:14:18,571 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858571
2025-06-05T04:14:18,587 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:18,588 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:18,592 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:14:18,604 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=41038
2025-06-05T04:14:18,604 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:14:18,608 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:18,608 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]41038
2025-06-05T04:14:18,608 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:18,608 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:18,608 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,608 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,608 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:18,608 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:18,609 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:14:18,609 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858609
2025-06-05T04:14:18,609 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858609
2025-06-05T04:14:18,609 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858609
2025-06-05T04:14:18,609 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858609
2025-06-05T04:14:18,615 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:18,616 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:18,618 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=41036
2025-06-05T04:14:18,618 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:14:18,624 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:18,624 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=41041
2025-06-05T04:14:18,624 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]41036
2025-06-05T04:14:18,624 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:14:18,624 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:18,624 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:18,625 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,625 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,625 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:18,625 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:18,625 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:14:18,626 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:18,626 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]41041
2025-06-05T04:14:18,626 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:18,626 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:18,626 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,626 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,627 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:18,627 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:18,627 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:14:18,627 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858627
2025-06-05T04:14:18,627 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858627
2025-06-05T04:14:18,627 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858627
2025-06-05T04:14:18,627 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858627
2025-06-05T04:14:18,641 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858641
2025-06-05T04:14:18,641 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858641
2025-06-05T04:14:18,641 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:14:18,642 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858642
2025-06-05T04:14:18,642 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858642
2025-06-05T04:14:18,649 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:14:18,653 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:14:18,653 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:18,653 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:18,665 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:18,665 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:18,679 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:18,679 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:18,709 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:18,709 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:18,709 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:18,709 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:18,723 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:18,724 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:18,724 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:18,724 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:18,724 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:18,724 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,724 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,725 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,725 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:18,725 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,725 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:18,725 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,725 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:18,725 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:18,725 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:18,725 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:18,725 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,725 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,725 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:18,725 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,726 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2025-06-05T04:14:18,726 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2025-06-05T04:14:18,727 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:18,727 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:18,727 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,727 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,727 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,727 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,727 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:18,727 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:18,727 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,727 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,727 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,727 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,728 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2025-06-05T04:14:18,728 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2025-06-05T04:14:18,736 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:18,736 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:18,737 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:18,737 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:18,737 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:18,738 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:18,737 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:18,738 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:18,739 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:18,739 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:18,740 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:18,740 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,740 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,740 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,740 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,740 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,740 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,740 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:18,740 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:18,741 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,741 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,741 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,741 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,741 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-06-05T04:14:18,741 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-06-05T04:14:18,749 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:18,749 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:18,749 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:18,749 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:18,749 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:18,749 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:18,751 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:18,752 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:18,760 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:18,760 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:18,761 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:18,761 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,761 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,761 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,761 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,761 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,761 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,762 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:18,762 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:18,762 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,762 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,762 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,762 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,762 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2025-06-05T04:14:18,762 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2025-06-05T04:14:18,762 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:18,763 [INFO ] epollEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,763 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:18,763 [INFO ] epollEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,763 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,763 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,763 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,763 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,763 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:18,763 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:18,763 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,763 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,763 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,763 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,764 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2025-06-05T04:14:18,764 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2025-06-05T04:14:18,770 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:18,770 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:18,770 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:18,770 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:18,771 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:18,771 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:18,771 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:18,771 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:18,877 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=41051
2025-06-05T04:14:18,877 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:14:18,880 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:18,881 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]41051
2025-06-05T04:14:18,881 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:18,881 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:18,881 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,881 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:18,881 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:18,881 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:18,881 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:14:18,882 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858882
2025-06-05T04:14:18,882 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096858882
2025-06-05T04:14:18,882 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858882
2025-06-05T04:14:18,882 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096858882
2025-06-05T04:14:18,886 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:14:18,902 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:18,902 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:18,965 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:18,965 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:18,976 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:18,976 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:18,977 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:18,977 [INFO ] epollEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,977 [INFO ] epollEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:18,977 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,977 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:18,977 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,977 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:18,977 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:18,977 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:18,977 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,977 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:18,977 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,977 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:18,978 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2025-06-05T04:14:18,978 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2025-06-05T04:14:18,985 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:18,985 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:18,985 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:18,985 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:21,728 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,728 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,729 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,729 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,741 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,741 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,763 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,763 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,764 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,764 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,978 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:21,978 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:33,742 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=41264
2025-06-05T04:14:33,743 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:14:33,750 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:33,750 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]41264
2025-06-05T04:14:33,750 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,750 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:33,750 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,750 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:33,750 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:33,751 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:33,751 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:14:33,756 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873756
2025-06-05T04:14:33,756 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873756
2025-06-05T04:14:33,756 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873756
2025-06-05T04:14:33,756 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873756
2025-06-05T04:14:33,768 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:14:33,771 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=41259
2025-06-05T04:14:33,772 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:14:33,775 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:33,776 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]41259
2025-06-05T04:14:33,776 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:33,776 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:33,776 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,776 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,776 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:33,776 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:33,779 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:14:33,779 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873779
2025-06-05T04:14:33,779 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873779
2025-06-05T04:14:33,779 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873779
2025-06-05T04:14:33,779 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873779
2025-06-05T04:14:33,787 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:33,787 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:33,798 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:14:33,804 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=41258
2025-06-05T04:14:33,805 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:14:33,808 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=41270
2025-06-05T04:14:33,809 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:14:33,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=41267
2025-06-05T04:14:33,812 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:14:33,812 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:33,812 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]41258
2025-06-05T04:14:33,812 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:33,812 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:33,812 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,812 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,812 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:33,812 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:33,812 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:33,812 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]41270
2025-06-05T04:14:33,812 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:33,813 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:33,813 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,813 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,813 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:33,813 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:33,813 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:14:33,813 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873813
2025-06-05T04:14:33,813 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873813
2025-06-05T04:14:33,814 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873814
2025-06-05T04:14:33,814 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873814
2025-06-05T04:14:33,814 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873814
2025-06-05T04:14:33,814 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:14:33,814 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873814
2025-06-05T04:14:33,814 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873814
2025-06-05T04:14:33,814 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873814
2025-06-05T04:14:33,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:33,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]41267
2025-06-05T04:14:33,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:33,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:33,816 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,816 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:33,816 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:33,816 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:33,816 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:33,816 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:33,821 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:14:33,821 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873821
2025-06-05T04:14:33,821 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096873821
2025-06-05T04:14:33,821 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873821
2025-06-05T04:14:33,821 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096873821
2025-06-05T04:14:33,829 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:14:33,830 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:14:33,833 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:14:33,847 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:33,848 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:33,848 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:33,848 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:33,851 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:33,851 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:33,897 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:33,897 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:33,897 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:33,897 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:33,910 [INFO ] epollEventLoopGroup-5-25 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:33,910 [INFO ] epollEventLoopGroup-5-25 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:33,910 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:33,910 [INFO ] epollEventLoopGroup-5-26 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,910 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:33,911 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:33,910 [INFO ] epollEventLoopGroup-5-26 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,911 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,911 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,911 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:33,911 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,911 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,911 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:33,911 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:33,911 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:33,911 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:33,911 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:33,911 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:33,911 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:33,911 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:33,911 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:33,911 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:33,911 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:33,911 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:33,911 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:33,911 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:33,911 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:33,911 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:33,911 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:33,911 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:33,911 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:33,912 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2025-06-05T04:14:33,912 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-06-05T04:14:33,912 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2025-06-05T04:14:33,912 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-06-05T04:14:33,914 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:33,914 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:33,914 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:33,914 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:33,914 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:33,915 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:33,922 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:33,922 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:33,922 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:33,922 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:33,923 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:33,923 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:33,923 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:33,923 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:33,926 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:33,926 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:33,926 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:33,926 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:33,926 [INFO ] epollEventLoopGroup-5-29 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,926 [INFO ] epollEventLoopGroup-5-28 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,927 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:33,926 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:33,926 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:33,927 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:33,926 [INFO ] epollEventLoopGroup-5-28 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,926 [INFO ] epollEventLoopGroup-5-29 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,927 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:33,927 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:33,927 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:33,927 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:33,927 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,927 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:33,927 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,927 [INFO ] epollEventLoopGroup-5-27 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,927 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,927 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:33,927 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,927 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:33,928 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:33,927 [INFO ] epollEventLoopGroup-5-27 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:33,928 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:33,928 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,927 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:33,928 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:22,632 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:22,632 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:22,633 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:33,928 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:33,928 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:33,928 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:22,633 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:22,633 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:33,927 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:22,633 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:22,633 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:22,633 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:33,928 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:22,633 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:22,633 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:22,633 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:22,633 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:22,633 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:22,633 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:22,633 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:22,633 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:22,633 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:22,633 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:22,633 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:22,633 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:22,633 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:22,633 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:22,633 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:22,633 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:22,633 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2025-06-05T04:14:22,633 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2025-06-05T04:14:22,633 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2025-06-05T04:14:22,633 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2025-06-05T04:14:22,634 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2025-06-05T04:14:22,634 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2025-06-05T04:14:22,646 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:22,633 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:22,646 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:22,633 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:22,646 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:22,646 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:22,647 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:22,647 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:22,647 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:22,647 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:22,646 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:22,647 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:22,647 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:22,647 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:22,647 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:22,646 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:22,648 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:22,648 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:22,777 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=41273
2025-06-05T04:14:22,777 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:14:22,781 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:22,781 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]41273
2025-06-05T04:14:22,781 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:22,781 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:22,781 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:22,781 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:22,781 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:22,781 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:22,782 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:14:22,782 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096862782
2025-06-05T04:14:22,782 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096862782
2025-06-05T04:14:22,782 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096862782
2025-06-05T04:14:22,782 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096862782
2025-06-05T04:14:22,787 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:14:22,801 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:22,802 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:22,862 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:22,862 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:22,874 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:22,874 [INFO ] epollEventLoopGroup-5-30 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:22,874 [INFO ] epollEventLoopGroup-5-30 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:22,875 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:22,875 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:22,875 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:22,875 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:22,875 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:22,875 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:22,875 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:22,875 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:22,875 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:22,875 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:22,875 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2025-06-05T04:14:22,875 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2025-06-05T04:14:22,887 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:22,887 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:22,887 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:22,887 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:38,912 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:38,912 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:38,912 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:38,912 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:27,656 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:27,656 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:27,656 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:27,656 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:27,656 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:27,656 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:27,898 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:27,898 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:28,367 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=41500
2025-06-05T04:14:28,367 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:14:28,371 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:28,371 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]41500
2025-06-05T04:14:28,371 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:28,371 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:28,371 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,371 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,371 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:28,371 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:28,372 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:14:28,372 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868372
2025-06-05T04:14:28,372 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868372
2025-06-05T04:14:28,372 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868372
2025-06-05T04:14:28,372 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868372
2025-06-05T04:14:28,382 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=41499
2025-06-05T04:14:28,382 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:14:28,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:14:28,386 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:28,386 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]41499
2025-06-05T04:14:28,386 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:28,386 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:28,386 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,386 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,386 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:28,386 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:28,391 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:14:28,391 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868391
2025-06-05T04:14:28,391 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868391
2025-06-05T04:14:28,391 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868391
2025-06-05T04:14:28,391 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868391
2025-06-05T04:14:28,392 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=41506
2025-06-05T04:14:28,392 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:14:28,397 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:28,397 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]41506
2025-06-05T04:14:28,397 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:28,397 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:28,397 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,397 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,397 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:28,397 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:28,400 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868400
2025-06-05T04:14:28,400 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:14:28,400 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868400
2025-06-05T04:14:28,400 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868400
2025-06-05T04:14:28,400 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868400
2025-06-05T04:14:28,402 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:14:28,409 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:28,409 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:28,411 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:14:28,420 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:28,420 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:28,428 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:28,428 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:28,469 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=41505
2025-06-05T04:14:28,469 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:14:28,473 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:28,473 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]41505
2025-06-05T04:14:28,474 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:28,474 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:28,474 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,474 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,474 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:28,474 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:28,476 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868476
2025-06-05T04:14:28,476 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:14:28,476 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868476
2025-06-05T04:14:28,476 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868476
2025-06-05T04:14:28,476 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868476
2025-06-05T04:14:28,485 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=41507
2025-06-05T04:14:28,485 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:14:28,488 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:14:28,489 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:28,490 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]41507
2025-06-05T04:14:28,490 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:28,490 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,490 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:28,490 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,490 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:28,490 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:28,491 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:14:28,491 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868491
2025-06-05T04:14:28,491 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868491
2025-06-05T04:14:28,491 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868491
2025-06-05T04:14:28,491 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868491
2025-06-05T04:14:28,498 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:28,498 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:28,499 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:28,499 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:28,499 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:28,499 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:28,502 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:14:28,511 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:28,511 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:28,512 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:28,512 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:28,512 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:28,512 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:28,512 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:28,512 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:28,512 [INFO ] epollEventLoopGroup-5-33 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,512 [INFO ] epollEventLoopGroup-5-31 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,512 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:28,512 [INFO ] epollEventLoopGroup-5-33 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,513 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:28,513 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:28,512 [INFO ] epollEventLoopGroup-5-31 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:28,513 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:28,513 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,513 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,513 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,513 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:28,513 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:28,513 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:28,513 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:28,513 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:28,513 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,513 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:28,513 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,513 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:28,513 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,513 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:28,513 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:28,513 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:28,513 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:28,513 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,514 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,514 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,513 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:28,513 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:28,514 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,514 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,514 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,514 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,514 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2025-06-05T04:14:28,514 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2025-06-05T04:14:28,514 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2025-06-05T04:14:28,514 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:28,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:28,516 [INFO ] epollEventLoopGroup-5-32 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,516 [INFO ] epollEventLoopGroup-5-32 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,517 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,517 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,517 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,517 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,517 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:28,517 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:28,517 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,517 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,517 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,517 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,517 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-06-05T04:14:28,517 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-06-05T04:14:28,520 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:28,520 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:28,523 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:28,523 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:28,523 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:28,523 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:28,524 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:28,524 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:28,524 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:28,524 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:28,527 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:28,527 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:28,527 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:28,527 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:28,584 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:28,584 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:28,584 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:28,584 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:28,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:28,595 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:28,596 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:28,596 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:28,596 [INFO ] epollEventLoopGroup-5-34 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,596 [INFO ] epollEventLoopGroup-5-35 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,596 [INFO ] epollEventLoopGroup-5-34 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,596 [INFO ] epollEventLoopGroup-5-35 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,596 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,596 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,596 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,596 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,596 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,597 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,596 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,597 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,597 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:28,597 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:28,597 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,597 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,597 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,597 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,597 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:28,597 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:28,597 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,597 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,597 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,597 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,597 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2025-06-05T04:14:28,597 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2025-06-05T04:14:28,597 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2025-06-05T04:14:28,597 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2025-06-05T04:14:28,606 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:28,606 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:28,606 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:28,606 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:28,607 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:28,607 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:28,607 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:28,607 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:28,659 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=41515
2025-06-05T04:14:28,659 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:14:28,663 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:28,663 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]41515
2025-06-05T04:14:28,663 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:28,663 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:28,663 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,663 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:28,663 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:28,663 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:28,664 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:14:28,664 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868664
2025-06-05T04:14:28,664 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096868664
2025-06-05T04:14:28,664 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868664
2025-06-05T04:14:28,664 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096868664
2025-06-05T04:14:28,669 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:14:28,683 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:28,683 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:28,747 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:28,747 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:28,758 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:28,758 [INFO ] epollEventLoopGroup-5-36 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,758 [INFO ] epollEventLoopGroup-5-36 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:28,758 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,758 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:28,758 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,758 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:28,759 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:28,759 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:28,759 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,759 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:28,759 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,759 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:28,759 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2025-06-05T04:14:28,759 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2025-06-05T04:14:28,767 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:28,767 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:28,767 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:28,767 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:36,495 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,495 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,495 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,495 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,497 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,497 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,577 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,577 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,577 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,577 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,739 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:36,739 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:37,241 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=41754
2025-06-05T04:14:37,241 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:14:37,244 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:37,244 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]41754
2025-06-05T04:14:37,244 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:37,244 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:37,245 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:37,245 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:37,245 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=41753
2025-06-05T04:14:37,245 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:37,245 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:14:37,245 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:37,246 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:14:37,247 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096877247
2025-06-05T04:14:37,247 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096877247
2025-06-05T04:14:37,247 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096877247
2025-06-05T04:14:37,247 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096877247
2025-06-05T04:14:37,249 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:37,249 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]41753
2025-06-05T04:14:37,249 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:37,249 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:37,249 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:37,249 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:37,250 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:37,250 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:37,253 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:14:37,253 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096877253
2025-06-05T04:14:37,253 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096877253
2025-06-05T04:14:37,253 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096877253
2025-06-05T04:14:37,253 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096877253
2025-06-05T04:14:37,260 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:14:37,267 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:14:37,280 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:37,280 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:37,286 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:37,286 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:37,286 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=41752
2025-06-05T04:14:37,287 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:14:37,294 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:37,294 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]41752
2025-06-05T04:14:37,294 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:37,294 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:37,294 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:37,294 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:37,294 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:37,294 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:37,295 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:14:37,295 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096877295
2025-06-05T04:14:37,295 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096877295
2025-06-05T04:14:37,296 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096877296
2025-06-05T04:14:37,296 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096877296
2025-06-05T04:14:37,315 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:14:37,335 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:37,335 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:37,364 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=41761
2025-06-05T04:14:37,364 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:14:37,368 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:37,368 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]41761
2025-06-05T04:14:37,368 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:37,368 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:37,368 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:37,368 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:37,368 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:37,368 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:37,369 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:14:37,369 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096877369
2025-06-05T04:14:37,369 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096877369
2025-06-05T04:14:37,369 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096877369
2025-06-05T04:14:37,369 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096877369
2025-06-05T04:14:37,375 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:37,375 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:37,375 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:37,375 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:37,387 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:14:37,391 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:37,391 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:37,392 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:37,392 [INFO ] epollEventLoopGroup-5-37 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:37,392 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:37,392 [INFO ] epollEventLoopGroup-5-37 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:37,392 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:37,392 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:37,392 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:37,392 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:37,392 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:37,392 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:37,392 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:37,392 [INFO ] epollEventLoopGroup-5-38 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:37,392 [INFO ] epollEventLoopGroup-5-38 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:37,392 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:37,392 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:37,393 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:37,393 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:37,393 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:37,393 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:37,393 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:37,393 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:37,393 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:37,393 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:37,393 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:37,393 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:37,393 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:37,393 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:37,393 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:37,393 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:37,393 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:37,393 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:37,394 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-06-05T04:14:37,394 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2025-06-05T04:14:37,394 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-06-05T04:14:37,394 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2025-06-05T04:14:37,398 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:37,398 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:37,402 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:37,402 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:37,402 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:37,402 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:37,403 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:37,403 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:37,403 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:37,403 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:37,406 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:37,407 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:37,410 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:48,705 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:48,705 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:48,705 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:48,705 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:48,705 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:48,705 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:48,705 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:48,705 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:48,705 [INFO ] epollEventLoopGroup-5-39 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:48,705 [INFO ] epollEventLoopGroup-5-39 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:48,706 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:48,706 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:48,706 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:48,706 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:48,706 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:48,706 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:48,706 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:48,706 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:48,706 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:48,706 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:48,706 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2025-06-05T04:14:48,706 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2025-06-05T04:14:48,709 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=41762
2025-06-05T04:14:48,709 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:14:48,713 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:48,713 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]41762
2025-06-05T04:14:48,713 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:48,713 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:48,713 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:48,713 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:48,713 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:48,713 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:48,714 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:14:48,714 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096888714
2025-06-05T04:14:48,714 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096888714
2025-06-05T04:14:48,715 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096888715
2025-06-05T04:14:48,715 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096888715
2025-06-05T04:14:48,715 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:48,715 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:48,715 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:48,715 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:48,730 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:14:48,746 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:48,746 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:48,792 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=41767
2025-06-05T04:14:48,792 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:14:48,796 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:48,796 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]41767
2025-06-05T04:14:48,796 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:48,796 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:48,796 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:48,796 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:48,796 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:48,796 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:48,797 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:14:48,797 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096888797
2025-06-05T04:14:48,797 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096888797
2025-06-05T04:14:48,797 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096888797
2025-06-05T04:14:48,797 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096888797
2025-06-05T04:14:48,797 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:14:48,805 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:48,805 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:48,805 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:48,805 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:48,811 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:48,811 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:48,816 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:48,816 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:48,816 [INFO ] epollEventLoopGroup-5-41 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:48,816 [INFO ] epollEventLoopGroup-5-41 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:48,816 [INFO ] epollEventLoopGroup-5-40 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:48,817 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:48,817 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:48,816 [INFO ] epollEventLoopGroup-5-40 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:48,817 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:48,817 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:48,817 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:48,817 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:48,817 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:48,817 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:48,817 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:48,817 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:48,817 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:48,817 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:48,817 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:48,817 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:48,817 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:48,817 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:48,817 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:48,817 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:48,817 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:48,817 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:48,817 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2025-06-05T04:14:48,817 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2025-06-05T04:14:48,817 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2025-06-05T04:14:48,817 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2025-06-05T04:14:48,825 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:48,825 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:48,825 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:48,825 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:48,826 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:48,826 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:48,826 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:48,826 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:48,890 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:48,891 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:48,901 [INFO ] epollEventLoopGroup-5-42 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:48,901 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:48,902 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:48,901 [INFO ] epollEventLoopGroup-5-42 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:48,902 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:48,902 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:48,902 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:48,902 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:48,902 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:48,902 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:48,902 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:48,902 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:48,902 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:48,902 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:48,902 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2025-06-05T04:14:48,902 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2025-06-05T04:14:48,909 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:48,909 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:48,909 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:48,909 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:50,394 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,394 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,394 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,394 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,412 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,412 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,523 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,523 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,523 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,523 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,608 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:50,608 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:14:51,174 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=42020
2025-06-05T04:14:51,175 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:14:51,178 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:51,178 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]42020
2025-06-05T04:14:51,179 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:51,179 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,179 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,179 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:51,179 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:51,179 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:14:51,179 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:14:51,180 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891180
2025-06-05T04:14:51,180 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891180
2025-06-05T04:14:51,180 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891180
2025-06-05T04:14:51,180 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891180
2025-06-05T04:14:51,180 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:14:51,196 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:51,196 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:51,214 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=42014
2025-06-05T04:14:51,214 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:14:51,217 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:51,218 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]42014
2025-06-05T04:14:51,218 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:51,218 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:51,218 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,218 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,218 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:51,218 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:14:51,219 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:14:51,219 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891219
2025-06-05T04:14:51,219 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891219
2025-06-05T04:14:51,219 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891219
2025-06-05T04:14:51,219 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891219
2025-06-05T04:14:51,220 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:14:51,227 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=42015
2025-06-05T04:14:51,227 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:14:51,231 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:51,231 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]42015
2025-06-05T04:14:51,231 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:51,231 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:51,231 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,231 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,231 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:51,231 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:14:51,232 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:14:51,232 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891232
2025-06-05T04:14:51,232 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891232
2025-06-05T04:14:51,232 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891232
2025-06-05T04:14:51,232 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891232
2025-06-05T04:14:51,233 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:14:51,236 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:51,236 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:51,248 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:51,249 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:51,308 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:51,308 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:51,308 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:51,308 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:51,308 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:51,308 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:51,321 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:51,321 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:51,321 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:51,321 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:51,321 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:51,321 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:51,321 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:51,321 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:51,321 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:51,321 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:51,321 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:51,321 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:51,321 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:51,321 [INFO ] epollEventLoopGroup-5-43 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,321 [INFO ] epollEventLoopGroup-5-44 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:51,322 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:51,321 [INFO ] epollEventLoopGroup-5-44 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:51,322 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:51,322 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,321 [INFO ] epollEventLoopGroup-5-43 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,322 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,322 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,322 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:51,322 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:51,322 [INFO ] epollEventLoopGroup-5-45 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:51,322 [INFO ] epollEventLoopGroup-5-45 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,322 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:51,322 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:51,322 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,322 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,322 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,322 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,322 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,322 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,322 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:51,322 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:51,322 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:51,322 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:51,322 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:14:51,322 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:51,322 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,322 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:14:51,322 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,322 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,322 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:51,322 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,322 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:51,322 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:51,322 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,322 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,322 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,322 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:51,322 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:51,322 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:14:51,322 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,322 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,322 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,322 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:51,322 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,323 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:51,322 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-06-05T04:14:51,323 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:51,322 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,323 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:51,323 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:51,323 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-06-05T04:14:51,323 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:51,324 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2025-06-05T04:14:51,323 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:51,324 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2025-06-05T04:14:51,324 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:51,324 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:51,324 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:51,324 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:51,324 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:51,324 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:14:51,324 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:14:51,334 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:51,334 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:14:51,335 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:51,335 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:14:51,335 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:51,335 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:14:51,336 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:51,336 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:14:51,340 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=42023
2025-06-05T04:14:51,340 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:14:51,344 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:51,344 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]42023
2025-06-05T04:14:51,344 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:51,344 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:51,344 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,344 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,344 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:51,344 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:14:51,345 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:14:51,345 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891345
2025-06-05T04:14:51,345 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891345
2025-06-05T04:14:51,345 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891345
2025-06-05T04:14:51,345 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891345
2025-06-05T04:14:51,345 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:14:51,353 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=42024
2025-06-05T04:14:51,353 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:14:51,357 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:51,357 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]42024
2025-06-05T04:14:51,357 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:51,357 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:51,357 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,357 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,357 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:51,357 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:14:51,358 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:14:51,358 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891358
2025-06-05T04:14:51,358 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891358
2025-06-05T04:14:51,358 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891358
2025-06-05T04:14:51,358 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891358
2025-06-05T04:14:51,358 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:14:51,360 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:51,360 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:51,373 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:51,373 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:51,439 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=42029
2025-06-05T04:14:51,439 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:14:51,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:14:51,443 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]42029
2025-06-05T04:14:51,443 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:14:51,443 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:14:51,443 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,443 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:14:51,443 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:51,443 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:14:51,443 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:51,443 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:51,443 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:14:51,443 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:51,443 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:51,444 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891444
2025-06-05T04:14:51,444 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096891444
2025-06-05T04:14:51,444 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891444
2025-06-05T04:14:51,444 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096891444
2025-06-05T04:14:51,444 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:14:51,454 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:51,454 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:51,454 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:51,454 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:51,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:51,455 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:51,455 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:51,455 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:51,455 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:51,455 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:51,455 [INFO ] epollEventLoopGroup-5-46 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,455 [INFO ] epollEventLoopGroup-5-46 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,455 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,455 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,455 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,455 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,456 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:51,456 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:14:51,456 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,456 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,456 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,456 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,456 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2025-06-05T04:14:51,456 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2025-06-05T04:14:51,457 [INFO ] epollEventLoopGroup-5-47 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,457 [INFO ] epollEventLoopGroup-5-47 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,457 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,457 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,457 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,457 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,457 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:51,457 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:14:51,458 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,458 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,458 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,458 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,458 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2025-06-05T04:14:51,458 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2025-06-05T04:14:51,460 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:14:51,460 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:14:51,464 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:51,464 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:51,464 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:14:51,464 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:14:51,465 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:51,465 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:51,465 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:14:51,465 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:14:51,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:14:51,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:14:51,561 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:14:51,561 [INFO ] epollEventLoopGroup-5-48 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,561 [INFO ] epollEventLoopGroup-5-48 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:14:51,561 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,561 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:14:51,561 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,561 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:14:51,562 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:51,562 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:14:51,562 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,562 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:14:51,562 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,562 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:14:51,562 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2025-06-05T04:14:51,562 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2025-06-05T04:14:51,569 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:14:51,569 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:51,569 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:14:51,569 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:15:12,318 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,318 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,318 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,318 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,319 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,319 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,452 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,452 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,453 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:12,453 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:23,858 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:23,858 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:13,148 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=42302
2025-06-05T04:15:13,148 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:15:13,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=42303
2025-06-05T04:15:13,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:15:13,151 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:13,151 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]42302
2025-06-05T04:15:13,151 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:13,152 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:13,152 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,152 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,152 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:15:13,152 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:15:13,153 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:15:13,153 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913153
2025-06-05T04:15:13,153 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913153
2025-06-05T04:15:13,153 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913153
2025-06-05T04:15:13,153 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913153
2025-06-05T04:15:13,154 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:15:13,154 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:13,154 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]42303
2025-06-05T04:15:13,154 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:13,154 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:13,154 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,154 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,154 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:15:13,154 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:15:13,155 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:15:13,155 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913155
2025-06-05T04:15:13,155 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913155
2025-06-05T04:15:13,155 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913155
2025-06-05T04:15:13,155 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913155
2025-06-05T04:15:13,155 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:15:13,170 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:13,170 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:13,172 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:13,172 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:13,174 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=42305
2025-06-05T04:15:13,174 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:15:13,178 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:13,178 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]42305
2025-06-05T04:15:13,178 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:13,178 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:13,178 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,178 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,178 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:15:13,178 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:15:13,179 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:15:13,179 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913179
2025-06-05T04:15:13,179 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913179
2025-06-05T04:15:13,180 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913180
2025-06-05T04:15:13,180 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913180
2025-06-05T04:15:13,180 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:15:13,200 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:13,200 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:13,261 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:13,261 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:13,261 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:13,261 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:13,261 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:13,261 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:13,274 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:13,274 [INFO ] epollEventLoopGroup-5-51 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,274 [INFO ] epollEventLoopGroup-5-51 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,274 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,274 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,274 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:13,275 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,275 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,275 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:13,275 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:13,275 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:13,275 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:13,275 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:13,275 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:13,275 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:13,275 [INFO ] epollEventLoopGroup-5-50 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,275 [INFO ] epollEventLoopGroup-5-50 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:13,276 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,276 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,276 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,276 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:13,276 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:15:13,276 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:15:13,276 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,276 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,276 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,276 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,276 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2025-06-05T04:15:13,276 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2025-06-05T04:15:13,276 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-06-05T04:15:13,276 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:13,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:13,277 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:13,277 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:13,277 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:13,277 [INFO ] epollEventLoopGroup-5-49 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,277 [INFO ] epollEventLoopGroup-5-49 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,277 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,277 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,277 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,277 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,277 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:15:13,277 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:15:13,278 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,278 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,278 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,278 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,278 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2025-06-05T04:15:13,278 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2025-06-05T04:15:13,286 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:15:13,286 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:15:13,286 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:15:13,286 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:15:13,287 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:15:13,287 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:15:13,287 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:15:13,287 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:15:13,288 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:15:13,288 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:15:13,288 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:15:13,288 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:15:13,318 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=42314
2025-06-05T04:15:13,318 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:15:13,322 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:13,322 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]42314
2025-06-05T04:15:13,322 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:13,322 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:13,322 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,322 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,322 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:15:13,322 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:15:13,323 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:15:13,323 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913323
2025-06-05T04:15:13,323 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913323
2025-06-05T04:15:13,323 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913323
2025-06-05T04:15:13,323 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913323
2025-06-05T04:15:13,324 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:15:13,327 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=42311
2025-06-05T04:15:13,327 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:15:13,331 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:13,331 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]42311
2025-06-05T04:15:13,331 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:13,331 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:13,331 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,331 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,331 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:15:13,331 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:15:13,332 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:15:13,332 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913332
2025-06-05T04:15:13,332 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913332
2025-06-05T04:15:13,332 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913332
2025-06-05T04:15:13,332 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913332
2025-06-05T04:15:13,333 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:15:13,340 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:13,340 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:13,348 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:13,348 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:13,415 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:13,415 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:13,415 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:13,415 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:13,420 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=42317
2025-06-05T04:15:13,420 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:15:13,424 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:13,424 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]42317
2025-06-05T04:15:13,424 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:13,424 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:13,425 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,425 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:13,425 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:15:13,425 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:15:13,425 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:13,425 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:15:13,425 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913425
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:13,425 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096913425
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:13,426 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913426
2025-06-05T04:15:13,426 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096913426
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:13,426 [INFO ] epollEventLoopGroup-5-53 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,426 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:13,426 [INFO ] epollEventLoopGroup-5-53 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:13,426 [INFO ] epollEventLoopGroup-5-52 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,426 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:13,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:13,426 [INFO ] epollEventLoopGroup-5-52 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,426 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,426 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:13,427 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,426 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,427 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:13,426 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,427 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,427 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:15:13,427 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:15:13,427 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,427 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:13,427 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,427 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,427 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,427 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,427 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,427 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:13,427 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:15:13,427 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:13,427 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:13,427 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:13,427 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:15:13,427 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,427 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,427 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,427 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2025-06-05T04:15:13,427 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,427 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2025-06-05T04:15:13,428 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2025-06-05T04:15:13,428 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2025-06-05T04:15:13,436 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:15:13,436 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:15:13,436 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:15:13,436 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:15:13,436 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:15:13,436 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:15:13,436 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:15:13,436 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:15:13,442 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:13,443 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:13,539 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:13,539 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:13,550 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:13,550 [INFO ] epollEventLoopGroup-5-54 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,550 [INFO ] epollEventLoopGroup-5-54 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:13,550 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,550 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:13,550 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,550 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:13,551 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:15:13,551 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:15:13,551 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,551 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:13,551 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,551 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:13,551 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2025-06-05T04:15:13,551 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2025-06-05T04:15:13,559 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:15:13,559 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:15:13,559 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:15:13,559 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:15:47,295 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,295 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,295 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,295 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,297 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,297 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,445 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,445 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,446 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:47,446 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:58,848 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:58,848 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:15:48,057 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=42641
2025-06-05T04:15:48,057 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:15:48,064 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:48,064 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]42641
2025-06-05T04:15:48,064 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:48,064 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,064 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,065 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:15:48,065 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:48,065 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:15:48,066 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:15:48,066 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948066
2025-06-05T04:15:48,066 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948066
2025-06-05T04:15:48,066 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948066
2025-06-05T04:15:48,066 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948066
2025-06-05T04:15:48,066 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:15:48,093 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:48,094 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:48,112 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=42637
2025-06-05T04:15:48,112 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:15:48,118 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:48,118 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]42637
2025-06-05T04:15:48,118 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:48,118 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,118 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:48,118 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,119 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:15:48,119 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:15:48,120 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:15:48,120 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948120
2025-06-05T04:15:48,120 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948120
2025-06-05T04:15:48,120 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948120
2025-06-05T04:15:48,120 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948120
2025-06-05T04:15:48,120 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:15:48,136 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:48,136 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:48,148 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=42636
2025-06-05T04:15:48,148 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:15:48,155 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:48,155 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]42636
2025-06-05T04:15:48,155 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:48,155 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:48,155 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,155 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,155 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:15:48,155 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:15:48,155 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:15:48,155 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948155
2025-06-05T04:15:48,155 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948155
2025-06-05T04:15:48,156 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948156
2025-06-05T04:15:48,156 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948156
2025-06-05T04:15:48,156 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:15:48,174 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:48,174 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:48,210 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:48,210 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:48,211 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:48,211 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:48,223 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:48,223 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:48,224 [INFO ] epollEventLoopGroup-5-56 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:48,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:48,224 [INFO ] epollEventLoopGroup-5-56 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,224 [INFO ] epollEventLoopGroup-5-55 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,224 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,224 [INFO ] epollEventLoopGroup-5-55 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:48,224 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:48,224 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:48,224 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,224 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:48,225 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:48,224 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,225 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:48,225 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:48,225 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:15:48,225 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,225 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:15:48,225 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,225 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,225 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,225 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,225 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,225 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:15:48,225 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:15:48,225 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,225 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,225 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,225 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,225 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-06-05T04:15:48,225 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-06-05T04:15:48,225 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2025-06-05T04:15:48,225 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2025-06-05T04:15:48,232 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:15:48,232 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:15:48,232 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:15:48,232 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:15:48,233 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:15:48,233 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:15:48,233 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:15:48,233 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:15:48,235 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:48,235 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:48,244 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=42646
2025-06-05T04:15:48,244 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:48,246 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:48,247 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:48,247 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,247 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:48,247 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,247 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,247 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,247 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,247 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,247 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:15:48,247 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:15:48,247 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,247 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,247 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,247 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,247 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2025-06-05T04:15:48,247 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2025-06-05T04:15:48,250 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:48,250 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]42646
2025-06-05T04:15:48,250 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,250 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:48,250 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,250 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:48,250 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:15:48,250 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:15:48,251 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:15:48,251 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948251
2025-06-05T04:15:48,251 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948251
2025-06-05T04:15:48,251 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948251
2025-06-05T04:15:48,251 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948251
2025-06-05T04:15:48,251 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:15:48,255 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:15:48,255 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:15:48,255 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:15:48,255 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:15:48,266 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:48,266 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:48,284 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=42645
2025-06-05T04:15:48,284 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:15:48,287 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:48,287 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]42645
2025-06-05T04:15:48,287 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:48,287 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:48,287 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,287 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,288 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:15:48,288 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:15:48,288 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:15:48,288 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948288
2025-06-05T04:15:48,288 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948288
2025-06-05T04:15:48,288 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948288
2025-06-05T04:15:48,288 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948288
2025-06-05T04:15:48,288 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:15:48,303 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:48,303 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:48,340 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:48,340 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:48,345 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:48,345 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:48,350 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:48,350 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:48,350 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:48,351 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:48,351 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,351 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,351 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,351 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,351 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,351 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,351 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:15:48,351 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:15:48,351 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,351 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,351 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,351 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,352 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2025-06-05T04:15:48,352 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2025-06-05T04:15:48,356 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:48,356 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:48,357 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,357 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:48,357 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,357 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:48,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:48,358 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:48,357 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,357 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,358 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:15:48,358 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:15:48,358 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,358 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,358 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,358 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,358 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2025-06-05T04:15:48,358 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2025-06-05T04:15:48,361 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:15:48,361 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:15:48,361 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:15:48,361 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:15:48,366 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:15:48,366 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:15:48,366 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:15:48,366 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:15:48,439 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=42656
2025-06-05T04:15:48,439 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:15:48,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:15:48,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]42656
2025-06-05T04:15:48,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:15:48,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:15:48,443 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,443 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:15:48,443 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:15:48,443 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:15:48,443 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:15:48,443 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948443
2025-06-05T04:15:48,443 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749096948443
2025-06-05T04:15:48,443 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948443
2025-06-05T04:15:48,443 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749096948443
2025-06-05T04:15:48,443 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:15:48,457 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:15:48,457 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:15:48,520 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:15:48,520 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:15:48,531 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:15:48,531 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,531 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:15:48,532 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,532 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:15:48,532 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,532 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:15:48,532 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:15:48,532 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:15:48,532 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,532 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:15:48,532 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,532 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:15:48,532 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2025-06-05T04:15:48,532 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2025-06-05T04:15:48,539 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:15:48,539 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:15:48,539 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:15:48,539 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:16:43,247 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,247 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,247 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,247 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,268 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,268 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,373 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,373 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,379 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,379 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,553 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:43,553 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:16:44,017 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=43090
2025-06-05T04:16:44,017 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:16:44,018 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=43095
2025-06-05T04:16:44,018 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:16:44,024 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:16:44,024 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:16:44,024 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]43090
2025-06-05T04:16:44,024 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:16:44,024 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]43095
2025-06-05T04:16:44,025 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,025 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:16:44,025 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,025 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,025 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,025 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:16:44,025 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:16:44,025 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:16:44,025 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:16:44,025 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:16:44,025 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:16:44,025 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004025
2025-06-05T04:16:44,025 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004025
2025-06-05T04:16:44,025 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004025
2025-06-05T04:16:44,025 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004025
2025-06-05T04:16:44,025 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004025
2025-06-05T04:16:44,025 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:16:44,025 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004025
2025-06-05T04:16:44,026 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:16:44,026 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:16:44,026 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004026
2025-06-05T04:16:44,026 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004026
2025-06-05T04:16:44,026 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:16:44,047 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=43089
2025-06-05T04:16:44,048 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:16:44,047 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:16:44,048 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:16:44,051 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:16:44,051 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:16:44,054 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:16:44,054 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]43089
2025-06-05T04:16:44,054 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,054 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:16:44,054 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,055 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:16:44,055 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:16:44,055 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:16:44,055 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:16:44,056 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004055
2025-06-05T04:16:44,056 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004055
2025-06-05T04:16:44,056 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004056
2025-06-05T04:16:44,056 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004056
2025-06-05T04:16:44,056 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:16:44,074 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:16:44,074 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:16:44,135 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:16:44,135 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:16:44,135 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:16:44,135 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:16:44,135 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:16:44,135 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:16:44,148 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:16:44,148 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:16:44,148 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:16:44,148 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:16:44,148 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:16:44,148 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:16:44,149 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:16:44,149 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/c30eafbf06e94e9c8dbd82b4e06f003b/onnx_handler.py", line 11, in initialize
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:16:44,149 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:16:44,149 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:16:44,149 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:16:44,149 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,149 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,149 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,149 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,149 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:16:44,149 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,149 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:16:44,149 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,149 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,149 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,149 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:16:44,149 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:16:44,149 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,149 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:16:44,150 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,150 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:16:44,150 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,149 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:16:44,150 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:16:44,150 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:16:44,149 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:16:44,150 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:16:44,150 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/d9cd4fdee8124e4cb80b1ac3b1e818f3/onnx_handler.py", line 11, in initialize
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:16:44,150 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:16:44,150 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:16:44,150 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:16:44,150 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:16:44,150 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,150 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,149 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,150 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,150 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/f15efda26d74457088ad0e93ced94330/onnx_handler.py", line 11, in initialize
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:16:44,150 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:16:44,150 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,150 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,150 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:16:44,150 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,150 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,150 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:16:44,151 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,151 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,151 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,151 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,151 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2025-06-05T04:16:44,151 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2025-06-05T04:16:44,151 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-06-05T04:16:44,151 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2025-06-05T04:16:44,151 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-06-05T04:16:44,151 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2025-06-05T04:16:44,161 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:16:44,161 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:16:44,161 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:16:44,161 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:16:44,163 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:16:44,163 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:16:44,163 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:16:44,163 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:16:44,164 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:16:44,164 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:16:44,164 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:16:44,164 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:16:44,180 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=43098
2025-06-05T04:16:44,181 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:16:44,185 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:16:44,185 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]43098
2025-06-05T04:16:44,185 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:16:44,185 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:16:44,185 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,185 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,185 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:16:44,185 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:16:44,186 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:16:44,186 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004186
2025-06-05T04:16:44,186 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004186
2025-06-05T04:16:44,186 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004186
2025-06-05T04:16:44,186 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004186
2025-06-05T04:16:44,186 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:16:44,192 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=43101
2025-06-05T04:16:44,192 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:16:44,196 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:16:44,196 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]43101
2025-06-05T04:16:44,197 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:16:44,197 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:16:44,197 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,197 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,197 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:16:44,197 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:16:44,198 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:16:44,198 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004198
2025-06-05T04:16:44,198 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004198
2025-06-05T04:16:44,198 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004198
2025-06-05T04:16:44,198 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004198
2025-06-05T04:16:44,198 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:16:44,206 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:16:44,206 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:16:44,216 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:16:44,216 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:16:44,282 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:16:44,282 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:16:44,282 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:16:44,282 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/9750190e0f7c4b7c9f34ae1bec004dc2/onnx_handler.py", line 11, in initialize
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:16:44,294 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:16:44,294 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:16:44,294 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,294 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:16:44,294 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,294 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:16:44,294 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,294 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,295 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:16:44,295 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:16:44,295 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,295 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:16:44,295 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,295 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/617fd611ef674ece9e8e957d5497fb49/onnx_handler.py", line 11, in initialize
2025-06-05T04:16:44,295 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:16:44,295 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:16:44,295 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,294 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,295 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,294 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,295 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:16:44,295 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:16:44,295 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:16:44,295 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:16:44,295 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,295 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,295 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,295 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,295 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,295 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,295 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,295 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,295 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2025-06-05T04:16:44,295 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2025-06-05T04:16:44,296 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2025-06-05T04:16:44,296 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2025-06-05T04:16:44,303 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:16:44,303 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:16:44,303 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:16:44,303 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:16:44,304 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:16:44,304 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:16:44,304 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:16:44,304 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:16:44,399 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=43158
2025-06-05T04:16:44,400 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]43158
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:16:44,404 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,404 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:16:44,404 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004404
2025-06-05T04:16:44,404 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097004404
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004404
2025-06-05T04:16:44,404 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097004404
2025-06-05T04:16:44,405 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:16:44,423 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:16:44,423 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:16:44,489 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:16:44,489 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:16:44,501 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/1899e7f3554441df9f0581f2c6459d99/onnx_handler.py", line 11, in initialize
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:16:44,502 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,502 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:16:44,502 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:16:44,502 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,502 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:16:44,502 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,502 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:16:44,503 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:16:44,503 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:16:44,503 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,503 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:16:44,503 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,503 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:16:44,503 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2025-06-05T04:16:44,503 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2025-06-05T04:16:44,512 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:16:44,512 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:16:44,512 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:16:44,512 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:21:40,630 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:21:40,630 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:21:40,715 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:21:40,715 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:21:40,716 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:21:40,716 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:21:40,743 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:21:40,743 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:21:40,930 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: logs/config/20250605041411375-startup.cfg
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:21:40,930 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: logs/config/20250605041411375-startup.cfg
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:21:40,935 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20250605041411375-startup.cfg",
  "modelCount": 6,
  "created": 1749096851377,
  "models": {
    "divatma": {
      "1.0": {
        "defaultVersion": true,
        "marName": "divatma.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "vetlom": {
      "1.0": {
        "defaultVersion": true,
        "marName": "vetlom.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "nganmach": {
      "1.0": {
        "defaultVersion": true,
        "marName": "nganmach.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "xuoc": {
      "1.0": {
        "defaultVersion": true,
        "marName": "xuoc.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "bamdinh": {
      "1.0": {
        "defaultVersion": true,
        "marName": "bamdinh.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "thieudong": {
      "1.0": {
        "defaultVersion": true,
        "marName": "thieudong.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    }
  }
}
2025-06-05T04:21:40,935 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20250605041411375-startup.cfg",
  "modelCount": 6,
  "created": 1749096851377,
  "models": {
    "divatma": {
      "1.0": {
        "defaultVersion": true,
        "marName": "divatma.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "vetlom": {
      "1.0": {
        "defaultVersion": true,
        "marName": "vetlom.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "nganmach": {
      "1.0": {
        "defaultVersion": true,
        "marName": "nganmach.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "xuoc": {
      "1.0": {
        "defaultVersion": true,
        "marName": "xuoc.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "bamdinh": {
      "1.0": {
        "defaultVersion": true,
        "marName": "bamdinh.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "thieudong": {
      "1.0": {
        "defaultVersion": true,
        "marName": "thieudong.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    }
  }
}
2025-06-05T04:21:40,940 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20250605041411375-startup.cfg
2025-06-05T04:21:40,940 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20250605041411375-startup.cfg
2025-06-05T04:21:40,940 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20250605041411375-startup.cfg validated successfully
2025-06-05T04:21:40,940 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20250605041411375-startup.cfg validated successfully
2025-06-05T04:21:41,041 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:21:41,041 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:21:41,041 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:21:41,041 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:21:41,042 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:21:41,042 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:21:41,042 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:21:41,042 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:21:41,042 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:21:41,042 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:21:41,046 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,046 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,119 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:21:41,119 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:21:41,119 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:21:41,119 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:21:41,119 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:21:41,119 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:21:41,119 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:21:41,119 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:21:41,120 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:21:41,120 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:21:41,122 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,122 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,191 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:21:41,191 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:21:41,191 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:21:41,191 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:21:41,192 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:21:41,192 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:21:41,192 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:21:41,192 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:21:41,192 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:21:41,192 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:21:41,193 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,193 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:21:41,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:21:41,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:21:41,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:21:41,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:21:41,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:21:41,262 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:21:41,262 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:21:41,262 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:21:41,262 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:21:41,263 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,263 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,337 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:21:41,337 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:21:41,338 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:21:41,338 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:21:41,338 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:21:41,338 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:21:41,338 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:21:41,338 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:21:41,338 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:21:41,338 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:21:41,348 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,348 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,439 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:21:41,439 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:21:41,440 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:21:41,440 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:21:41,440 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:21:41,440 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:21:41,440 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:21:41,440 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:21:41,440 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:21:41,440 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:21:41,450 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,450 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:41,453 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:21:41,453 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:21:41,514 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T04:21:41,514 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T04:21:41,514 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T04:21:41,514 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T04:21:41,515 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T04:21:41,515 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T04:21:41,515 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T04:21:41,515 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T04:21:41,516 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T04:21:41,516 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T04:21:41,900 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=47770
2025-06-05T04:21:41,901 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:21:41,908 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:41,909 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]47770
2025-06-05T04:21:41,909 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:41,910 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:41,910 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:41,912 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:41,913 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:41,913 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:41,918 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:21:41,921 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097301921
2025-06-05T04:21:41,921 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097301921
2025-06-05T04:21:41,924 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097301924
2025-06-05T04:21:41,924 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097301924
2025-06-05T04:21:41,952 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:21:41,964 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:41,965 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:41,980 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=47774
2025-06-05T04:21:41,981 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:21:41,984 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:41,984 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]47774
2025-06-05T04:21:41,984 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:41,984 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:41,984 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:41,984 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:41,985 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:41,985 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:41,986 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097301986
2025-06-05T04:21:41,986 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:21:41,986 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097301986
2025-06-05T04:21:41,986 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097301986
2025-06-05T04:21:41,986 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097301986
2025-06-05T04:21:42,002 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:21:42,024 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:42,024 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:42,053 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:42,053 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:42,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=47779
2025-06-05T04:21:42,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:21:42,058 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:42,058 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]47779
2025-06-05T04:21:42,059 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:42,059 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:42,059 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:42,059 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:42,059 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:42,059 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:42,061 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:21:42,061 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097302061
2025-06-05T04:21:42,061 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097302061
2025-06-05T04:21:42,061 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097302061
2025-06-05T04:21:42,061 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097302061
2025-06-05T04:21:42,068 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:42,068 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:42,076 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:21:42,080 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=47784
2025-06-05T04:21:42,081 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:42,084 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:42,085 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:42,085 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:42,085 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:42,085 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:42,085 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,085 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:42,085 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,085 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:42,085 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:42,086 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,086 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,086 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:42,086 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:42,086 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:21:42,087 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:42,087 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:42,088 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:42,088 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]47784
2025-06-05T04:21:42,088 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:42,088 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:42,088 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:42,088 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:42,088 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:42,088 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:42,090 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097302090
2025-06-05T04:21:42,090 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:21:42,090 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097302090
2025-06-05T04:21:42,091 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097302091
2025-06-05T04:21:42,091 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097302091
2025-06-05T04:21:42,096 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:42,098 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:42,086 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,086 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,099 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:42,099 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:42,099 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,099 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,099 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302099
2025-06-05T04:21:42,099 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302099
2025-06-05T04:21:42,101 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:21:42,101 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:21:42,103 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:42,103 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:42,104 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:21:42,106 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:42,107 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:42,107 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:42,107 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,107 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,107 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:42,108 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:42,108 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:42,108 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,108 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:42,108 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,108 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:42,108 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:42,108 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:42,108 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:42,109 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,109 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,109 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302109
2025-06-05T04:21:42,109 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:42,109 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302109
2025-06-05T04:21:42,109 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:42,109 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:42,110 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:21:42,110 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:21:42,110 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:21:42,110 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:42,110 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:42,110 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:42,110 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:42,110 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:42,110 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:42,117 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:42,117 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:42,121 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:42,122 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:42,181 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:42,181 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:42,182 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:42,182 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:42,192 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:42,192 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:42,195 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:42,195 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:42,195 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:42,195 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:42,195 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,195 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:42,195 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,196 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:21:42,196 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:42,196 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:42,196 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,196 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,196 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302196
2025-06-05T04:21:42,196 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302196
2025-06-05T04:21:42,196 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:21:42,196 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:21:42,198 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:42,198 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:42,200 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:42,200 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:42,200 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:42,200 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:42,200 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:42,200 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,200 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,200 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:42,200 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:42,200 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:42,201 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,201 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:42,201 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:42,201 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:42,201 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:42,201 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:42,201 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,201 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302201
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302201
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:42,202 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:21:42,202 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:42,201 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:21:42,202 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:42,202 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:21:42,202 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:42,202 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:42,205 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:42,205 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:42,205 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:42,205 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:42,211 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:42,211 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:42,245 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=47816
2025-06-05T04:21:42,246 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:21:42,250 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:42,250 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]47816
2025-06-05T04:21:42,250 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:42,250 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:42,250 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:42,250 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:42,250 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:42,250 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:42,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:21:42,251 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097302251
2025-06-05T04:21:42,251 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097302251
2025-06-05T04:21:42,251 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097302251
2025-06-05T04:21:42,251 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097302251
2025-06-05T04:21:42,257 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:21:42,270 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:42,271 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:42,280 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=47854
2025-06-05T04:21:42,281 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:21:42,285 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:42,285 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]47854
2025-06-05T04:21:42,285 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:42,285 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:42,285 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:21:42,285 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:42,285 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:42,285 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:42,287 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097302287
2025-06-05T04:21:42,287 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:21:42,287 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097302287
2025-06-05T04:21:42,287 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097302287
2025-06-05T04:21:42,287 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097302287
2025-06-05T04:21:42,292 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:21:42,306 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:42,306 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:42,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:42,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:42,348 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:42,348 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:42,358 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:42,358 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:42,359 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:42,359 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:42,361 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:42,361 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:42,361 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:42,362 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:42,362 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:42,362 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,362 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:42,362 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:42,362 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:42,362 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,362 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,362 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:42,362 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:42,362 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:42,362 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:42,362 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:42,362 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,362 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:42,362 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:42,362 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:42,362 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:42,363 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:42,363 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:42,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:42,363 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,362 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:42,363 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:42,363 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302363
2025-06-05T04:21:42,363 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302363
2025-06-05T04:21:42,362 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:42,363 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:42,363 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:42,363 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:42,363 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:42,363 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:42,363 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,363 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:42,363 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:42,363 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302363
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:42,363 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097302363
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:42,363 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:21:42,364 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:42,364 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:42,364 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:21:42,364 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:21:42,370 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:42,370 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:42,370 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:42,370 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:42,371 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:42,371 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:43,102 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:43,102 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:43,115 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:43,115 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:43,197 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:43,197 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:43,202 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:43,202 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:54,645 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:54,645 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:54,645 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:54,645 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:43,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=48101
2025-06-05T04:21:43,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:21:43,806 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:43,806 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]48101
2025-06-05T04:21:43,809 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:43,809 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:43,809 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:43,811 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:43,811 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:43,810 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:43,812 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:21:43,813 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097303813
2025-06-05T04:21:43,813 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097303813
2025-06-05T04:21:43,813 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097303813
2025-06-05T04:21:43,813 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097303813
2025-06-05T04:21:43,826 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:21:43,846 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:43,847 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:43,907 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=48098
2025-06-05T04:21:43,908 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:21:43,912 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:43,912 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]48098
2025-06-05T04:21:43,912 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:43,912 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:43,912 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:43,912 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:43,912 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:43,912 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:43,913 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097303913
2025-06-05T04:21:43,913 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097303913
2025-06-05T04:21:43,913 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:21:43,913 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097303913
2025-06-05T04:21:43,913 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097303913
2025-06-05T04:21:43,926 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=48107
2025-06-05T04:21:43,926 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:21:43,929 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:43,929 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:43,929 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:43,929 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]48107
2025-06-05T04:21:43,930 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:43,930 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:43,930 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:43,930 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:43,930 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:43,930 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:43,931 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097303931
2025-06-05T04:21:43,931 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:21:43,931 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097303931
2025-06-05T04:21:43,932 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097303932
2025-06-05T04:21:43,932 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:21:43,932 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097303932
2025-06-05T04:21:43,948 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:43,948 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:43,948 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:43,950 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:43,950 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:43,950 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:43,951 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:43,951 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:43,951 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:43,951 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:43,951 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:43,951 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:43,951 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:43,951 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:43,951 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:21:43,951 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:43,951 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:43,951 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:43,951 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:43,951 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:43,951 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:43,952 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:43,952 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:43,952 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:43,952 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:43,952 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:21:43,952 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:21:43,961 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:43,961 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:43,961 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:43,961 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:43,964 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:43,964 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:43,969 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=48104
2025-06-05T04:21:43,969 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:21:43,973 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:43,974 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]48104
2025-06-05T04:21:43,974 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:43,974 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:43,974 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:43,974 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:43,974 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:43,974 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:43,975 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:21:43,975 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097303975
2025-06-05T04:21:43,975 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097303975
2025-06-05T04:21:43,975 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097303975
2025-06-05T04:21:43,975 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097303975
2025-06-05T04:21:43,988 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:21:44,002 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:44,002 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:44,039 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:44,039 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:44,039 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:44,039 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:44,050 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:44,050 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:44,050 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:44,050 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:44,053 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:44,053 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:44,054 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:44,054 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:44,054 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:44,055 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,055 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:44,055 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:44,055 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,055 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:44,055 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,055 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,055 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:44,055 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:44,055 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,055 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:44,055 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,055 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:44,055 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,055 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:44,055 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,056 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,056 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,055 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:44,055 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:44,056 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:44,056 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,056 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,056 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:44,056 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:21:44,056 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:44,056 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:21:44,062 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:21:44,062 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:21:44,064 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:44,064 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:44,064 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:44,064 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:44,064 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:44,064 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:44,065 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:44,065 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:44,068 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:21:44,068 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,068 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,068 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,068 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,068 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,068 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,069 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:44,069 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:44,069 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,069 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,069 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,069 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,069 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:21:44,069 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:21:44,080 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:44,080 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:44,080 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:44,080 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:44,149 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=48138
2025-06-05T04:21:44,149 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:21:44,153 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:44,153 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]48138
2025-06-05T04:21:44,153 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:44,153 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:44,153 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:44,153 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:44,153 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:44,153 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:44,153 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=48137
2025-06-05T04:21:44,153 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:21:44,154 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:21:44,154 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097304154
2025-06-05T04:21:44,154 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097304154
2025-06-05T04:21:44,154 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097304154
2025-06-05T04:21:44,154 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097304154
2025-06-05T04:21:44,159 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:44,159 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]48137
2025-06-05T04:21:44,159 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:44,159 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:44,159 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:44,159 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:44,159 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:44,159 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:44,160 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:21:44,160 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097304160
2025-06-05T04:21:44,160 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097304160
2025-06-05T04:21:44,160 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097304160
2025-06-05T04:21:44,160 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097304160
2025-06-05T04:21:44,164 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:21:44,167 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:21:44,180 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:44,180 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:44,180 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:44,180 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:44,240 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:44,240 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:44,240 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:44,240 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:44,250 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:44,250 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:44,250 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:44,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:44,253 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:44,253 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:44,253 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:44,253 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:44,253 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:44,254 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:44,254 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:44,253 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,253 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:44,253 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,254 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:44,253 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:44,254 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,254 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,254 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:44,254 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:44,254 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:44,254 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:44,254 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:44,254 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,254 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:44,254 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:44,254 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:44,254 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:44,254 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:44,254 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:44,255 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,254 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:44,255 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,255 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:44,255 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,255 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:44,255 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:44,255 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,255 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:21:44,255 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,255 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:21:44,255 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:21:44,255 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:21:44,255 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:21:44,263 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:44,263 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:44,263 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:44,263 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:44,264 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:44,264 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:44,264 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:44,264 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:44,955 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:44,955 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,056 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,056 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,056 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,056 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,070 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,070 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,256 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,256 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,256 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,256 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:45,744 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=48387
2025-06-05T04:21:45,744 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:21:45,751 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:45,751 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]48387
2025-06-05T04:21:45,751 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:45,751 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:45,751 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:45,752 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:45,752 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:45,752 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:45,753 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097305753
2025-06-05T04:21:45,753 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097305753
2025-06-05T04:21:45,753 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:21:45,753 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097305753
2025-06-05T04:21:45,753 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097305753
2025-06-05T04:21:45,764 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:21:45,777 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=48384
2025-06-05T04:21:45,777 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:21:45,777 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:45,777 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:45,786 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:45,786 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]48384
2025-06-05T04:21:45,787 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:45,787 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:45,787 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:45,787 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:45,787 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:45,787 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:45,788 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:21:45,789 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097305789
2025-06-05T04:21:45,789 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097305789
2025-06-05T04:21:45,789 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097305789
2025-06-05T04:21:45,789 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097305789
2025-06-05T04:21:45,794 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=48393
2025-06-05T04:21:45,795 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:21:45,801 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:21:45,803 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:45,804 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]48393
2025-06-05T04:21:45,804 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:45,804 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:45,804 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:45,804 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:45,804 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:45,804 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:45,806 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:21:45,806 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097305806
2025-06-05T04:21:45,806 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097305806
2025-06-05T04:21:45,806 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097305806
2025-06-05T04:21:45,806 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097305806
2025-06-05T04:21:45,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:21:45,825 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:45,826 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:45,842 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=48388
2025-06-05T04:21:45,843 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:21:45,848 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:45,848 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]48388
2025-06-05T04:21:45,848 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:45,848 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:45,848 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:45,848 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:45,848 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:45,848 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:45,850 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:21:45,850 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097305850
2025-06-05T04:21:45,850 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097305850
2025-06-05T04:21:45,850 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097305850
2025-06-05T04:21:45,850 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:45,850 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097305850
2025-06-05T04:21:45,850 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:45,865 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:21:45,880 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:45,881 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:45,890 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:45,890 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:45,890 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:45,891 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:45,903 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:45,903 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:45,903 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:45,903 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:45,908 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:45,908 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:45,909 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:45,910 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:45,910 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:45,910 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:45,910 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:45,910 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:45,910 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:45,910 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:45,910 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:45,910 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:45,910 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:45,910 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:45,911 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:45,911 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:45,911 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:45,910 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:45,911 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:21:45,910 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:45,911 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:45,911 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:45,911 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:45,911 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:21:45,911 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:45,911 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:45,911 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:45,911 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:45,911 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:45,911 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:45,911 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:45,911 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:45,911 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:45,911 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:45,911 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:45,911 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:45,911 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:45,911 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:45,911 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:45,911 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:45,912 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2025-06-05T04:21:45,912 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2025-06-05T04:21:45,912 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2025-06-05T04:21:45,912 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2025-06-05T04:21:45,922 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:45,922 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:45,922 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:45,922 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:45,922 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:45,923 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:45,924 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:45,924 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:45,924 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:45,924 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:45,927 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:45,927 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:45,927 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:45,927 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:45,928 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:45,928 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:45,928 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:45,928 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:21:45,928 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:45,928 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:45,928 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:45,928 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:45,928 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:45,928 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:45,928 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:45,928 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:45,928 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:45,929 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2025-06-05T04:21:45,929 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2025-06-05T04:21:45,933 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:45,933 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:45,936 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:45,936 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:45,936 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:45,936 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:45,944 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:45,944 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:45,947 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:45,947 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:21:45,947 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:45,947 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:45,947 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:45,947 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:45,947 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:45,948 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:45,948 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:45,948 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:45,948 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:45,948 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:45,948 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:45,949 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-06-05T04:21:45,949 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-06-05T04:21:45,957 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:45,957 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:45,957 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:45,957 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:46,024 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=48424
2025-06-05T04:21:46,024 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:21:46,028 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:46,028 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]48424
2025-06-05T04:21:46,028 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:46,029 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:46,029 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:46,029 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:46,029 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:46,029 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:46,029 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:21:46,029 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097306029
2025-06-05T04:21:46,029 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097306029
2025-06-05T04:21:46,030 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097306030
2025-06-05T04:21:46,030 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097306030
2025-06-05T04:21:46,039 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:21:46,053 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:46,053 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:46,085 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=48423
2025-06-05T04:21:46,086 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:21:46,091 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:46,091 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]48423
2025-06-05T04:21:46,091 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:46,091 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:46,091 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:46,091 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:46,091 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:46,091 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:46,092 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:21:46,092 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097306092
2025-06-05T04:21:46,092 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097306092
2025-06-05T04:21:46,092 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097306092
2025-06-05T04:21:46,092 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097306092
2025-06-05T04:21:46,101 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:21:46,116 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:46,116 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:46,134 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:46,135 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:46,145 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:46,145 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:46,148 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:46,148 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:46,148 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:46,148 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:46,148 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:21:46,148 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:46,148 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:46,149 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:46,149 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:46,149 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:46,149 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:46,149 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:46,149 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:46,149 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2025-06-05T04:21:46,149 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2025-06-05T04:21:46,158 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:46,158 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:46,158 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:46,158 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:46,163 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:46,163 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:46,174 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:46,174 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:46,176 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:21:46,177 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:46,177 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:46,177 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:46,177 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:46,177 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:46,177 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:46,177 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:46,177 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:46,177 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:46,177 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:46,177 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:46,177 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:46,178 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2025-06-05T04:21:46,178 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2025-06-05T04:21:46,185 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:46,185 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:46,185 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:46,185 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:47,913 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:47,913 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:47,913 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:47,913 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:47,929 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:47,929 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:47,949 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:47,949 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:48,150 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:48,150 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:48,178 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:48,178 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:48,692 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=48687
2025-06-05T04:21:48,692 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:21:48,700 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:48,700 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]48687
2025-06-05T04:21:48,701 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:48,701 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,701 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:48,701 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,701 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:48,701 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:48,702 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:21:48,702 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308702
2025-06-05T04:21:48,702 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308702
2025-06-05T04:21:48,702 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308702
2025-06-05T04:21:48,702 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308702
2025-06-05T04:21:48,713 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=48686
2025-06-05T04:21:48,714 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:21:48,721 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:48,721 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]48686
2025-06-05T04:21:48,722 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:48,722 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:48,722 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,722 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,722 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:48,722 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:21:48,722 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:48,723 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308723
2025-06-05T04:21:48,723 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308723
2025-06-05T04:21:48,723 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308723
2025-06-05T04:21:48,723 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308723
2025-06-05T04:21:48,723 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:21:48,742 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:21:48,745 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:48,746 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:48,752 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=48692
2025-06-05T04:21:48,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:21:48,758 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:48,758 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]48692
2025-06-05T04:21:48,758 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:48,758 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:48,758 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,758 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,758 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:48,758 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:48,759 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:48,759 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:48,759 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:21:48,760 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308760
2025-06-05T04:21:48,760 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308760
2025-06-05T04:21:48,760 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308760
2025-06-05T04:21:48,760 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308760
2025-06-05T04:21:48,771 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:21:48,786 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:48,786 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:48,799 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=48695
2025-06-05T04:21:48,800 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:21:48,805 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:48,805 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]48695
2025-06-05T04:21:48,805 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:48,805 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:48,805 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,805 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,805 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:48,805 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:48,807 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:21:48,807 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308807
2025-06-05T04:21:48,807 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308807
2025-06-05T04:21:48,807 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308807
2025-06-05T04:21:48,807 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308807
2025-06-05T04:21:48,817 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:21:48,834 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:48,834 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:48,840 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:48,840 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:48,840 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:48,840 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:48,845 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:48,845 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:48,852 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:48,853 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:48,853 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:48,853 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:48,857 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:48,857 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:48,857 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:48,857 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:48,857 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:48,857 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:48,857 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:48,857 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:48,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:48,857 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:48,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:48,858 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:48,858 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:48,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:48,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:48,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:48,858 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:48,858 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:48,858 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:48,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:48,858 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:48,858 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:48,858 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:48,858 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:48,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:48,858 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:48,858 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:48,858 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:48,858 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:48,858 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:48,858 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:48,858 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:48,858 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:48,859 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:48,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:48,859 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:48,859 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:48,858 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:48,859 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:48,859 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:48,859 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:48,859 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:21:48,859 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:48,859 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:48,859 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:48,859 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:48,859 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:48,859 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:48,859 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:48,859 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:48,860 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:48,860 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:48,861 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2025-06-05T04:21:48,861 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2025-06-05T04:21:48,864 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:48,865 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:21:48,865 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:48,865 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:48,865 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:48,865 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:48,866 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:48,866 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:48,866 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:48,866 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:48,866 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:48,866 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:48,866 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:48,866 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:48,866 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2025-06-05T04:21:48,866 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2025-06-05T04:21:48,869 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:48,869 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:48,869 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:48,869 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:48,870 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:48,870 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:48,875 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:48,875 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:48,875 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:48,875 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:48,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:48,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:48,922 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=48701
2025-06-05T04:21:48,923 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:21:48,927 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:48,927 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]48701
2025-06-05T04:21:48,927 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:48,927 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:48,927 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,927 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,927 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:48,927 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:48,928 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:21:48,928 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308928
2025-06-05T04:21:48,928 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308928
2025-06-05T04:21:48,928 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:48,928 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308928
2025-06-05T04:21:48,928 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308928
2025-06-05T04:21:48,928 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:48,934 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:48,934 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:21:48,935 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:48,935 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:48,935 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:48,935 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:48,935 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:48,935 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:48,935 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:48,935 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:48,935 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:48,935 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:48,935 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:48,935 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-06-05T04:21:48,935 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-06-05T04:21:48,946 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:48,946 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:48,946 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:48,946 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:48,948 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:48,948 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:48,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=48722
2025-06-05T04:21:48,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:21:48,959 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:48,959 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]48722
2025-06-05T04:21:48,959 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:48,959 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:48,959 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,959 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:48,959 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:48,959 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:48,960 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308960
2025-06-05T04:21:48,960 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:21:48,960 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097308960
2025-06-05T04:21:48,960 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308960
2025-06-05T04:21:48,960 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097308960
2025-06-05T04:21:48,965 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:21:48,979 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:48,979 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:49,017 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:49,017 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:49,019 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:49,019 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:49,028 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:49,028 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:49,031 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:49,031 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:49,031 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:49,032 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:21:49,032 [INFO ] epollEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:49,032 [INFO ] epollEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:49,032 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:49,032 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:49,032 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:49,032 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:49,033 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:49,033 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:49,033 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:49,033 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:49,033 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:49,033 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:49,033 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2025-06-05T04:21:49,033 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:49,035 [INFO ] epollEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:49,035 [INFO ] epollEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:49,035 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:49,035 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:49,035 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:21:49,035 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:49,035 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:49,035 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:49,035 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:49,035 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:49,035 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:49,035 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:49,035 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:49,036 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2025-06-05T04:21:49,036 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2025-06-05T04:21:49,042 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:49,042 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:49,042 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:49,042 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:49,044 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:49,044 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:49,044 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:49,044 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:51,862 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:51,862 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:51,862 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:51,862 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:51,867 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:51,867 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:51,936 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:51,936 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:52,034 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:52,034 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:52,036 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:52,036 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:52,647 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=48979
2025-06-05T04:21:52,647 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:21:52,651 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=48978
2025-06-05T04:21:52,651 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:21:52,651 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:52,651 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]48979
2025-06-05T04:21:52,651 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:52,652 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:52,652 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,652 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,652 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:52,652 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:52,653 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:21:52,653 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312653
2025-06-05T04:21:52,653 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312653
2025-06-05T04:21:52,653 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312653
2025-06-05T04:21:52,653 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312653
2025-06-05T04:21:52,655 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:52,655 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]48978
2025-06-05T04:21:52,655 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:52,655 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:52,655 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,655 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,656 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:52,656 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:52,658 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312658
2025-06-05T04:21:52,658 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312658
2025-06-05T04:21:52,658 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:21:52,658 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312658
2025-06-05T04:21:52,658 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312658
2025-06-05T04:21:52,667 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:21:52,672 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:21:52,686 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:52,686 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:52,688 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:52,688 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:52,701 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=48984
2025-06-05T04:21:52,701 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:21:52,705 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:52,705 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]48984
2025-06-05T04:21:52,705 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:52,705 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:52,706 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,706 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,706 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:52,706 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:52,706 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=48987
2025-06-05T04:21:52,706 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:21:52,707 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:21:52,707 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312707
2025-06-05T04:21:52,707 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312707
2025-06-05T04:21:52,707 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312707
2025-06-05T04:21:52,707 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312707
2025-06-05T04:21:52,711 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:52,711 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]48987
2025-06-05T04:21:52,711 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:52,711 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:52,711 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,711 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,711 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:52,711 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:52,713 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:21:52,713 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312713
2025-06-05T04:21:52,713 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312713
2025-06-05T04:21:52,714 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312714
2025-06-05T04:21:52,714 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312714
2025-06-05T04:21:52,719 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:21:52,726 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:21:52,735 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:52,735 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:52,741 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:52,742 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:52,777 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:52,778 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:52,778 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:52,778 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:52,792 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:52,792 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:52,792 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:52,793 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:52,798 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:52,798 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:52,799 [INFO ] epollEventLoopGroup-5-25 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,799 [INFO ] epollEventLoopGroup-5-26 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:52,799 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:52,799 [INFO ] epollEventLoopGroup-5-25 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,799 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:52,799 [INFO ] epollEventLoopGroup-5-26 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:52,800 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:52,800 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:52,800 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,800 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:52,800 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:52,800 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,800 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:52,800 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:52,800 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:52,800 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:52,800 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:52,800 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:52,800 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:52,800 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:52,800 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:52,800 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,800 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,800 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,800 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:21:52,800 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,800 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:21:52,800 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,800 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,801 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:52,801 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:52,801 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,801 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,801 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,801 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,801 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2025-06-05T04:21:52,801 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2025-06-05T04:21:52,801 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2025-06-05T04:21:52,801 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2025-06-05T04:21:52,809 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:52,809 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:52,812 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:52,812 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:52,812 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:52,812 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:52,813 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:52,813 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:52,813 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:52,813 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:52,817 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:52,817 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:52,820 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:52,821 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:52,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:21:52,824 [INFO ] epollEventLoopGroup-5-27 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,824 [INFO ] epollEventLoopGroup-5-27 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,825 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,825 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,825 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,825 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,825 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:52,825 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:52,825 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,825 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,825 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,825 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,825 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2025-06-05T04:21:52,825 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2025-06-05T04:21:52,829 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:52,829 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:52,832 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:52,832 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:52,832 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:52,832 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:52,832 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:52,832 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:52,832 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:52,832 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:52,833 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:21:52,833 [INFO ] epollEventLoopGroup-5-28 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,833 [INFO ] epollEventLoopGroup-5-28 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,833 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,833 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,833 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,833 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,834 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:52,834 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:52,834 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,834 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,834 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,834 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,835 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-06-05T04:21:52,835 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-06-05T04:21:52,835 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:52,835 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:52,835 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:52,835 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:52,845 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:52,845 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:52,845 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:52,845 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:52,868 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=48993
2025-06-05T04:21:52,868 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:21:52,872 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:52,872 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]48993
2025-06-05T04:21:52,872 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:52,872 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:52,872 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,872 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,872 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:52,872 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:52,873 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:21:52,873 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312873
2025-06-05T04:21:52,873 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312873
2025-06-05T04:21:52,873 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312873
2025-06-05T04:21:52,873 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312873
2025-06-05T04:21:52,879 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:21:52,894 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:52,894 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:52,947 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=48990
2025-06-05T04:21:52,947 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:21:52,951 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:52,951 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]48990
2025-06-05T04:21:52,951 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:52,951 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:52,951 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,951 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:52,951 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:52,951 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:52,952 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:21:52,952 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312952
2025-06-05T04:21:52,952 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097312952
2025-06-05T04:21:52,952 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312952
2025-06-05T04:21:52,952 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097312952
2025-06-05T04:21:52,959 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:52,959 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:52,963 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:21:52,968 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:52,968 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:52,971 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:21:52,971 [INFO ] epollEventLoopGroup-5-29 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,971 [INFO ] epollEventLoopGroup-5-29 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:52,971 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,971 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:52,971 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,971 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:52,972 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:52,972 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:52,972 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,972 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:52,972 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,972 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:52,972 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2025-06-05T04:21:52,972 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2025-06-05T04:21:52,978 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:52,978 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:52,980 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:52,980 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:52,980 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:52,980 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:53,068 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:53,068 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:53,078 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:53,079 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:53,081 [INFO ] epollEventLoopGroup-5-30 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:53,081 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:53,081 [INFO ] epollEventLoopGroup-5-30 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:53,082 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:53,082 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:53,082 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:21:53,082 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:53,082 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:53,082 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:53,082 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:53,082 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:53,082 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:53,082 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:53,082 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:53,082 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:53,082 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:53,082 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2025-06-05T04:21:53,082 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2025-06-05T04:21:53,090 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:53,090 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:53,090 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:53,090 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:57,785 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,785 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,785 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,785 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,809 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,809 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,819 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,819 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,956 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:57,956 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:58,066 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:58,066 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:21:58,538 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=49307
2025-06-05T04:21:58,538 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:21:58,546 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:58,546 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]49307
2025-06-05T04:21:58,546 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:58,546 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,546 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,546 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:58,546 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:21:58,547 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:58,547 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:21:58,548 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318548
2025-06-05T04:21:58,548 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318548
2025-06-05T04:21:58,548 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318548
2025-06-05T04:21:58,548 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318548
2025-06-05T04:21:58,567 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:21:58,584 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:58,584 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:58,597 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=49316
2025-06-05T04:21:58,597 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:21:58,597 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=49308
2025-06-05T04:21:58,597 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:21:58,601 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:58,601 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]49316
2025-06-05T04:21:58,601 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:58,601 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:58,601 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,601 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,602 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:58,602 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:21:58,602 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:58,602 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]49308
2025-06-05T04:21:58,602 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:58,602 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:58,602 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,602 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,603 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:58,603 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:21:58,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:21:58,603 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318603
2025-06-05T04:21:58,603 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318603
2025-06-05T04:21:58,603 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318603
2025-06-05T04:21:58,603 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318603
2025-06-05T04:21:58,603 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:21:58,606 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318606
2025-06-05T04:21:58,603 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=49313
2025-06-05T04:21:58,606 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318606
2025-06-05T04:21:58,606 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:21:58,606 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318606
2025-06-05T04:21:58,606 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318606
2025-06-05T04:21:58,615 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:58,615 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]49313
2025-06-05T04:21:58,615 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:58,615 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:58,615 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,615 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,615 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:58,615 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:21:58,618 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:21:58,618 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318618
2025-06-05T04:21:58,618 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318618
2025-06-05T04:21:58,619 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318619
2025-06-05T04:21:58,619 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318619
2025-06-05T04:21:58,622 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:21:58,629 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:21:58,635 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:21:58,639 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:58,639 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:58,645 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:58,645 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:58,649 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:58,650 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:58,695 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:58,695 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:58,703 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:58,703 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:58,705 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:58,705 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:58,706 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:58,706 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:58,710 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:58,710 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:58,716 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:58,717 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:58,717 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:21:58,717 [INFO ] epollEventLoopGroup-5-31 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,717 [INFO ] epollEventLoopGroup-5-31 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,717 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,717 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,717 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,717 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,717 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:58,717 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:21:58,717 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,717 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,717 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,717 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,718 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2025-06-05T04:21:58,718 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2025-06-05T04:21:58,719 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:58,719 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:58,721 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:58,721 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:58,721 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:58,721 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:58,722 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:58,722 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:58,722 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:58,723 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:21:58,723 [INFO ] epollEventLoopGroup-5-32 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,723 [INFO ] epollEventLoopGroup-5-32 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,724 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,724 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,724 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,724 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,724 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:58,724 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:21:58,724 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,724 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,724 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,724 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,724 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-06-05T04:21:58,724 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:58,724 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:58,725 [INFO ] epollEventLoopGroup-5-33 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:58,725 [INFO ] epollEventLoopGroup-5-34 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:58,725 [INFO ] epollEventLoopGroup-5-34 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,724 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-06-05T04:21:58,725 [INFO ] epollEventLoopGroup-5-33 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:58,725 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,725 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:58,725 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,725 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:58,725 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:58,725 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:21:58,725 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:58,725 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:58,725 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:58,725 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,725 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:21:58,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:58,726 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:58,726 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,726 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:58,726 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:21:58,726 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,726 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,726 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:58,726 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,726 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,726 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:58,726 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,726 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,726 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:58,726 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,726 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:21:58,726 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2025-06-05T04:21:58,726 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2025-06-05T04:21:58,726 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2025-06-05T04:21:58,726 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2025-06-05T04:21:58,731 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:58,731 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:58,731 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:21:58,731 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:21:58,738 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:58,738 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:58,738 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:21:58,738 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:21:58,739 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:58,739 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:21:58,739 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:58,739 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:21:58,740 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:58,740 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:21:58,740 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:58,740 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:21:58,824 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=49319
2025-06-05T04:21:58,824 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:21:58,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:58,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]49319
2025-06-05T04:21:58,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:58,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:58,829 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,829 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,829 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:58,829 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:21:58,830 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:21:58,830 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318830
2025-06-05T04:21:58,830 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318830
2025-06-05T04:21:58,830 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318830
2025-06-05T04:21:58,830 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318830
2025-06-05T04:21:58,840 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:21:58,857 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:58,857 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:58,926 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=49376
2025-06-05T04:21:58,926 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:21:58,931 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:21:58,931 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]49376
2025-06-05T04:21:58,931 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:21:58,931 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:21:58,931 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,931 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:21:58,932 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:58,932 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:21:58,932 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:21:58,932 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318932
2025-06-05T04:21:58,932 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097318932
2025-06-05T04:21:58,933 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318933
2025-06-05T04:21:58,933 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097318933
2025-06-05T04:21:58,938 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:58,938 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:58,946 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:21:58,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:58,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:58,955 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:21:58,955 [INFO ] epollEventLoopGroup-5-35 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,955 [INFO ] epollEventLoopGroup-5-35 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:58,956 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,956 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:58,956 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,956 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:58,956 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:58,956 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:21:58,956 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,956 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:58,956 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,956 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:58,957 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2025-06-05T04:21:58,957 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2025-06-05T04:21:58,964 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:21:58,964 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:21:58,966 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:58,966 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:58,966 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:21:58,966 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:21:59,049 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:21:59,049 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:21:59,062 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:21:59,062 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:21:59,064 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:21:59,064 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:21:59,064 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:21:59,065 [INFO ] epollEventLoopGroup-5-36 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:59,065 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:21:59,065 [INFO ] epollEventLoopGroup-5-36 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:21:59,065 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:59,065 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:21:59,065 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:59,065 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:21:59,065 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:59,065 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:21:59,065 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:59,065 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:21:59,065 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:59,065 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:21:59,066 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2025-06-05T04:21:59,066 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2025-06-05T04:21:59,075 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:21:59,075 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:59,075 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:21:59,075 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:22:06,724 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,724 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,730 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,730 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,732 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,732 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,732 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,732 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,963 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:06,963 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:07,072 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:07,072 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:07,412 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=49636
2025-06-05T04:22:07,413 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:22:07,422 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:07,422 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]49636
2025-06-05T04:22:07,423 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,423 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:07,423 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,423 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:22:07,423 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:22:07,423 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:07,426 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:22:07,427 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=49633
2025-06-05T04:22:07,427 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:22:07,427 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327427
2025-06-05T04:22:07,427 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327427
2025-06-05T04:22:07,427 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327427
2025-06-05T04:22:07,427 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327427
2025-06-05T04:22:07,437 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:07,438 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]49633
2025-06-05T04:22:07,438 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:07,438 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,438 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,438 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:22:07,438 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:22:07,439 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:07,447 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327447
2025-06-05T04:22:07,447 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:22:07,447 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327447
2025-06-05T04:22:07,448 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327448
2025-06-05T04:22:07,448 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327448
2025-06-05T04:22:07,468 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:22:07,485 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:07,486 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:07,488 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:22:07,498 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:07,498 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:07,519 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=49637
2025-06-05T04:22:07,519 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:22:07,520 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=49630
2025-06-05T04:22:07,520 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:22:07,524 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:07,524 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]49637
2025-06-05T04:22:07,524 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:07,524 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:07,524 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,524 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,524 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:22:07,524 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:22:07,525 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:07,525 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]49630
2025-06-05T04:22:07,525 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:07,525 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:07,525 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,525 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,525 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:22:07,525 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:22:07,525 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:22:07,525 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327525
2025-06-05T04:22:07,525 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327525
2025-06-05T04:22:07,525 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327525
2025-06-05T04:22:07,525 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327525
2025-06-05T04:22:07,530 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:22:07,531 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327531
2025-06-05T04:22:07,531 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327531
2025-06-05T04:22:07,531 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327531
2025-06-05T04:22:07,531 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327531
2025-06-05T04:22:07,551 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:22:07,551 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:22:07,566 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:07,566 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:07,566 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:07,566 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:07,584 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:07,584 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:07,584 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:07,584 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:07,597 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:07,598 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:07,599 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:07,599 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:07,603 [INFO ] epollEventLoopGroup-5-38 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:07,603 [INFO ] epollEventLoopGroup-5-38 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:07,603 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:22:07,603 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,603 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:07,603 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,603 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:07,603 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:07,603 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:07,604 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:07,604 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:07,604 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:07,604 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,604 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,604 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:22:07,604 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,604 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-06-05T04:22:07,604 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-06-05T04:22:07,604 [INFO ] epollEventLoopGroup-5-37 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,604 [INFO ] epollEventLoopGroup-5-37 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,605 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,605 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,605 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,605 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,605 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:22:07,605 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:22:07,605 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,605 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,605 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,605 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,605 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2025-06-05T04:22:07,605 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2025-06-05T04:22:07,616 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:22:07,616 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:22:07,616 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:22:07,616 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:22:07,617 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:22:07,617 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:22:07,617 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:22:07,617 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:22:07,628 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:07,628 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:07,628 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:07,628 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:07,640 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:07,640 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:07,640 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:07,640 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:07,642 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:07,642 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:07,642 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:07,642 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:07,642 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:07,642 [INFO ] epollEventLoopGroup-5-39 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,642 [INFO ] epollEventLoopGroup-5-40 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:07,642 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:07,642 [INFO ] epollEventLoopGroup-5-39 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,642 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:07,643 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:07,642 [INFO ] epollEventLoopGroup-5-40 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,643 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,643 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:07,643 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,643 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,643 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:07,643 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:07,643 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:07,643 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:07,643 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,643 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:07,643 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,643 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:07,643 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:22:07,643 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:07,643 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:22:07,643 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:07,643 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:22:07,643 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,643 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,643 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,643 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,643 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,643 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,643 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:07,643 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,644 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:07,644 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:07,644 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:07,644 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2025-06-05T04:22:07,644 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2025-06-05T04:22:07,644 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2025-06-05T04:22:07,644 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:22:07,644 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2025-06-05T04:22:07,644 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:22:07,644 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:22:07,655 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:22:07,655 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:22:07,655 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:22:07,655 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:22:07,655 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:22:07,655 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:22:07,759 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=49642
2025-06-05T04:22:07,760 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:22:07,764 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:07,764 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]49642
2025-06-05T04:22:07,764 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:07,764 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:07,764 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,764 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,764 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:22:07,764 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:22:07,765 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:22:07,765 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327765
2025-06-05T04:22:07,765 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327765
2025-06-05T04:22:07,765 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327765
2025-06-05T04:22:07,765 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327765
2025-06-05T04:22:07,774 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:22:07,789 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:07,789 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:07,814 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=49753
2025-06-05T04:22:07,814 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:22:07,818 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:07,818 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]49753
2025-06-05T04:22:07,818 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:07,818 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:07,818 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,818 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:07,818 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:22:07,818 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:22:07,819 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:22:07,819 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327819
2025-06-05T04:22:07,819 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097327819
2025-06-05T04:22:07,819 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327819
2025-06-05T04:22:07,819 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097327819
2025-06-05T04:22:07,819 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:22:07,833 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:07,833 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:07,871 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:07,871 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:07,877 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:07,877 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:07,882 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:07,882 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:07,885 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:22:07,885 [INFO ] epollEventLoopGroup-5-41 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,885 [INFO ] epollEventLoopGroup-5-41 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,885 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,885 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,885 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,885 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,886 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:22:07,886 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:22:07,886 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,886 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,886 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,886 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,886 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2025-06-05T04:22:07,886 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2025-06-05T04:22:07,888 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:07,888 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:07,891 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:07,891 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:07,891 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:07,891 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:07,891 [INFO ] epollEventLoopGroup-5-42 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,891 [INFO ] epollEventLoopGroup-5-42 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:07,891 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:07,891 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:07,891 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:07,891 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:07,892 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,892 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:07,892 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:07,892 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:07,892 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:22:07,892 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:22:07,892 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:22:07,892 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:07,892 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,892 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2025-06-05T04:22:07,892 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2025-06-05T04:22:07,895 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:22:07,895 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:22:07,895 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:22:07,895 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:22:07,900 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:22:07,900 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:22:07,900 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:22:07,900 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:22:20,587 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,587 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,587 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,587 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,626 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,626 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,626 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,626 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,869 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,869 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,874 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:20,874 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:21,417 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=49988
2025-06-05T04:22:21,417 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:22:21,424 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:21,424 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]49988
2025-06-05T04:22:21,424 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:21,424 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:21,424 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,424 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,424 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:22:21,424 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:22:21,426 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:22:21,426 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341426
2025-06-05T04:22:21,426 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341426
2025-06-05T04:22:21,426 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341426
2025-06-05T04:22:21,426 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341426
2025-06-05T04:22:21,427 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:22:21,429 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=49989
2025-06-05T04:22:21,429 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:22:21,434 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=49995
2025-06-05T04:22:21,434 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:22:21,435 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:21,436 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]49989
2025-06-05T04:22:21,436 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:21,436 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:21,436 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,436 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,436 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:22:21,436 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:22:21,436 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=49994
2025-06-05T04:22:21,436 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:22:21,437 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:22:21,437 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341437
2025-06-05T04:22:21,437 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341437
2025-06-05T04:22:21,437 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341437
2025-06-05T04:22:21,437 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341437
2025-06-05T04:22:21,438 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:22:21,439 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:21,439 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]49995
2025-06-05T04:22:21,439 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:21,439 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,439 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:21,439 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,439 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:22:21,439 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:22:21,440 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:22:21,440 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341440
2025-06-05T04:22:21,440 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341440
2025-06-05T04:22:21,440 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341440
2025-06-05T04:22:21,440 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341440
2025-06-05T04:22:21,441 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:22:21,441 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:21,441 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]49994
2025-06-05T04:22:21,441 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:21,442 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:21,442 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,442 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,442 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:22:21,442 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:22:21,442 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:22:21,442 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341442
2025-06-05T04:22:21,442 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341442
2025-06-05T04:22:21,443 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341443
2025-06-05T04:22:21,443 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341443
2025-06-05T04:22:21,443 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:22:21,444 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:21,444 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:21,453 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:21,454 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:21,461 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:21,461 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:21,463 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:21,463 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:21,543 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:21,543 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:21,544 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:21,543 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:21,544 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:21,544 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:21,544 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:21,544 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:21,554 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:21,555 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:21,554 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:21,554 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:21,555 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:21,555 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:21,558 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:21,559 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:21,559 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:22:21,559 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:21,559 [INFO ] epollEventLoopGroup-5-43 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,559 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:21,559 [INFO ] epollEventLoopGroup-5-43 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,559 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:21,559 [INFO ] epollEventLoopGroup-5-46 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,559 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:21,559 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,559 [INFO ] epollEventLoopGroup-5-46 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,559 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:21,559 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:21,559 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:21,559 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:21,559 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:21,559 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,559 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,559 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,559 [INFO ] epollEventLoopGroup-5-44 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,559 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:21,559 [INFO ] epollEventLoopGroup-5-44 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,560 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:21,560 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:21,560 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,560 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,559 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:21,560 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,560 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:21,560 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,560 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:21,560 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,560 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,560 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,560 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,560 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:22:21,560 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:22:21,560 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:22:21,560 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:21,560 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:21,560 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:22:21,560 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:22:21,560 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,560 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:22:21,560 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,560 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,560 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,560 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:21,560 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,560 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,560 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,560 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:21,560 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,560 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:21,560 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,560 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,560 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,560 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-06-05T04:22:21,560 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2025-06-05T04:22:21,560 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-06-05T04:22:21,560 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:21,560 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,561 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:21,560 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2025-06-05T04:22:21,560 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:21,561 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:21,561 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2025-06-05T04:22:21,561 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2025-06-05T04:22:21,561 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:21,561 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:21,561 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:21,561 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:21,561 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:21,561 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:21,561 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:21,561 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:21,562 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:21,562 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:21,562 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:21,562 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:22:21,562 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:22:21,562 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:22:21,562 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:22:21,562 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:22:21,562 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:22:21,563 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:21,564 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:21,568 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:22:21,568 [INFO ] epollEventLoopGroup-5-45 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,568 [INFO ] epollEventLoopGroup-5-45 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,569 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,569 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,569 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,569 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,569 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:22:21,569 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:22:21,570 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,570 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,570 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,570 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,570 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2025-06-05T04:22:21,570 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2025-06-05T04:22:21,572 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:22:21,572 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:22:21,572 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:22:21,572 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:22:21,573 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:22:21,573 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:22:21,574 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:22:21,574 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:22:21,579 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:22:21,579 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:22:21,579 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:22:21,579 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:22:21,721 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=50000
2025-06-05T04:22:21,721 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:22:21,725 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:21,725 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]50000
2025-06-05T04:22:21,725 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:21,725 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:21,726 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,726 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,726 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:22:21,726 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:22:21,727 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:22:21,727 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341727
2025-06-05T04:22:21,727 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341727
2025-06-05T04:22:21,727 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341727
2025-06-05T04:22:21,727 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341727
2025-06-05T04:22:21,727 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:22:21,732 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=50003
2025-06-05T04:22:21,732 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:22:21,737 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:21,737 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]50003
2025-06-05T04:22:21,737 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:21,737 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,737 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:21,737 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:21,737 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:22:21,737 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:22:21,738 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341738
2025-06-05T04:22:21,738 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:22:21,738 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097341738
2025-06-05T04:22:21,738 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341738
2025-06-05T04:22:21,738 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097341738
2025-06-05T04:22:21,739 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:22:21,741 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:21,741 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:21,753 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:21,753 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:21,814 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:21,814 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:21,814 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:21,814 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:21,825 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:21,825 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:21,825 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:21,825 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:21,828 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:21,828 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:21,828 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:21,828 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:21,828 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:21,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:21,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:21,829 [INFO ] epollEventLoopGroup-5-47 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:21,829 [INFO ] epollEventLoopGroup-5-47 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,829 [INFO ] epollEventLoopGroup-5-48 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:21,829 [INFO ] epollEventLoopGroup-5-48 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:21,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:21,829 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:21,829 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:21,829 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:21,829 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:21,829 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:21,829 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:21,829 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:22:21,829 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:22:21,829 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,829 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:21,829 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:21,829 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,829 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:21,829 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,829 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:21,829 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:22:21,829 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,829 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:21,829 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:21,829 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:21,829 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:21,829 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:21,830 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:22:21,830 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:21,830 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:22:21,830 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2025-06-05T04:22:21,830 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:22:21,830 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:21,830 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2025-06-05T04:22:21,830 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2025-06-05T04:22:21,830 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:22:21,830 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2025-06-05T04:22:21,830 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:22:21,830 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:22:21,839 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:22:21,839 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:22:21,840 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:22:21,840 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:22:42,557 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,557 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,557 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,557 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,558 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,558 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,567 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,567 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,826 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,826 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,826 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:42,826 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:22:54,591 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=50363
2025-06-05T04:22:54,592 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:22:54,600 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:54,600 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]50363
2025-06-05T04:22:54,601 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:54,601 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:54,601 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:54,601 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:22:54,601 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:22:54,602 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:54,602 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:22:54,602 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097374602
2025-06-05T04:22:54,602 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097374602
2025-06-05T04:22:54,602 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097374602
2025-06-05T04:22:54,602 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097374602
2025-06-05T04:22:54,603 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:22:54,618 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:54,618 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:54,624 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=50370
2025-06-05T04:22:54,624 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:22:54,632 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:54,632 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]50370
2025-06-05T04:22:54,632 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:54,632 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:54,632 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:54,632 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:54,633 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:22:54,633 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:22:54,633 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=50361
2025-06-05T04:22:54,633 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:22:54,634 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097374634
2025-06-05T04:22:54,634 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097374634
2025-06-05T04:22:54,634 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:22:54,634 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097374634
2025-06-05T04:22:54,634 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097374634
2025-06-05T04:22:54,634 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:22:54,642 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:54,642 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]50361
2025-06-05T04:22:54,642 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:54,642 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:54,642 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:54,642 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:54,642 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:22:54,642 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:22:54,643 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:22:54,643 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097374643
2025-06-05T04:22:54,643 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097374643
2025-06-05T04:22:54,644 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097374644
2025-06-05T04:22:54,644 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097374644
2025-06-05T04:22:54,644 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:22:54,648 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=50362
2025-06-05T04:22:54,649 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:22:54,651 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:54,651 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:54,653 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:54,654 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]50362
2025-06-05T04:22:54,654 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:54,654 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:54,654 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:54,654 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:54,654 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:22:54,654 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:22:54,655 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:22:54,655 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097374655
2025-06-05T04:22:54,655 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097374655
2025-06-05T04:22:54,655 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097374655
2025-06-05T04:22:54,655 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097374655
2025-06-05T04:22:54,656 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:22:54,664 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:54,664 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:54,674 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:54,674 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:54,717 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:54,717 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:54,717 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:54,717 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:54,721 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:54,721 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:54,731 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:54,731 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:54,731 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:54,731 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:54,732 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:54,732 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:54,735 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:54,735 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:54,737 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:54,737 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:54,737 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:54,737 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:54,738 [INFO ] epollEventLoopGroup-5-49 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:54,738 [INFO ] epollEventLoopGroup-5-49 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:54,738 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:54,738 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:54,738 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:54,738 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:54,738 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:54,738 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:54,738 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:54,738 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:54,738 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:54,738 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:54,738 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:54,738 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:54,739 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:54,739 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:54,739 [INFO ] epollEventLoopGroup-5-51 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:54,739 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2025-06-05T04:22:54,739 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:22:54,739 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2025-06-05T04:22:54,739 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:22:54,739 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:22:54,739 [INFO ] epollEventLoopGroup-5-51 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:54,739 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:54,739 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:54,739 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:54,739 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:54,739 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:54,740 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:54,740 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:54,740 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:54,740 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:54,740 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:54,740 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:22:54,740 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:22:54,745 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:54,746 [INFO ] epollEventLoopGroup-5-50 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:54,746 [INFO ] epollEventLoopGroup-5-50 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:54,746 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:54,746 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:54,746 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:54,746 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:22:54,746 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:54,747 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:22:54,747 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:22:54,747 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:54,747 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:54,747 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:54,747 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:54,747 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2025-06-05T04:22:54,747 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2025-06-05T04:22:54,748 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:54,749 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:54,751 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:22:54,751 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:22:54,752 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:54,752 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:54,753 [INFO ] epollEventLoopGroup-5-52 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:54,753 [INFO ] epollEventLoopGroup-5-52 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:54,753 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:54,753 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:22:54,753 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:54,753 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:54,753 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:54,754 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:22:54,754 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:22:54,754 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:54,754 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:54,754 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:54,754 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:54,754 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2025-06-05T04:22:54,754 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2025-06-05T04:22:54,755 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:22:54,755 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:22:54,759 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:22:54,759 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:22:54,759 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:22:54,759 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:22:54,765 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:22:54,765 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:22:54,765 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:22:54,765 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:22:43,602 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=50379
2025-06-05T04:22:43,602 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:22:43,606 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:43,606 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]50379
2025-06-05T04:22:43,606 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:43,606 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:43,606 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:43,606 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:43,607 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:22:43,607 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:22:43,607 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:22:43,607 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097363607
2025-06-05T04:22:43,607 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097363607
2025-06-05T04:22:43,608 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097363608
2025-06-05T04:22:43,608 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097363608
2025-06-05T04:22:43,608 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:22:43,621 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:43,621 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:43,640 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=50380
2025-06-05T04:22:43,641 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:22:43,645 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:22:43,645 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]50380
2025-06-05T04:22:43,645 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:22:43,645 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:22:43,645 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:43,645 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:22:43,645 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:22:43,645 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:22:43,646 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:22:43,646 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097363646
2025-06-05T04:22:43,646 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097363646
2025-06-05T04:22:43,646 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097363646
2025-06-05T04:22:43,646 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097363646
2025-06-05T04:22:43,646 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:22:43,660 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:22:43,660 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:22:43,690 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:43,690 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:43,701 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:43,701 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:43,704 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:22:43,704 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:43,704 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:22:43,704 [INFO ] epollEventLoopGroup-5-53 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:43,704 [INFO ] epollEventLoopGroup-5-53 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:43,704 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:43,704 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:43,704 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:43,704 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:43,705 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:22:43,705 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:22:43,705 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:43,705 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:43,705 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:43,705 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:43,705 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2025-06-05T04:22:43,705 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2025-06-05T04:22:43,713 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:22:43,713 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:22:43,713 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:22:43,713 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:22:43,715 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:22:43,715 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:22:43,717 [INFO ] epollEventLoopGroup-5-54 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:43,717 [INFO ] epollEventLoopGroup-5-54 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:22:43,717 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:22:43,718 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:22:43,718 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:43,718 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:22:43,718 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:22:43,718 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:22:43,718 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:43,718 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:22:43,718 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:22:43,718 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:22:43,718 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:43,718 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:22:43,718 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:43,718 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:22:43,718 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2025-06-05T04:22:43,718 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2025-06-05T04:22:43,727 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:22:43,727 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:22:43,727 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:22:43,727 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:23:17,440 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,440 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,441 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,441 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,447 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,447 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,454 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,454 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,702 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,702 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,715 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:17,715 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:23:18,158 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=50798
2025-06-05T04:23:18,159 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:23:18,163 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:23:18,163 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]50798
2025-06-05T04:23:18,163 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:23:18,163 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:18,163 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:23:18,163 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:18,163 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:23:18,163 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:23:18,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:23:18,164 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097398164
2025-06-05T04:23:18,164 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097398164
2025-06-05T04:23:18,165 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097398165
2025-06-05T04:23:18,165 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097398165
2025-06-05T04:23:18,166 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:23:18,179 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=50807
2025-06-05T04:23:18,179 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:23:18,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:23:18,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]50807
2025-06-05T04:23:18,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:23:18,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:23:18,183 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:18,183 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:18,183 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:23:18,183 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:23:18,184 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:23:18,184 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097398184
2025-06-05T04:23:18,184 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097398184
2025-06-05T04:23:18,184 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097398184
2025-06-05T04:23:18,184 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097398184
2025-06-05T04:23:18,185 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:23:18,185 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:23:18,185 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:23:18,200 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:23:18,200 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:23:18,240 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=50804
2025-06-05T04:23:18,240 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:23:18,243 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=50799
2025-06-05T04:23:18,243 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:23:18,244 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:23:18,244 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]50804
2025-06-05T04:23:18,244 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:23:18,244 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:23:18,244 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:18,244 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:18,244 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:23:18,244 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:23:18,245 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:23:18,245 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097398245
2025-06-05T04:23:18,245 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097398245
2025-06-05T04:23:18,245 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097398245
2025-06-05T04:23:18,245 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097398245
2025-06-05T04:23:18,246 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:23:18,248 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:23:18,248 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]50799
2025-06-05T04:23:18,248 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:23:18,248 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:23:18,248 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:18,248 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:18,248 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:23:18,248 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:23:18,248 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:23:18,249 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097398249
2025-06-05T04:23:18,249 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097398249
2025-06-05T04:23:18,249 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097398249
2025-06-05T04:23:18,249 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097398249
2025-06-05T04:23:18,249 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:23:18,261 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:23:18,261 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:23:18,265 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:23:18,265 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:23:18,265 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:23:18,265 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:23:18,265 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:23:18,265 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:23:18,278 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:23:18,278 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:23:18,279 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:23:18,279 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:23:18,283 [INFO ] epollEventLoopGroup-5-56 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:23:18,283 [INFO ] epollEventLoopGroup-5-56 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:23:18,283 [INFO ] epollEventLoopGroup-5-55 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:18,283 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:23:18,283 [INFO ] epollEventLoopGroup-5-55 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:18,284 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:18,284 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:18,284 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:23:18,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:23:18,284 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:23:18,284 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:23:18,284 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:18,284 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:23:18,284 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:23:18,284 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:18,284 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:18,284 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:18,284 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:23:18,284 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:18,284 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:23:18,284 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:23:18,284 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:18,284 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:23:18,285 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:18,285 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:18,285 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:18,285 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:18,284 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:18,285 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:18,285 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:18,285 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2025-06-05T04:23:18,285 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2025-06-05T04:23:18,285 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2025-06-05T04:23:18,285 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2025-06-05T04:23:18,294 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:23:18,294 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:23:18,294 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:23:18,294 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:23:18,295 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:23:18,295 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:23:18,295 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:23:18,295 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:23:18,336 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:23:18,336 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:23:18,336 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:23:18,337 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:23:18,347 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:23:18,347 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:23:18,347 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:23:18,347 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:23:18,350 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:23:18,350 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:23:18,350 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:23:18,350 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:18,350 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:18,350 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:18,350 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:18,350 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:23:18,350 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:23:18,350 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:18,350 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:18,350 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:18,350 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:18,351 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-06-05T04:23:18,351 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:23:18,352 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:23:18,352 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:23:18,352 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:23:18,352 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:18,352 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:18,352 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:18,352 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:18,353 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:23:18,353 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:23:18,353 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:18,353 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:18,353 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:18,353 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:18,353 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2025-06-05T04:23:18,353 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2025-06-05T04:23:29,660 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:23:29,660 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:23:29,660 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:23:29,660 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:23:29,661 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:23:29,661 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:23:29,661 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:23:29,661 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:23:29,765 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=50810
2025-06-05T04:23:29,766 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:23:29,770 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:23:29,770 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]50810
2025-06-05T04:23:29,770 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:23:29,770 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:23:29,770 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:29,770 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:29,770 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:23:29,770 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:23:29,771 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:23:29,771 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097409771
2025-06-05T04:23:29,771 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097409771
2025-06-05T04:23:29,771 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097409771
2025-06-05T04:23:29,771 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097409771
2025-06-05T04:23:29,771 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:23:29,777 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=50813
2025-06-05T04:23:29,777 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:23:29,782 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:23:29,782 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]50813
2025-06-05T04:23:29,782 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:23:29,782 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:23:29,782 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:29,782 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:23:29,782 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:23:29,782 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:23:29,783 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:23:29,783 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097409783
2025-06-05T04:23:29,783 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097409783
2025-06-05T04:23:29,783 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097409783
2025-06-05T04:23:29,783 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097409783
2025-06-05T04:23:29,783 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:23:29,785 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:23:29,785 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:23:29,797 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:23:29,797 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:23:29,845 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:23:29,845 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:23:29,845 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:23:29,845 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:23:29,856 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:23:29,856 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:23:29,856 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:23:29,856 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:23:29,859 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:23:29,859 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:23:29,859 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:23:29,859 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:23:29,859 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:23:29,859 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:23:29,859 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:23:29,859 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:23:29,860 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:23:29,860 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:23:29,860 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:23:29,860 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:29,860 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:23:29,860 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:23:29,860 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:29,860 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:23:29,860 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:23:29,860 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:23:29,860 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:23:29,860 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:23:29,860 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:29,860 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:23:29,860 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:29,860 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:29,860 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:29,860 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:23:29,860 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:23:29,860 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:23:29,860 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:29,860 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:23:29,860 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:29,860 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2025-06-05T04:23:29,860 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2025-06-05T04:23:29,860 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2025-06-05T04:23:29,868 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:23:29,868 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:23:29,868 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:23:29,868 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:23:29,870 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:23:29,870 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:23:29,870 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:23:29,870 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:24:13,297 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:13,297 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:13,297 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:13,297 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:13,362 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:13,362 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:13,364 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:13,364 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:24,861 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:24,861 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:24,861 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:24,861 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:24:14,012 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=51349
2025-06-05T04:24:14,012 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:24:14,019 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:24:14,019 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]51349
2025-06-05T04:24:14,019 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:24:14,019 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,019 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,020 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:24:14,020 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:24:14,020 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454020
2025-06-05T04:24:14,020 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454020
2025-06-05T04:24:14,021 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454021
2025-06-05T04:24:14,021 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454021
2025-06-05T04:24:14,021 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:24:14,021 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:24:14,021 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:24:14,046 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:24:14,047 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:24:14,089 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=51350
2025-06-05T04:24:14,089 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:24:14,093 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:24:14,093 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]51350
2025-06-05T04:24:14,094 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:24:14,094 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:24:14,094 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,094 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,094 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:24:14,094 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:24:14,094 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:24:14,094 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454094
2025-06-05T04:24:14,094 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454094
2025-06-05T04:24:14,094 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454094
2025-06-05T04:24:14,094 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454094
2025-06-05T04:24:14,095 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:24:14,110 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:24:14,110 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:24:14,138 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=51355
2025-06-05T04:24:14,139 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:24:14,143 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:24:14,143 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]51355
2025-06-05T04:24:14,143 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:24:14,143 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:24:14,143 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,143 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,143 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:24:14,143 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:24:14,144 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:24:14,144 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454144
2025-06-05T04:24:14,144 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454144
2025-06-05T04:24:14,144 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454144
2025-06-05T04:24:14,144 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454144
2025-06-05T04:24:14,144 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:24:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:24:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:24:14,159 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:24:14,159 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:24:14,160 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:24:14,160 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:24:14,164 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,164 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:24:14,164 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,164 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,164 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,164 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,164 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,165 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:24:14,165 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:24:14,165 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,165 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,165 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,165 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,165 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2025-06-05T04:24:14,165 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2025-06-05T04:24:14,168 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:24:14,168 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:24:14,173 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:24:14,173 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:24:14,173 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:24:14,173 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:24:14,180 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:24:14,180 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:24:14,183 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:24:14,183 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:24:14,183 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,183 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:24:14,183 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,183 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,183 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,184 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:24:14,184 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:24:14,184 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,184 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,184 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,184 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,184 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2025-06-05T04:24:14,184 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2025-06-05T04:24:14,192 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:24:14,192 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:24:14,192 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:24:14,192 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:24:14,204 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=51358
2025-06-05T04:24:14,205 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:24:14,209 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:24:14,209 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]51358
2025-06-05T04:24:14,209 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:24:14,209 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:24:14,209 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,209 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,209 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:24:14,209 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:24:14,210 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:24:14,210 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454210
2025-06-05T04:24:14,210 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454210
2025-06-05T04:24:14,210 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454210
2025-06-05T04:24:14,210 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454210
2025-06-05T04:24:14,210 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:24:14,232 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:24:14,232 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:24:14,234 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:24:14,234 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:24:14,244 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:24:14,244 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:24:14,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:24:14,253 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,253 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,253 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:24:14,253 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:24:14,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,253 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,253 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,254 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-06-05T04:24:14,254 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-06-05T04:24:14,263 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:24:14,263 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:24:14,263 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:24:14,263 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:24:14,303 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:24:14,303 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:24:14,314 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:24:14,315 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:24:14,322 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:24:14,323 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,323 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,323 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,323 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,323 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,323 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,323 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:24:14,323 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:24:14,323 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,323 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,323 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,323 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,323 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2025-06-05T04:24:14,323 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2025-06-05T04:24:14,331 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:24:14,331 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:24:14,331 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:24:14,331 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:24:14,377 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=51376
2025-06-05T04:24:14,377 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:24:14,381 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:24:14,381 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]51376
2025-06-05T04:24:14,381 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:24:14,381 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:24:14,382 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,382 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,382 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:24:14,382 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:24:14,382 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:24:14,382 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454382
2025-06-05T04:24:14,382 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454382
2025-06-05T04:24:14,382 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454382
2025-06-05T04:24:14,382 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454382
2025-06-05T04:24:14,383 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:24:14,396 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:24:14,396 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:24:14,402 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=51375
2025-06-05T04:24:14,402 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]51375
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:24:14,407 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,407 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:24:14,407 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454407
2025-06-05T04:24:14,407 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097454407
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454407
2025-06-05T04:24:14,407 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097454407
2025-06-05T04:24:14,408 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:24:14,423 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:24:14,423 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:24:14,468 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:24:14,468 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:24:14,468 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:24:14,468 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:24:14,479 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:24:14,479 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:24:14,479 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:24:14,479 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:24:14,482 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:24:14,482 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,482 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,482 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,482 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:24:14,482 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:24:14,482 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:24:14,482 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,482 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:24:14,482 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:24:14,483 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:24:14,483 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:24:14,483 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:24:14,483 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:24:14,483 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,483 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:24:14,483 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,483 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:24:14,483 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,483 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,483 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,483 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,483 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:24:14,483 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:24:14,483 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:24:14,483 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,483 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:24:14,483 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,483 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:24:14,483 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2025-06-05T04:24:14,483 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2025-06-05T04:24:14,483 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2025-06-05T04:24:14,483 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2025-06-05T04:24:14,491 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:24:14,491 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:24:14,491 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:24:14,491 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:24:14,492 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:24:14,492 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:24:14,492 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:24:14,492 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:25:43,145 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,145 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,162 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,162 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,232 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,232 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,302 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,302 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,462 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,462 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,464 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,464 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:25:43,867 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=52261
2025-06-05T04:25:43,867 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:25:43,871 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:25:43,871 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]52261
2025-06-05T04:25:43,871 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:25:43,871 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:25:43,871 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:43,871 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:43,871 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:25:43,871 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:25:43,872 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:25:43,872 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097543872
2025-06-05T04:25:43,872 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097543872
2025-06-05T04:25:43,872 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097543872
2025-06-05T04:25:43,872 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097543872
2025-06-05T04:25:43,873 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:25:43,890 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:25:43,891 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:25:43,990 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:25:43,991 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:25:43,995 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=52266
2025-06-05T04:25:43,995 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:25:44,002 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:25:44,002 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:25:44,002 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:25:44,003 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]52266
2025-06-05T04:25:44,003 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:25:44,003 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:25:44,003 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,003 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,003 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:25:44,003 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:25:44,003 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:25:44,003 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544003
2025-06-05T04:25:44,003 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544003
2025-06-05T04:25:44,004 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544004
2025-06-05T04:25:44,004 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544004
2025-06-05T04:25:44,004 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:25:44,005 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/9047655cafcd4b0f82e243b4e6096541/onnx_handler.py", line 16, in initialize
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed:Load model /tmp/models/9047655cafcd4b0f82e243b4e6096541/model.onnx failed. File doesn't exist
2025-06-05T04:25:44,006 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,006 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,006 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,006 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,006 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,006 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,006 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:25:44,006 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:25:44,006 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,006 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,006 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,006 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds.
2025-06-05T04:25:44,006 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds.
2025-06-05T04:25:44,018 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:25:44,018 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:25:44,018 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:25:44,018 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:25:44,019 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:25:44,019 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:25:44,058 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=52271
2025-06-05T04:25:44,058 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:25:44,062 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:25:44,062 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]52271
2025-06-05T04:25:44,062 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:25:44,062 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:25:44,062 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,062 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,062 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:25:44,062 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:25:44,063 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:25:44,063 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544063
2025-06-05T04:25:44,063 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544063
2025-06-05T04:25:44,063 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544063
2025-06-05T04:25:44,063 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544063
2025-06-05T04:25:44,063 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:25:44,077 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:25:44,077 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:25:44,095 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=52276
2025-06-05T04:25:44,096 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:25:44,100 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:25:44,100 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]52276
2025-06-05T04:25:44,100 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:25:44,100 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:25:44,100 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,100 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,100 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:25:44,100 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:25:44,100 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:25:44,101 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544101
2025-06-05T04:25:44,101 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544101
2025-06-05T04:25:44,101 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544101
2025-06-05T04:25:44,101 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544101
2025-06-05T04:25:44,101 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:25:44,109 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:25:44,109 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:25:44,114 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:25:44,114 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:25:44,120 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:25:44,120 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/onnx_handler.py", line 16, in initialize
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:25:44,123 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed:Load model /tmp/models/9dd2a99fbf67485ca6c3d4590a4db7e8/model.onnx failed. File doesn't exist
2025-06-05T04:25:44,123 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,123 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,123 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,123 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,123 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,123 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,124 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:25:44,124 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:25:44,124 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,124 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,124 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,124 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,124 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds.
2025-06-05T04:25:44,124 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds.
2025-06-05T04:25:44,127 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:25:44,127 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:25:44,133 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:25:44,133 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:25:44,133 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:25:44,133 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:25:44,139 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:25:44,139 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:25:44,151 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:25:44,151 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/3efce99e1632401d94785e0e8d784f1f/onnx_handler.py", line 16, in initialize
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:25:44,151 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed:Load model /tmp/models/3efce99e1632401d94785e0e8d784f1f/model.onnx failed. File doesn't exist
2025-06-05T04:25:44,151 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,151 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,151 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,151 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,152 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:25:44,152 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:25:44,152 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,152 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,152 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,152 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,152 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2025-06-05T04:25:44,152 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2025-06-05T04:25:44,162 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:25:44,162 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:25:44,162 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:25:44,162 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:25:44,164 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:25:44,164 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:25:44,175 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:25:44,175 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/onnx_handler.py", line 16, in initialize
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:25:44,179 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:25:44,180 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:25:44,180 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed:Load model /tmp/models/e1be07fd09144e9d9963e3271c4ae4fa/model.onnx failed. File doesn't exist
2025-06-05T04:25:44,180 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,180 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,180 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,180 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,180 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,180 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,180 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:25:44,180 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:25:44,180 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,180 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,181 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,181 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,181 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds.
2025-06-05T04:25:44,181 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds.
2025-06-05T04:25:44,190 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:25:44,190 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:25:44,190 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:25:44,190 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:25:44,249 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=52337
2025-06-05T04:25:44,249 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:25:44,253 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:25:44,253 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]52337
2025-06-05T04:25:44,253 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:25:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:25:44,254 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,254 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,254 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:25:44,254 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:25:44,254 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:25:44,254 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544254
2025-06-05T04:25:44,254 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544254
2025-06-05T04:25:44,254 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544254
2025-06-05T04:25:44,254 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544254
2025-06-05T04:25:44,255 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:25:44,268 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:25:44,268 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:25:44,292 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=52336
2025-06-05T04:25:44,292 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:25:44,297 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:25:44,297 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]52336
2025-06-05T04:25:44,297 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:25:44,297 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:25:44,297 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,297 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:25:44,297 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:25:44,297 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:25:44,297 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:25:44,297 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544297
2025-06-05T04:25:44,297 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097544297
2025-06-05T04:25:44,298 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544298
2025-06-05T04:25:44,298 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097544298
2025-06-05T04:25:44,298 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:25:44,311 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:25:44,311 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:25:44,330 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:25:44,330 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:25:44,343 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:25:44,343 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:25:44,346 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/457b1438d72f425b8f0be6b72fee1276/onnx_handler.py", line 16, in initialize
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:25:44,347 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed:Load model /tmp/models/457b1438d72f425b8f0be6b72fee1276/model.onnx failed. File doesn't exist
2025-06-05T04:25:44,347 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,347 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,347 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,347 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,347 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,347 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,348 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:25:44,348 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:25:44,348 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,348 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,348 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,348 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,348 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 144 seconds.
2025-06-05T04:25:44,348 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 144 seconds.
2025-06-05T04:25:44,357 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:25:44,357 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:25:44,357 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:25:44,357 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:25:44,358 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:25:44,358 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:25:44,369 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:25:44,369 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:25:44,371 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:25:44,371 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:25:44,371 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/onnx_handler.py", line 16, in initialize
2025-06-05T04:25:44,372 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:25:44,372 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:25:44,372 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,372 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed:Load model /tmp/models/da8fe388e3d246d6ba49a12b4ad8a7d1/model.onnx failed. File doesn't exist
2025-06-05T04:25:44,372 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,372 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:25:44,372 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:25:44,372 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:25:44,372 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,372 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:25:44,372 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,372 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds.
2025-06-05T04:25:44,372 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds.
2025-06-05T04:25:44,380 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:25:44,380 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:25:44,380 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:25:44,380 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:26:08,315 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:26:08,315 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:26:08,316 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:26:08,316 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:26:08,346 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:26:08,346 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:26:08,578 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:26:08,578 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:26:08,583 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:26:08,583 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:26:08,595 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T04:26:08,595 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T04:26:19,966 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:26:19,966 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:26:19,967 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:26:19,967 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:26:19,967 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:26:19,967 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:26:19,967 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:26:19,967 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:26:19,970 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T04:26:19,970 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T04:26:19,971 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,971 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:20,039 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:26:20,039 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:26:20,040 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:26:20,040 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:26:20,040 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:26:20,040 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:26:20,040 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:26:20,040 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:26:20,040 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T04:26:20,040 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T04:26:20,041 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:20,041 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:20,112 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:26:20,112 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:26:20,112 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:26:20,112 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:26:20,112 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:26:20,112 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:26:20,112 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:26:20,112 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:26:20,113 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T04:26:20,113 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T04:26:20,113 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:20,113 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:08,907 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:26:08,907 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:26:08,907 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:26:08,907 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:26:08,907 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:26:08,907 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:26:08,907 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:26:08,907 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:26:08,908 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T04:26:08,908 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T04:26:08,908 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:08,908 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:08,986 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:26:08,986 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:26:08,987 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:26:08,987 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:26:08,987 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:26:08,987 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:26:08,987 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:26:08,987 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:26:08,989 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T04:26:08,989 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T04:26:09,009 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:09,009 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:09,093 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:26:09,093 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:26:09,093 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:26:09,093 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:26:09,093 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:26:09,093 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:26:09,093 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:26:09,093 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:26:09,094 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:09,094 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:09,096 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:26:09,096 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:26:09,159 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T04:26:09,159 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T04:26:09,160 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T04:26:09,160 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T04:26:09,166 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T04:26:09,166 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T04:26:09,166 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T04:26:09,166 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T04:26:09,166 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T04:26:09,166 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T04:26:09,508 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=52899
2025-06-05T04:26:09,509 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:26:09,515 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:09,515 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]52899
2025-06-05T04:26:09,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:09,516 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:09,517 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,517 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,521 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:09,521 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:09,530 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:26:09,534 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569534
2025-06-05T04:26:09,534 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569534
2025-06-05T04:26:09,535 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569535
2025-06-05T04:26:09,535 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569535
2025-06-05T04:26:09,589 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:26:09,591 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:09,591 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:09,624 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=52903
2025-06-05T04:26:09,625 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:26:09,629 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:09,630 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]52903
2025-06-05T04:26:09,630 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:09,630 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,630 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:09,630 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,630 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:09,630 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:09,632 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:26:09,632 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569632
2025-06-05T04:26:09,632 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569632
2025-06-05T04:26:09,632 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569632
2025-06-05T04:26:09,632 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569632
2025-06-05T04:26:09,645 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:26:09,660 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:09,660 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:09,684 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:09,685 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:09,698 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:09,698 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:09,704 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:09,704 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:09,704 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:09,705 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:09,705 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:09,705 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:09,705 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:09,705 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:09,705 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:09,705 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:09,706 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:09,706 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:09,706 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:09,706 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:09,706 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:09,706 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:09,707 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:09,707 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:09,707 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:09,708 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:09,708 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:09,708 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:09,708 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:26:09,706 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:09,706 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:09,718 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:09,718 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:09,718 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:09,718 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:09,718 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097569718
2025-06-05T04:26:09,718 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097569718
2025-06-05T04:26:09,719 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:26:09,719 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:26:09,724 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:09,724 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:09,728 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:09,728 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:09,728 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:09,728 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:09,736 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:09,736 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:09,738 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:09,738 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:09,738 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:09,739 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:09,739 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:09,739 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:09,739 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:09,739 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:09,739 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:09,740 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:09,740 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:09,739 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:09,740 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:09,740 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:09,740 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097569740
2025-06-05T04:26:09,740 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097569740
2025-06-05T04:26:09,740 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:09,740 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:09,740 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:09,740 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:26:09,740 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:26:09,740 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:09,741 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:26:09,741 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:09,741 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:09,745 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=52908
2025-06-05T04:26:09,745 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:26:09,750 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:09,750 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:09,751 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:09,751 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]52908
2025-06-05T04:26:09,751 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:09,751 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:09,751 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,751 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,752 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:09,752 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:09,752 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:26:09,752 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569752
2025-06-05T04:26:09,752 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569752
2025-06-05T04:26:09,753 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569753
2025-06-05T04:26:09,753 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569753
2025-06-05T04:26:09,758 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:26:09,772 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:09,772 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:09,786 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=52914
2025-06-05T04:26:09,787 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:26:09,791 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:09,791 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]52914
2025-06-05T04:26:09,791 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:09,791 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:09,791 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,791 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,791 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:09,791 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:09,792 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569792
2025-06-05T04:26:09,792 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569792
2025-06-05T04:26:09,792 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569792
2025-06-05T04:26:09,792 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:26:09,792 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569792
2025-06-05T04:26:09,798 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:26:09,812 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:09,813 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:09,851 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:09,851 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:09,860 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:09,860 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:09,862 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:09,863 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:09,865 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:09,866 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:09,866 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:09,867 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:09,866 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:09,867 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:09,867 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:09,867 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:26:09,867 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:09,867 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:09,867 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:09,867 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:09,868 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:09,868 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:09,868 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:09,868 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:09,868 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097569868
2025-06-05T04:26:09,868 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097569868
2025-06-05T04:26:09,868 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:26:09,868 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:26:09,872 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:09,872 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:09,875 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:09,875 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:09,875 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:09,876 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:09,876 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:09,876 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:09,876 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:09,876 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:09,876 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:09,877 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:09,877 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:09,877 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:09,877 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:26:09,876 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:09,877 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:09,877 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:09,877 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:09,877 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:09,877 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:09,877 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:09,877 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:09,877 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:09,877 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097569877
2025-06-05T04:26:09,877 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097569877
2025-06-05T04:26:09,878 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:26:09,878 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:26:09,885 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:09,885 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:09,885 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:09,885 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:09,905 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=52951
2025-06-05T04:26:09,905 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=52946
2025-06-05T04:26:09,905 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:26:09,905 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:26:09,909 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:09,909 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:09,909 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]52951
2025-06-05T04:26:09,909 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]52946
2025-06-05T04:26:09,909 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:09,909 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:09,909 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,909 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:09,909 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,909 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:09,909 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:09,909 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:09,909 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,909 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:26:09,909 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:09,909 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:09,910 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:26:09,910 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569910
2025-06-05T04:26:09,910 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569910
2025-06-05T04:26:09,910 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569910
2025-06-05T04:26:09,910 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:26:09,910 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569910
2025-06-05T04:26:09,910 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569910
2025-06-05T04:26:09,910 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097569910
2025-06-05T04:26:09,910 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569910
2025-06-05T04:26:09,910 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097569910
2025-06-05T04:26:09,917 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:26:09,917 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:26:09,931 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:09,931 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:09,932 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:09,932 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:10,004 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:10,004 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:10,004 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:10,004 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:10,014 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:10,014 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:10,014 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:10,014 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:10,017 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:10,017 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:10,018 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:10,018 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:10,018 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:10,018 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:10,018 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:10,018 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:10,018 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:10,018 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:10,018 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:10,018 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:10,018 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:10,018 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:10,018 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:10,018 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:10,018 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:10,018 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097570018
2025-06-05T04:26:10,018 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097570018
2025-06-05T04:26:10,018 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:10,019 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:10,019 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:10,020 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:10,020 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:10,020 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:10,020 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:10,019 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:10,019 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:10,020 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:10,020 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:26:10,020 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:10,020 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:26:10,020 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:10,020 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:10,020 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:10,020 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:10,020 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:10,020 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:10,021 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:10,021 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:10,021 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:10,021 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:10,021 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097570021
2025-06-05T04:26:10,021 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097570021
2025-06-05T04:26:10,021 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:26:10,021 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:26:10,027 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:10,027 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:10,028 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:10,028 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:10,028 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:10,028 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:10,720 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:10,720 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:10,741 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:10,741 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:10,869 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:10,869 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:10,878 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:10,878 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:11,019 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:11,019 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:11,022 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:11,022 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:11,455 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=53233
2025-06-05T04:26:11,455 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:26:11,466 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:11,469 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]53233
2025-06-05T04:26:11,469 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:11,469 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,469 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,470 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:11,470 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:11,470 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:11,473 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:26:11,474 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571474
2025-06-05T04:26:11,474 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571474
2025-06-05T04:26:11,474 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571474
2025-06-05T04:26:11,474 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571474
2025-06-05T04:26:11,499 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:26:11,499 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=53230
2025-06-05T04:26:11,500 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:26:11,507 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:11,507 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]53230
2025-06-05T04:26:11,507 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:11,507 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:11,507 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,507 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,507 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:11,507 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:11,508 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571508
2025-06-05T04:26:11,508 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571508
2025-06-05T04:26:11,508 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571508
2025-06-05T04:26:11,508 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:26:11,508 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571508
2025-06-05T04:26:11,511 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:11,512 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:11,526 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:26:11,553 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:11,553 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:11,603 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:11,603 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:11,610 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:11,610 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:11,615 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:11,615 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:11,620 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,620 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:11,620 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,620 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:11,620 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:11,620 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,621 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:11,621 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:11,621 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:11,621 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:11,621 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,621 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:26:11,621 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:26:11,624 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:11,624 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:11,627 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:11,628 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:11,628 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:11,628 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,628 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,628 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:26:11,628 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,628 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,628 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:11,628 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:11,628 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,628 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,628 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,628 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,629 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:26:11,629 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:26:11,631 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=53236
2025-06-05T04:26:11,632 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:26:11,632 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=53239
2025-06-05T04:26:11,633 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:26:11,634 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:11,634 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:11,636 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:11,636 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]53236
2025-06-05T04:26:11,637 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:11,637 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:11,637 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,637 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,637 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:11,637 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:11,637 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:11,637 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]53239
2025-06-05T04:26:11,637 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:11,637 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:11,638 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,638 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,638 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:11,638 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:11,638 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571638
2025-06-05T04:26:11,638 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571638
2025-06-05T04:26:11,638 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:26:11,638 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571638
2025-06-05T04:26:11,638 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571638
2025-06-05T04:26:11,639 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571639
2025-06-05T04:26:11,639 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571639
2025-06-05T04:26:11,639 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571639
2025-06-05T04:26:11,639 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:26:11,639 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571639
2025-06-05T04:26:11,642 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:11,642 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:11,642 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:11,642 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:11,653 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:26:11,653 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:26:11,667 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:11,667 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:11,667 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:11,668 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:11,742 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:11,743 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:11,742 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:11,743 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:11,755 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:11,755 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:11,755 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:11,755 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:11,758 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:11,759 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:11,759 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:11,759 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:11,759 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,760 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:11,760 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:11,760 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:11,760 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,760 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:11,760 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:26:11,760 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,760 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:11,760 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:11,760 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,760 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,760 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,760 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:11,761 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:11,761 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:26:11,761 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:11,761 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:11,761 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:11,761 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:11,761 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,761 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:26:11,761 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,762 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:11,762 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:11,762 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,762 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,762 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,762 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,762 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:26:11,762 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:26:11,771 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:11,771 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:11,771 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:11,771 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:11,772 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:11,772 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:11,772 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:11,772 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:11,827 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=53296
2025-06-05T04:26:11,828 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:26:11,828 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=53297
2025-06-05T04:26:11,828 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:26:11,832 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:11,832 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]53296
2025-06-05T04:26:11,832 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:11,832 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:11,832 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,832 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,832 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:11,832 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:11,832 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:11,832 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]53297
2025-06-05T04:26:11,832 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:11,832 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:11,832 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,832 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:11,832 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:11,832 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:11,833 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:26:11,833 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571833
2025-06-05T04:26:11,833 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571833
2025-06-05T04:26:11,833 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571833
2025-06-05T04:26:11,833 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571833
2025-06-05T04:26:11,833 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571833
2025-06-05T04:26:11,833 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097571833
2025-06-05T04:26:11,833 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:26:11,833 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571833
2025-06-05T04:26:11,833 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097571833
2025-06-05T04:26:11,846 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:26:11,846 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:26:11,859 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:11,859 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:11,859 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:11,859 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:11,952 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:11,952 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:11,952 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:11,953 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:11,963 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:11,963 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:11,963 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:11,963 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:11,965 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:11,966 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:11,966 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:11,966 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,966 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:11,966 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:11,966 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:11,966 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:11,967 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:11,967 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:11,967 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:11,967 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,967 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:11,967 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:11,967 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:11,967 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:11,967 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:11,967 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:11,968 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,968 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:11,968 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,968 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:11,968 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:11,968 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:11,969 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,969 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:11,969 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,969 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:11,969 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:26:11,969 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:26:11,976 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:11,976 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:11,977 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:11,977 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:11,977 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:11,977 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:12,622 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,622 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,629 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,629 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,761 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,761 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,763 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,763 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,968 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,968 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,969 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:12,969 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:13,335 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=53512
2025-06-05T04:26:13,335 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:26:13,347 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=53509
2025-06-05T04:26:13,347 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:26:13,359 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:13,361 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:13,363 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]53512
2025-06-05T04:26:13,364 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:13,364 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:13,364 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:13,364 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:13,364 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:13,364 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:13,367 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]53509
2025-06-05T04:26:13,368 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:26:13,368 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097573368
2025-06-05T04:26:13,368 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097573368
2025-06-05T04:26:13,368 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097573368
2025-06-05T04:26:13,368 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097573368
2025-06-05T04:26:13,374 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:13,374 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:13,374 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:13,374 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:13,374 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:13,374 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:13,378 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:26:13,378 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097573378
2025-06-05T04:26:13,378 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097573378
2025-06-05T04:26:13,379 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097573379
2025-06-05T04:26:13,379 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097573379
2025-06-05T04:26:13,393 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:26:13,399 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:26:13,413 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:13,413 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:13,415 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:13,415 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:13,499 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:13,499 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:13,499 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:13,499 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:13,511 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:13,512 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:13,512 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:13,512 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:13,520 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:13,520 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:13,521 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:13,521 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:13,521 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:13,521 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:13,521 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:13,520 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:13,521 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:26:13,520 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:13,520 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:13,520 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:13,521 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:13,521 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:13,522 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:13,522 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:13,521 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:13,521 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:13,522 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:13,522 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:13,522 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:13,522 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:13,522 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:13,522 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:13,522 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:13,522 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:13,522 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:13,523 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:13,523 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:13,522 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:13,523 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:13,523 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:13,523 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2025-06-05T04:26:13,523 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-06-05T04:26:13,523 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2025-06-05T04:26:13,523 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-06-05T04:26:13,538 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:13,538 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:13,538 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:13,538 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:13,540 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:13,540 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:13,540 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:13,540 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:13,554 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=53524
2025-06-05T04:26:13,554 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:26:13,558 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:13,558 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]53524
2025-06-05T04:26:13,558 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:13,559 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:13,559 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:13,559 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:13,559 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:13,559 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:13,562 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097573562
2025-06-05T04:26:13,561 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:26:13,562 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097573562
2025-06-05T04:26:13,562 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097573562
2025-06-05T04:26:13,562 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097573562
2025-06-05T04:26:13,571 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:26:13,573 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=53521
2025-06-05T04:26:13,573 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:26:13,580 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:13,580 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]53521
2025-06-05T04:26:13,581 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:13,581 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:13,581 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:13,581 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:13,581 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:13,581 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:13,582 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:26:13,582 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097573582
2025-06-05T04:26:13,582 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097573582
2025-06-05T04:26:13,582 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097573582
2025-06-05T04:26:13,582 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097573582
2025-06-05T04:26:13,586 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:13,587 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:13,588 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:26:13,602 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:13,602 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:13,663 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:13,663 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:13,663 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:13,663 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:13,674 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:13,674 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:13,674 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:13,674 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:13,677 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:13,677 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:13,677 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:13,677 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:13,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:26:13,678 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:13,678 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:13,678 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:13,678 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:13,678 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:13,678 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:13,678 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:13,679 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:13,679 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:13,679 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:13,679 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:13,679 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:13,679 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:13,679 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:13,679 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:13,679 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:13,679 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:13,679 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:13,680 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:13,680 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:13,679 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:13,680 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:13,680 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:13,679 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:26:13,680 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:13,680 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:13,680 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:13,680 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:13,680 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2025-06-05T04:26:13,680 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2025-06-05T04:26:13,680 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2025-06-05T04:26:13,680 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2025-06-05T04:26:13,689 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:13,689 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:13,689 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:13,689 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:24,964 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:24,964 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:24,964 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:24,964 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:24,966 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=53584
2025-06-05T04:26:24,966 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:26:24,971 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:24,971 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]53584
2025-06-05T04:26:24,971 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:24,971 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:24,971 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:24,971 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:24,971 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:24,971 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:24,972 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:26:24,972 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097584972
2025-06-05T04:26:24,972 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097584972
2025-06-05T04:26:24,973 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097584973
2025-06-05T04:26:24,973 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097584973
2025-06-05T04:26:24,978 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:26:24,991 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:24,992 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:25,017 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=53581
2025-06-05T04:26:25,018 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:26:25,022 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:25,022 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]53581
2025-06-05T04:26:25,022 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:25,022 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:25,022 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:25,022 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:25,022 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:25,022 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:25,023 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:26:25,023 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097585023
2025-06-05T04:26:25,023 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097585023
2025-06-05T04:26:25,023 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097585023
2025-06-05T04:26:25,023 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097585023
2025-06-05T04:26:25,028 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:26:25,042 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:25,042 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:25,073 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:25,073 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:25,083 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:25,084 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:25,084 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:25,085 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:25,087 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:25,087 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:25,087 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:26:25,087 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:25,087 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:25,087 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:25,087 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:25,087 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:25,087 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:25,088 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:25,088 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:25,088 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:25,088 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:25,088 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2025-06-05T04:26:25,088 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2025-06-05T04:26:25,096 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:25,096 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:25,096 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:25,096 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:25,096 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:25,096 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:25,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:26:25,104 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:25,104 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:25,104 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:25,104 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:25,104 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:25,104 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:25,104 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:25,104 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:25,104 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:25,104 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:25,105 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:25,105 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:25,105 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2025-06-05T04:26:25,105 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2025-06-05T04:26:25,113 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:25,113 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:25,113 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:25,113 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:14,058 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:34254 "GET / HTTP/1.1" 405 2
2025-06-05T04:26:14,059 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749097574
2025-06-05T04:26:15,503 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,503 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,503 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,503 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,659 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,659 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,659 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,659 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,793 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,793 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,810 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:15,810 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:16,214 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=53815
2025-06-05T04:26:16,217 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:26:16,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:16,225 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]53815
2025-06-05T04:26:16,226 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:16,226 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,226 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,226 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:16,226 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:16,226 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:16,227 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:26:16,227 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576227
2025-06-05T04:26:16,227 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576227
2025-06-05T04:26:16,227 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576227
2025-06-05T04:26:16,227 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576227
2025-06-05T04:26:16,267 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:26:16,267 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:16,267 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:16,278 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=53814
2025-06-05T04:26:16,278 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:26:16,285 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:16,286 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]53814
2025-06-05T04:26:16,286 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:16,287 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,287 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,287 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:16,287 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:16,287 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:16,288 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:26:16,288 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576288
2025-06-05T04:26:16,288 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576288
2025-06-05T04:26:16,288 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576288
2025-06-05T04:26:16,288 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576288
2025-06-05T04:26:16,319 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:26:16,337 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:16,337 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:16,360 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:16,360 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:16,373 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:16,373 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:16,376 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:16,377 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:16,377 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:16,377 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:16,378 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,378 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:16,378 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,378 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:16,378 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:16,378 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:16,378 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:16,378 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,378 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,378 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:26:16,378 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:16,378 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:16,379 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,379 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,379 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,379 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,379 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-06-05T04:26:16,379 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-06-05T04:26:16,388 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:16,388 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:16,388 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:16,388 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:16,396 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:16,396 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:16,407 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:16,407 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:16,410 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:26:16,410 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,410 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,410 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,410 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,410 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,410 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,411 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:16,411 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:16,411 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,411 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,411 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,411 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,411 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2025-06-05T04:26:16,411 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2025-06-05T04:26:16,420 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:16,420 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:16,420 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:16,420 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:16,476 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=53821
2025-06-05T04:26:16,477 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:26:16,482 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:16,483 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]53821
2025-06-05T04:26:16,483 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:16,483 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:16,483 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,483 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,483 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:16,483 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:16,484 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:26:16,484 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576484
2025-06-05T04:26:16,484 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576484
2025-06-05T04:26:16,484 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576484
2025-06-05T04:26:16,484 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576484
2025-06-05T04:26:16,496 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:26:16,511 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:16,511 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=53820
2025-06-05T04:26:16,511 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:16,511 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:26:16,515 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:16,515 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]53820
2025-06-05T04:26:16,515 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:16,515 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:16,515 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,515 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,515 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:16,515 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:16,516 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:26:16,516 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576516
2025-06-05T04:26:16,516 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576516
2025-06-05T04:26:16,516 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576516
2025-06-05T04:26:16,516 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576516
2025-06-05T04:26:16,527 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:26:16,542 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:16,543 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:16,599 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=53880
2025-06-05T04:26:16,599 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:26:16,603 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:16,603 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]53880
2025-06-05T04:26:16,603 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:16,603 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:16,603 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,603 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,603 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:16,603 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:16,604 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:26:16,604 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576604
2025-06-05T04:26:16,604 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576604
2025-06-05T04:26:16,604 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576604
2025-06-05T04:26:16,604 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576604
2025-06-05T04:26:16,614 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:16,614 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:16,614 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:16,614 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:16,615 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:26:16,623 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:16,623 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:16,623 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:16,623 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:16,624 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=53882
2025-06-05T04:26:16,624 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:16,626 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:16,626 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:16,626 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,626 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,626 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,627 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:16,626 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:16,627 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:16,627 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:16,627 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:16,627 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:16,627 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:16,627 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:16,627 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:16,627 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:16,627 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:16,627 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,627 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:26:16,627 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:16,627 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,627 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,627 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,627 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:16,627 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:16,627 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,627 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,627 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,627 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,627 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,628 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:16,628 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:26:16,628 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:16,628 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,628 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,628 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,628 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,628 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2025-06-05T04:26:16,628 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2025-06-05T04:26:16,628 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2025-06-05T04:26:16,628 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2025-06-05T04:26:16,629 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:16,629 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]53882
2025-06-05T04:26:16,629 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:16,629 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,629 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:16,629 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:16,629 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:16,629 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:16,630 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:26:16,630 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576630
2025-06-05T04:26:16,630 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097576630
2025-06-05T04:26:16,630 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576630
2025-06-05T04:26:16,630 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097576630
2025-06-05T04:26:16,633 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:16,633 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:16,639 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:16,639 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:16,639 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:16,639 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:16,639 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:16,639 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:16,639 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:16,639 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:16,642 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:26:16,656 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:16,656 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:16,727 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:16,727 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:16,727 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:16,727 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:16,738 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:16,738 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:16,738 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:16,738 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:16,741 [INFO ] epollEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:16,741 [INFO ] epollEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:16,741 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,741 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,741 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:16,741 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:16,741 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:26:16,742 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:16,742 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:16,742 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,742 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,742 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,742 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2025-06-05T04:26:16,742 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:16,743 [INFO ] epollEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:16,743 [INFO ] epollEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:16,743 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,743 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:26:16,743 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:16,744 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,744 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:16,744 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:16,744 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:16,744 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,744 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:16,744 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,744 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:16,744 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2025-06-05T04:26:16,744 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2025-06-05T04:26:16,751 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:16,751 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:16,751 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:16,751 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:16,752 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:16,752 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:16,752 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:16,752 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:19,405 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,405 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,436 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,436 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,653 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,653 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,653 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,653 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,767 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,767 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,769 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:19,769 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:20,170 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=54115
2025-06-05T04:26:20,170 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:26:20,178 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:20,179 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]54115
2025-06-05T04:26:20,179 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:20,179 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:20,179 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,179 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,179 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:20,179 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:20,180 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:26:20,180 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580180
2025-06-05T04:26:20,180 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580180
2025-06-05T04:26:20,180 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580180
2025-06-05T04:26:20,180 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580180
2025-06-05T04:26:20,201 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:26:20,209 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=54112
2025-06-05T04:26:20,210 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:26:20,219 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:20,220 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]54112
2025-06-05T04:26:20,221 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,220 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:20,221 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,221 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:20,221 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:20,221 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:20,224 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:26:20,224 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580224
2025-06-05T04:26:20,224 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580224
2025-06-05T04:26:20,225 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580225
2025-06-05T04:26:20,225 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580225
2025-06-05T04:26:20,229 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:20,229 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:20,262 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:26:20,273 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:20,273 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:20,336 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:20,336 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:20,336 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:20,336 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:20,348 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:20,348 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:20,348 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:20,348 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:20,353 [INFO ] epollEventLoopGroup-5-26 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,353 [INFO ] epollEventLoopGroup-5-25 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:20,353 [INFO ] epollEventLoopGroup-5-26 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:20,353 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:20,353 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:20,353 [INFO ] epollEventLoopGroup-5-25 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:20,353 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:20,353 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:20,353 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:20,353 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:20,354 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:20,354 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:20,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:26:20,354 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,354 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,354 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,354 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,354 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,354 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,354 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,354 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,354 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:20,354 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:20,354 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,354 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,354 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,354 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,355 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-06-05T04:26:20,355 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-06-05T04:26:20,355 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2025-06-05T04:26:20,355 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2025-06-05T04:26:20,364 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:20,364 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:20,364 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:20,364 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:20,365 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:20,365 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:20,365 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:20,365 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:20,488 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=54118
2025-06-05T04:26:20,488 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:26:20,492 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:20,492 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]54118
2025-06-05T04:26:20,492 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:20,492 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:20,492 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,492 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,493 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:20,493 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:20,493 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=54119
2025-06-05T04:26:20,493 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:26:20,494 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:26:20,494 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580494
2025-06-05T04:26:20,494 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580494
2025-06-05T04:26:20,494 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580494
2025-06-05T04:26:20,494 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580494
2025-06-05T04:26:20,498 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:20,499 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]54119
2025-06-05T04:26:20,499 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:20,499 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:20,499 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,499 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,499 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:20,499 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:20,500 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:26:20,500 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:26:20,500 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580500
2025-06-05T04:26:20,500 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580500
2025-06-05T04:26:20,500 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580500
2025-06-05T04:26:20,500 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580500
2025-06-05T04:26:20,506 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:26:20,516 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:20,516 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:20,521 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:20,521 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:20,548 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=54179
2025-06-05T04:26:20,548 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:26:20,552 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:20,552 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]54179
2025-06-05T04:26:20,552 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:20,552 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:20,552 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,552 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,552 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:20,552 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:20,553 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:26:20,553 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580553
2025-06-05T04:26:20,553 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580553
2025-06-05T04:26:20,554 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580554
2025-06-05T04:26:20,554 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580554
2025-06-05T04:26:20,559 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:26:20,573 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:20,574 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:20,577 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:20,577 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:20,577 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:20,577 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:20,587 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=54178
2025-06-05T04:26:20,588 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:26:20,588 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:20,588 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:20,588 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:20,588 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:20,594 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:20,594 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:20,594 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:20,594 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:20,594 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:20,594 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:20,594 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:20,595 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:26:20,595 [INFO ] epollEventLoopGroup-5-27 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,595 [INFO ] epollEventLoopGroup-5-27 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,595 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,595 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,595 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:20,595 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]54178
2025-06-05T04:26:20,595 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,595 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:20,595 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,595 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:20,595 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,595 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:20,595 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:20,595 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:20,595 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:20,595 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:20,595 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,595 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,595 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,595 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:20,596 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:20,596 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:20,596 [INFO ] epollEventLoopGroup-5-28 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:20,596 [INFO ] epollEventLoopGroup-5-28 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,596 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:26:20,596 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,596 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,596 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,596 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,597 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:20,597 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:20,597 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,597 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,597 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,597 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,597 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2025-06-05T04:26:20,597 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:26:20,597 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2025-06-05T04:26:20,597 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580597
2025-06-05T04:26:20,597 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097580597
2025-06-05T04:26:20,597 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580597
2025-06-05T04:26:20,597 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097580597
2025-06-05T04:26:20,604 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:26:20,606 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:20,606 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:20,606 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:20,606 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:20,607 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:20,607 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:20,607 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:20,607 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:20,619 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:20,619 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:20,661 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:20,661 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:20,668 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:20,668 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:20,672 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:20,672 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:20,674 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:26:20,674 [INFO ] epollEventLoopGroup-5-29 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,674 [INFO ] epollEventLoopGroup-5-29 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,674 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,674 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,674 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,674 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,674 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:20,674 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:20,675 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,675 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,675 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,675 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,675 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2025-06-05T04:26:20,675 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2025-06-05T04:26:20,678 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:20,678 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:20,681 [INFO ] epollEventLoopGroup-5-30 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:20,681 [INFO ] epollEventLoopGroup-5-30 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:20,681 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:26:20,681 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,681 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:20,681 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,681 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:20,681 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:20,681 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:20,682 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,682 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:20,682 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,682 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:20,682 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2025-06-05T04:26:20,682 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2025-06-05T04:26:20,683 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:20,683 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:20,683 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:20,683 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:20,691 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:20,691 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:20,691 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:20,691 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:25,288 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,288 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,288 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,288 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,528 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,528 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,529 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,529 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,607 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,607 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,614 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,614 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:25,914 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=54425
2025-06-05T04:26:25,915 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:26:25,921 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:25,922 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]54425
2025-06-05T04:26:25,922 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:25,922 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:25,922 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:25,922 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:25,922 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:25,922 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:25,926 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:26:25,926 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097585926
2025-06-05T04:26:25,926 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097585926
2025-06-05T04:26:25,926 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097585926
2025-06-05T04:26:25,926 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097585926
2025-06-05T04:26:25,973 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:26:25,980 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:25,980 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:26,015 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=54424
2025-06-05T04:26:26,015 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:26:26,022 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:26,022 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]54424
2025-06-05T04:26:26,022 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:26,022 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:26,022 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,022 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,022 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:26,022 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:26,025 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:26:26,028 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586028
2025-06-05T04:26:26,028 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586028
2025-06-05T04:26:26,028 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586028
2025-06-05T04:26:26,028 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586028
2025-06-05T04:26:26,041 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:26:26,058 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:26,058 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:26,106 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:26,106 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:26,111 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:26,111 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:26,118 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:26,118 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:26,123 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:26,124 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:26,124 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:26:26,123 [INFO ] epollEventLoopGroup-5-31 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,123 [INFO ] epollEventLoopGroup-5-31 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,124 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,124 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,124 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,124 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,124 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:26,124 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:26,124 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,124 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,124 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,124 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,124 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2025-06-05T04:26:26,124 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2025-06-05T04:26:26,125 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:26,125 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:26,128 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:26:26,129 [INFO ] epollEventLoopGroup-5-32 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,129 [INFO ] epollEventLoopGroup-5-32 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,129 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,129 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,129 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,129 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,129 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:26,129 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:26,129 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,129 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,129 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,129 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,130 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-06-05T04:26:26,130 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-06-05T04:26:26,137 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:26,137 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:26,137 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:26,137 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:26,138 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:26,138 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:26,138 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:26,138 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:26,313 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=54490
2025-06-05T04:26:26,313 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:26:26,317 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:26,318 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]54490
2025-06-05T04:26:26,318 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:26,318 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:26,318 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,318 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,318 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:26,318 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:26,318 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=54433
2025-06-05T04:26:26,319 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:26:26,319 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:26:26,319 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586319
2025-06-05T04:26:26,319 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586319
2025-06-05T04:26:26,319 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586319
2025-06-05T04:26:26,319 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586319
2025-06-05T04:26:26,321 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=54430
2025-06-05T04:26:26,321 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:26:26,324 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:26,324 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]54433
2025-06-05T04:26:26,324 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:26,324 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:26,324 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,324 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,324 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:26,324 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:26,325 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:26:26,325 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:26:26,325 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586325
2025-06-05T04:26:26,325 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586325
2025-06-05T04:26:26,325 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586325
2025-06-05T04:26:26,325 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586325
2025-06-05T04:26:26,327 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:26,327 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]54430
2025-06-05T04:26:26,327 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:26,327 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:26,327 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,327 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,327 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:26,327 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:26,329 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:26:26,329 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586329
2025-06-05T04:26:26,329 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586329
2025-06-05T04:26:26,329 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586329
2025-06-05T04:26:26,329 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586329
2025-06-05T04:26:26,342 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:26:26,344 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:26:26,346 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:26,346 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:26,357 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:26,358 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:26,358 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:26,358 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:26,364 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=54493
2025-06-05T04:26:26,365 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:26:26,369 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:26,369 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]54493
2025-06-05T04:26:26,369 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:26,369 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:26,369 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,369 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:26,369 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:26,369 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:26,370 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:26:26,370 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586370
2025-06-05T04:26:26,370 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097586370
2025-06-05T04:26:26,370 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586370
2025-06-05T04:26:26,370 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097586370
2025-06-05T04:26:26,382 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:26:26,396 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:26,396 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:26,436 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:26,436 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:26,436 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:26,436 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:26,436 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:26,436 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:26,447 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:26,447 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:26,447 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:26,447 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:26,447 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:26,447 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:26,451 [INFO ] epollEventLoopGroup-5-34 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,451 [INFO ] epollEventLoopGroup-5-34 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:26,451 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,451 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:26,451 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:26:26,451 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,451 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,452 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:26,452 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:26,452 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:26,452 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,452 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:26,452 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,452 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:26,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:26,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:26,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:26,452 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:26,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:26,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:26,452 [INFO ] epollEventLoopGroup-5-33 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,452 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:26,452 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:26,452 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:26,452 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:26,452 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2025-06-05T04:26:26,452 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:26,452 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2025-06-05T04:26:26,452 [INFO ] epollEventLoopGroup-5-35 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:26,452 [INFO ] epollEventLoopGroup-5-35 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,452 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:26,452 [INFO ] epollEventLoopGroup-5-33 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:26,453 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:26,453 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,453 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:26,453 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:26,453 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:26,453 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:26,453 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,453 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:26,453 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:26,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:26:26,453 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:26,453 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:26,453 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,453 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,453 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,453 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,453 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:26,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:26:26,453 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,453 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,454 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,454 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,454 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2025-06-05T04:26:26,454 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2025-06-05T04:26:26,454 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2025-06-05T04:26:26,454 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2025-06-05T04:26:26,461 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:26,461 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:26,461 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:26,461 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:26,463 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:26,463 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:26,463 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:26,463 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:26,463 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:26,463 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:26,464 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:26,464 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:26,464 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:26,464 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:26,474 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:26,474 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:26,476 [INFO ] epollEventLoopGroup-5-36 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:26,476 [INFO ] epollEventLoopGroup-5-36 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:26,476 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:26:26,476 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,476 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:26,476 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,476 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:26,477 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:26,477 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:26,477 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,477 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:26,477 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,477 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:26,477 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2025-06-05T04:26:26,477 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2025-06-05T04:26:26,484 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:26,484 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:26,484 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:26,484 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:34,168 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,168 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,173 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,173 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,495 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,495 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,497 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,497 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,497 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,497 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,520 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,520 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:34,834 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=54754
2025-06-05T04:26:34,834 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:26:34,886 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:34,897 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]54754
2025-06-05T04:26:34,898 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:34,898 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:34,898 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:34,899 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:34,898 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:34,899 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:26:34,900 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097594900
2025-06-05T04:26:34,900 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:26:34,900 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097594900
2025-06-05T04:26:34,900 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097594900
2025-06-05T04:26:34,900 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097594900
2025-06-05T04:26:34,906 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=54757
2025-06-05T04:26:34,906 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:26:34,913 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:34,914 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]54757
2025-06-05T04:26:34,914 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:34,914 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:34,914 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:34,914 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:34,914 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:34,914 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:26:34,920 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:26:34,920 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:26:34,921 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097594921
2025-06-05T04:26:34,921 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097594921
2025-06-05T04:26:34,921 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097594921
2025-06-05T04:26:34,921 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097594921
2025-06-05T04:26:34,944 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:34,944 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:34,945 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:26:34,973 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:34,973 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:35,046 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:35,046 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:35,046 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:35,046 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:35,058 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:35,058 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:35,058 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:35,058 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:35,061 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:35,061 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:35,061 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:35,061 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:35,062 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:35,062 [INFO ] epollEventLoopGroup-5-38 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,062 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:35,062 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:35,062 [INFO ] epollEventLoopGroup-5-38 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,062 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:35,062 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:35,062 [INFO ] epollEventLoopGroup-5-37 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,062 [INFO ] epollEventLoopGroup-5-37 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,062 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:35,062 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:35,062 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:35,062 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:35,063 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:35,063 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:26:35,062 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,062 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,062 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,063 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:35,062 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,063 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:35,063 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,063 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,063 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,063 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:35,063 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,063 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:35,063 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,063 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,063 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,063 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,063 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2025-06-05T04:26:35,063 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2025-06-05T04:26:35,063 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-06-05T04:26:35,063 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-06-05T04:26:35,073 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:35,073 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:35,073 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:35,073 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:35,074 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:35,074 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:35,074 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:35,074 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:35,240 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=54823
2025-06-05T04:26:35,241 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:26:35,245 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:35,245 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]54823
2025-06-05T04:26:35,245 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:35,245 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:35,245 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:35,245 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:35,245 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:35,245 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:35,246 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097595246
2025-06-05T04:26:35,246 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097595246
2025-06-05T04:26:35,246 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:26:35,246 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097595246
2025-06-05T04:26:35,246 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097595246
2025-06-05T04:26:35,260 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=54815
2025-06-05T04:26:35,260 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:26:35,261 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:26:35,263 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=54817
2025-06-05T04:26:35,263 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:26:35,265 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=54814
2025-06-05T04:26:35,265 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:26:35,265 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:35,265 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]54815
2025-06-05T04:26:35,265 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:35,265 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:35,265 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:35,265 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:35,265 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:35,265 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:35,266 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:26:35,266 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097595266
2025-06-05T04:26:35,266 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097595266
2025-06-05T04:26:35,266 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097595266
2025-06-05T04:26:35,266 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097595266
2025-06-05T04:26:35,270 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:35,270 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]54817
2025-06-05T04:26:35,270 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:35,270 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:35,271 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:35,271 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:35,271 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:35,271 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:35,271 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:35,271 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]54814
2025-06-05T04:26:35,271 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:35,271 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:35,271 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:35,271 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:35,272 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:35,272 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:35,272 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097595272
2025-06-05T04:26:35,272 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:26:35,272 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097595272
2025-06-05T04:26:35,272 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097595272
2025-06-05T04:26:35,272 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097595272
2025-06-05T04:26:35,275 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:26:35,276 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097595276
2025-06-05T04:26:35,276 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097595276
2025-06-05T04:26:35,276 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097595276
2025-06-05T04:26:35,276 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097595276
2025-06-05T04:26:35,276 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:26:35,277 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:26:35,282 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:26:35,282 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:35,282 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:35,292 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:35,292 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:35,295 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:35,295 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:35,296 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:35,296 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:35,368 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:35,368 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:35,368 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:35,368 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:35,368 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:35,368 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:35,368 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:35,369 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:35,378 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:35,379 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:35,379 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:35,379 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:35,379 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:35,379 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:35,379 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:35,379 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:35,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:35,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:35,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:35,385 [INFO ] epollEventLoopGroup-5-41 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:35,385 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:35,385 [INFO ] epollEventLoopGroup-5-41 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:35,385 [INFO ] epollEventLoopGroup-5-39 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,385 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:35,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:35,385 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,385 [INFO ] epollEventLoopGroup-5-39 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,385 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:26:35,385 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,386 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,386 [INFO ] epollEventLoopGroup-5-40 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,386 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,386 [INFO ] epollEventLoopGroup-5-40 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,386 [INFO ] epollEventLoopGroup-5-42 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:35,386 [INFO ] epollEventLoopGroup-5-42 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:35,385 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:35,386 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:35,385 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:35,386 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:35,386 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,386 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:35,386 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,386 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:35,386 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:35,386 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,386 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,386 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:35,386 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:35,386 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,386 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:35,386 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:35,386 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:35,386 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:35,386 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:35,386 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,386 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,386 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,386 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:35,386 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,386 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:35,386 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,387 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:35,387 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:35,386 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,387 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,387 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,386 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,387 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,387 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,386 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:35,386 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:35,387 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:35,387 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:35,387 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,387 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,387 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,387 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,387 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2025-06-05T04:26:35,387 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2025-06-05T04:26:35,387 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2025-06-05T04:26:35,387 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2025-06-05T04:26:35,386 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:35,387 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:35,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:35,387 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:35,388 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,387 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,388 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:35,387 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:35,388 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:35,388 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:35,388 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:35,387 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:35,388 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2025-06-05T04:26:35,388 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2025-06-05T04:26:35,388 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:35,388 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:35,396 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:35,396 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:35,397 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:35,397 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:35,397 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:35,397 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:35,398 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:35,398 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:35,398 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:35,398 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:35,399 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:35,399 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:48,066 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,066 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,066 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,066 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,389 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,389 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,389 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,389 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,390 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,390 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,390 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:26:48,390 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:00,106 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=55105
2025-06-05T04:27:00,106 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:27:00,113 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:00,114 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]55105
2025-06-05T04:27:00,114 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:00,114 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:00,114 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:00,114 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:27:00,114 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:27:00,114 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:00,115 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:27:00,115 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097620115
2025-06-05T04:27:00,115 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097620115
2025-06-05T04:27:00,115 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097620115
2025-06-05T04:27:00,115 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097620115
2025-06-05T04:27:00,117 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:27:00,142 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:00,142 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:00,148 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=55106
2025-06-05T04:27:00,149 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:27:00,157 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:00,158 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]55106
2025-06-05T04:27:00,158 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:00,158 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:00,158 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:00,159 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:27:00,159 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:27:00,159 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:00,160 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:27:00,160 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097620160
2025-06-05T04:27:00,160 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097620160
2025-06-05T04:27:00,160 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097620160
2025-06-05T04:27:00,160 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097620160
2025-06-05T04:27:00,161 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:27:00,190 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:00,190 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:48,960 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:48,960 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:48,974 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:48,974 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:48,974 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:48,975 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:48,989 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:48,989 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:48,990 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:48,990 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:48,991 [INFO ] epollEventLoopGroup-5-43 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:48,991 [INFO ] epollEventLoopGroup-5-43 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:48,991 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:48,991 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:48,991 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:48,992 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:48,992 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:48,992 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:26:48,991 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:48,991 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:48,992 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:48,992 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:26:48,992 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:48,992 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:48,992 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:48,992 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:48,992 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-06-05T04:26:48,992 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-06-05T04:26:48,993 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:48,994 [INFO ] epollEventLoopGroup-5-44 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:48,994 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:26:48,994 [INFO ] epollEventLoopGroup-5-44 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:48,994 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:48,994 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:48,994 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:48,994 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:48,994 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:48,994 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:26:48,994 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:48,994 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:48,994 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:48,994 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:48,995 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2025-06-05T04:26:48,995 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2025-06-05T04:26:49,005 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:49,005 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:49,005 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:26:49,005 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:26:49,007 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:49,007 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:49,007 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:26:49,007 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:26:49,252 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=55168
2025-06-05T04:26:49,253 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:26:49,253 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=55166
2025-06-05T04:26:49,253 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:26:49,259 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:49,259 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]55166
2025-06-05T04:26:49,259 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:49,259 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:49,259 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:49,259 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:49,259 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:49,259 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:26:49,260 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:49,260 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]55168
2025-06-05T04:26:49,260 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:49,260 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:49,260 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:49,261 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:49,261 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:49,261 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:26:49,261 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:26:49,261 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097609261
2025-06-05T04:26:49,261 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097609261
2025-06-05T04:26:49,261 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097609261
2025-06-05T04:26:49,261 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097609261
2025-06-05T04:26:49,261 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097609261
2025-06-05T04:26:49,261 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:26:49,261 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097609261
2025-06-05T04:26:49,262 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:26:49,262 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097609262
2025-06-05T04:26:49,262 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097609262
2025-06-05T04:26:49,262 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:26:49,272 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=55167
2025-06-05T04:26:49,272 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:26:49,276 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=55165
2025-06-05T04:26:49,276 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:26:49,277 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:49,278 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]55167
2025-06-05T04:26:49,278 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:49,278 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:49,278 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:49,278 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:49,278 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:49,278 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:26:49,279 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:49,279 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:49,279 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:26:49,279 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097609279
2025-06-05T04:26:49,279 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097609279
2025-06-05T04:26:49,279 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097609279
2025-06-05T04:26:49,279 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097609279
2025-06-05T04:26:49,279 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:26:49,281 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:26:49,281 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]55165
2025-06-05T04:26:49,281 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:26:49,281 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:26:49,281 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:49,281 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:26:49,281 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:49,281 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:49,281 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:26:49,281 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:49,282 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:26:49,282 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097609282
2025-06-05T04:26:49,282 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097609282
2025-06-05T04:26:49,282 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097609282
2025-06-05T04:26:49,282 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097609282
2025-06-05T04:26:49,283 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:26:49,296 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:49,296 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:49,300 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:26:49,300 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:26:49,362 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:49,362 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:49,362 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:49,362 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:49,362 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:49,362 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:26:49,362 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:49,362 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:26:49,375 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:49,375 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:49,375 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:49,376 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:49,376 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:49,376 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:49,376 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:26:49,376 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:49,381 [INFO ] epollEventLoopGroup-5-48 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:49,381 [INFO ] epollEventLoopGroup-5-48 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:49,381 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:26:49,382 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:49,382 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:49,382 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:49,382 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:49,382 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:49,382 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:26:49,382 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:49,382 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:49,382 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:49,382 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:49,382 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:49,382 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:49,382 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:49,382 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:49,382 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:49,382 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:49,382 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:49,383 [INFO ] epollEventLoopGroup-5-47 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:49,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:49,383 [INFO ] epollEventLoopGroup-5-47 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:49,383 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:49,383 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:26:49,383 [INFO ] epollEventLoopGroup-5-45 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:26:49,383 [INFO ] epollEventLoopGroup-5-45 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:49,383 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:49,383 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:49,383 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:26:49,383 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:26:49,383 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:49,383 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:49,383 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:26:49,383 [INFO ] epollEventLoopGroup-5-46 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:49,383 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:49,383 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:26:49,383 [INFO ] epollEventLoopGroup-5-46 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:26:49,384 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:49,383 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:49,384 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:26:49,383 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:26:49,384 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:49,384 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:49,384 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:49,384 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:49,383 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:26:49,384 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:49,384 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:49,384 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:49,384 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:49,384 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:49,384 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:26:49,384 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:49,384 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:26:49,384 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:49,384 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:26:49,384 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:49,384 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:26:49,384 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:26:49,384 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:26:49,384 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:26:49,384 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:26:49,384 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:49,384 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:49,384 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:26:49,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:26:49,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:26:49,384 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:26:49,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:26:49,385 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:49,385 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:26:49,385 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:49,385 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:26:49,385 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2025-06-05T04:26:49,385 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2025-06-05T04:26:49,385 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2025-06-05T04:26:49,385 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2025-06-05T04:26:49,385 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2025-06-05T04:26:49,385 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2025-06-05T04:26:49,395 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:49,395 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:49,395 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:26:49,395 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:26:49,396 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:49,396 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:26:49,396 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:49,396 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:26:49,397 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:49,397 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:49,397 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:26:49,397 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:26:49,400 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:26:49,400 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:49,400 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:26:49,400 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:27:10,004 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,004 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,006 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,006 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,393 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,393 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,396 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,396 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,396 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,396 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,396 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,396 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:10,670 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=55483
2025-06-05T04:27:10,670 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:27:10,674 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:10,674 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]55483
2025-06-05T04:27:10,675 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:10,675 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:10,675 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:10,675 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:10,675 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:27:10,675 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:27:10,675 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:27:10,676 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097630676
2025-06-05T04:27:10,676 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097630676
2025-06-05T04:27:10,676 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097630676
2025-06-05T04:27:10,676 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097630676
2025-06-05T04:27:10,676 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:27:10,692 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:10,692 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:10,717 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=55484
2025-06-05T04:27:10,718 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:27:10,741 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:10,756 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]55484
2025-06-05T04:27:10,756 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:10,756 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:10,756 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:10,756 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:27:10,756 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:27:10,757 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:10,767 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:27:10,768 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097630768
2025-06-05T04:27:10,768 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097630768
2025-06-05T04:27:10,768 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097630768
2025-06-05T04:27:10,768 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097630768
2025-06-05T04:27:10,769 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:27:10,801 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:10,802 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:10,818 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:10,818 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:10,832 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:10,832 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:10,850 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:10,850 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:10,850 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:10,850 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:10,851 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:27:10,851 [INFO ] epollEventLoopGroup-5-49 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:10,851 [INFO ] epollEventLoopGroup-5-49 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:10,851 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:10,851 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:10,851 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:10,851 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:10,852 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:27:10,852 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:27:10,852 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:10,852 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:10,852 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:10,852 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:10,852 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-06-05T04:27:10,852 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-06-05T04:27:10,868 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:27:10,868 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:27:10,868 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:27:10,868 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:27:10,875 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:10,875 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:10,890 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:10,890 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:10,892 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:10,892 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:10,892 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:10,893 [INFO ] epollEventLoopGroup-5-50 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:10,893 [INFO ] epollEventLoopGroup-5-50 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:10,893 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:10,893 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:10,893 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:27:10,893 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:10,893 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:10,893 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:27:10,893 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:27:10,893 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:10,893 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:10,893 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:10,893 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:10,894 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2025-06-05T04:27:10,894 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2025-06-05T04:27:10,902 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:27:10,902 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:27:10,902 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:27:10,902 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:27:11,111 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=55546
2025-06-05T04:27:11,111 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:27:11,115 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:11,115 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]55546
2025-06-05T04:27:11,115 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:11,115 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:11,115 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:11,115 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:11,115 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:27:11,115 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:27:11,116 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:27:11,116 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097631116
2025-06-05T04:27:11,116 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097631116
2025-06-05T04:27:11,116 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097631116
2025-06-05T04:27:11,116 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097631116
2025-06-05T04:27:11,117 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:27:11,130 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:11,130 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:11,141 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=55543
2025-06-05T04:27:11,141 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:27:11,145 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:11,146 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]55543
2025-06-05T04:27:11,146 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:11,146 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:11,146 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:11,146 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:11,146 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:27:11,146 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:27:11,147 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:27:11,147 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097631147
2025-06-05T04:27:11,147 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097631147
2025-06-05T04:27:11,147 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097631147
2025-06-05T04:27:11,147 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097631147
2025-06-05T04:27:11,147 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:27:11,154 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=55547
2025-06-05T04:27:11,154 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:27:11,155 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=55548
2025-06-05T04:27:11,155 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:27:11,158 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:11,158 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]55547
2025-06-05T04:27:11,158 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:11,158 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:11,158 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:11,158 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:11,159 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:27:11,159 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:27:11,159 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:27:11,159 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097631159
2025-06-05T04:27:11,159 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097631159
2025-06-05T04:27:11,159 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:11,160 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097631160
2025-06-05T04:27:11,160 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097631160
2025-06-05T04:27:11,160 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]55548
2025-06-05T04:27:11,160 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:11,160 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:11,160 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:11,160 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:11,160 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:27:11,160 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:27:11,160 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:27:11,160 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:27:11,160 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097631160
2025-06-05T04:27:11,160 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097631160
2025-06-05T04:27:11,161 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097631160
2025-06-05T04:27:11,161 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097631160
2025-06-05T04:27:11,161 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:27:11,162 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:11,162 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:11,174 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:11,174 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:11,176 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:11,176 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:11,215 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:11,215 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:11,215 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:11,215 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:11,224 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:11,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:11,224 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:11,224 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:11,225 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:11,225 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:11,226 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:11,226 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:11,228 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:11,228 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:11,228 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:11,229 [INFO ] epollEventLoopGroup-5-51 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:11,229 [INFO ] epollEventLoopGroup-5-52 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:11,229 [INFO ] epollEventLoopGroup-5-51 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:11,229 [INFO ] epollEventLoopGroup-5-52 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:11,229 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:11,229 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:11,229 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:11,229 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:11,229 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:11,229 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:27:11,229 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:11,229 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:11,229 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:11,229 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:11,230 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:27:11,230 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:27:11,230 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:11,230 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:11,230 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:11,230 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:27:11,230 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:27:11,230 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:11,230 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:11,230 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:11,230 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:11,230 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:11,230 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2025-06-05T04:27:11,230 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2025-06-05T04:27:11,230 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2025-06-05T04:27:11,230 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2025-06-05T04:27:11,236 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:11,236 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:11,239 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:11,239 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:11,240 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:11,240 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:11,240 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:11,240 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:11,240 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:11,240 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:11,240 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:11,240 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:11,241 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:27:11,241 [INFO ] epollEventLoopGroup-5-54 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:11,241 [INFO ] epollEventLoopGroup-5-54 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:11,241 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:11,241 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:11,241 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:11,241 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:11,242 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:27:11,242 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:11,242 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:11,242 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:27:11,242 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:11,242 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:11,242 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:27:11,242 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:11,242 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:11,242 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:11,242 [INFO ] epollEventLoopGroup-5-53 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:11,242 [INFO ] epollEventLoopGroup-5-53 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:11,242 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:11,243 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:27:11,243 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:11,243 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2025-06-05T04:27:11,243 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:11,243 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2025-06-05T04:27:11,243 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:11,243 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:11,243 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:27:11,243 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:27:11,243 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:11,243 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:11,243 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:11,243 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:11,243 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2025-06-05T04:27:11,243 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2025-06-05T04:27:11,243 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:27:11,243 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:27:11,243 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:27:11,243 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:27:11,251 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:27:11,251 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:27:11,251 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:27:11,251 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:27:11,252 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:27:11,252 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:27:11,252 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:27:11,252 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:27:44,850 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:44,850 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:44,890 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:44,890 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,227 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,227 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,227 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,227 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,239 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,239 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,240 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,240 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:27:45,624 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=55900
2025-06-05T04:27:45,624 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:27:45,632 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:45,653 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]55900
2025-06-05T04:27:45,653 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:45,653 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:45,653 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:45,653 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:45,653 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:27:45,653 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:27:45,655 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:27:45,655 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097665655
2025-06-05T04:27:45,655 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097665655
2025-06-05T04:27:45,655 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097665655
2025-06-05T04:27:45,655 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097665655
2025-06-05T04:27:45,656 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:27:45,669 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=55903
2025-06-05T04:27:45,669 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:27:45,677 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:45,677 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]55903
2025-06-05T04:27:45,678 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:45,678 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:45,678 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:45,678 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:27:45,678 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:27:45,678 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:45,679 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:45,679 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:45,680 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:27:45,686 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097665686
2025-06-05T04:27:45,686 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097665686
2025-06-05T04:27:45,686 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097665686
2025-06-05T04:27:45,686 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097665686
2025-06-05T04:27:45,687 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:27:45,706 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:45,706 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:45,792 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:45,792 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:45,792 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:45,792 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:45,804 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:45,804 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:45,804 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:45,804 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:45,810 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:45,810 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:27:45,811 [INFO ] epollEventLoopGroup-5-56 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:45,811 [INFO ] epollEventLoopGroup-5-56 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:45,811 [INFO ] epollEventLoopGroup-5-55 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:45,811 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:45,811 [INFO ] epollEventLoopGroup-5-55 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:45,811 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:45,811 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:45,811 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:45,811 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:45,811 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:45,811 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:45,811 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:45,811 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:27:45,811 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:27:45,811 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:27:45,811 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:27:45,811 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:45,811 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:45,811 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:45,811 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:45,811 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:45,811 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:45,811 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:45,811 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-06-05T04:27:45,811 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2025-06-05T04:27:45,811 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-06-05T04:27:45,820 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:27:45,820 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:27:45,820 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:27:45,820 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:27:45,825 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:27:45,825 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:27:45,825 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:27:45,825 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:27:45,967 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=55965
2025-06-05T04:27:45,967 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:27:45,972 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:45,972 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]55965
2025-06-05T04:27:45,972 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:45,972 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:45,972 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:45,972 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:45,972 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:27:45,972 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:27:45,972 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:27:45,973 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097665973
2025-06-05T04:27:45,973 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097665973
2025-06-05T04:27:45,973 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097665973
2025-06-05T04:27:45,973 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097665973
2025-06-05T04:27:45,973 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:27:45,977 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=55964
2025-06-05T04:27:45,977 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:27:45,982 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:45,982 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]55964
2025-06-05T04:27:45,982 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:45,982 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:45,982 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:45,982 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:45,982 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:27:45,982 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:27:45,983 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:27:45,983 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097665983
2025-06-05T04:27:45,983 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097665983
2025-06-05T04:27:45,983 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097665983
2025-06-05T04:27:45,983 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097665983
2025-06-05T04:27:45,983 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:27:45,987 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:45,987 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:45,998 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:45,998 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:46,006 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=55961
2025-06-05T04:27:46,007 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:27:46,011 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:46,012 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]55961
2025-06-05T04:27:46,012 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:46,012 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:46,012 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:46,012 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:46,012 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:27:46,012 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:27:46,012 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:27:46,012 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097666012
2025-06-05T04:27:46,012 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097666012
2025-06-05T04:27:46,012 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097666012
2025-06-05T04:27:46,012 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097666012
2025-06-05T04:27:46,013 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:27:46,022 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=55960
2025-06-05T04:27:46,022 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:27:46,026 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:27:46,026 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]55960
2025-06-05T04:27:46,026 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:27:46,026 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:27:46,026 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:46,026 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:27:46,026 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:27:46,026 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:27:46,027 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:46,027 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:46,027 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:27:46,027 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097666027
2025-06-05T04:27:46,027 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097666027
2025-06-05T04:27:46,027 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097666027
2025-06-05T04:27:46,027 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097666027
2025-06-05T04:27:46,028 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:27:46,042 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:27:46,043 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:27:46,058 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:46,058 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:46,059 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:46,059 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:46,070 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:46,070 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:46,070 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:46,070 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:46,073 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:46,073 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:46,073 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:46,073 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:46,073 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:46,073 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:46,073 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:27:46,073 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:27:46,073 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:27:46,073 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:46,073 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:46,073 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:46,073 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:46,074 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2025-06-05T04:27:46,074 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:46,074 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:46,074 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:46,074 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:46,074 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:46,074 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:27:46,074 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:46,074 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:46,074 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:27:46,074 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:27:46,074 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:46,074 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:46,074 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:46,074 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:46,075 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2025-06-05T04:27:46,075 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2025-06-05T04:27:46,078 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:46,078 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:46,084 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:27:46,084 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:27:46,084 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:27:46,084 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:27:46,084 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:27:46,084 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:27:46,084 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:27:46,084 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:27:46,090 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:46,090 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:46,092 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:46,093 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:46,093 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:46,093 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:46,093 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:27:46,093 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:46,093 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:46,093 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:46,093 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:46,093 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:27:46,093 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:27:46,093 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:46,093 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:46,093 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:46,093 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2025-06-05T04:27:46,093 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2025-06-05T04:27:46,101 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:27:46,101 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:27:46,101 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:27:46,101 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:27:46,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:27:46,104 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:27:46,107 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:46,107 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:27:46,107 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:27:46,107 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:46,107 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:27:46,107 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:46,107 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:27:46,107 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:27:46,107 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:27:46,108 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:46,108 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:27:46,108 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:46,108 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:27:46,108 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2025-06-05T04:27:46,108 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2025-06-05T04:27:46,118 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:27:46,118 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:27:46,118 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:27:46,118 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:28:40,808 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:40,808 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:40,808 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:40,808 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,069 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,069 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,070 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,070 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,089 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,089 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,103 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,103 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:28:41,538 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=56375
2025-06-05T04:28:41,538 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:28:41,542 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:28:41,542 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]56375
2025-06-05T04:28:41,542 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:28:41,542 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:28:41,542 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,542 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,542 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:28:41,542 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:28:41,543 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:28:41,544 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721544
2025-06-05T04:28:41,544 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721544
2025-06-05T04:28:41,544 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721544
2025-06-05T04:28:41,544 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721544
2025-06-05T04:28:41,545 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:28:41,561 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:28:41,561 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:28:41,587 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=56376
2025-06-05T04:28:41,587 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:28:41,592 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:28:41,592 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]56376
2025-06-05T04:28:41,592 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:28:41,592 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:28:41,592 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,592 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,592 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:28:41,592 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:28:41,593 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:28:41,593 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721593
2025-06-05T04:28:41,593 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721593
2025-06-05T04:28:41,593 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721593
2025-06-05T04:28:41,593 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721593
2025-06-05T04:28:41,594 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:28:41,617 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:28:41,617 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:28:41,669 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:28:41,669 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:28:41,669 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:28:41,669 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:28:41,682 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:28:41,682 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:28:41,685 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:28:41,685 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,685 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:28:41,685 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,685 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,685 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,685 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,685 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:28:41,685 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:28:41,686 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,686 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,686 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,686 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,686 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2025-06-05T04:28:41,686 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2025-06-05T04:28:41,686 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:28:41,686 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:28:41,691 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:28:41,692 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,692 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,692 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,692 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,692 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,692 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,692 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:28:41,692 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:28:41,692 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,692 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,692 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,692 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,693 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-06-05T04:28:41,693 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-06-05T04:28:41,696 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:28:41,696 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:28:41,696 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:28:41,696 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:28:41,704 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:28:41,704 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:28:41,704 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:28:41,704 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:28:41,827 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=56444
2025-06-05T04:28:41,827 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:28:41,830 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=56431
2025-06-05T04:28:41,830 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:28:41,830 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=56432
2025-06-05T04:28:41,830 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:28:41,831 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:28:41,831 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]56444
2025-06-05T04:28:41,831 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:28:41,831 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:28:41,831 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,831 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,831 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:28:41,831 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:28:41,832 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:28:41,832 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721832
2025-06-05T04:28:41,832 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721832
2025-06-05T04:28:41,832 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721832
2025-06-05T04:28:41,832 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721832
2025-06-05T04:28:41,833 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:28:41,834 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:28:41,834 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]56431
2025-06-05T04:28:41,834 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:28:41,834 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:28:41,834 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,834 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,834 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:28:41,834 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:28:41,835 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721835
2025-06-05T04:28:41,835 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721835
2025-06-05T04:28:41,835 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:28:41,835 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721835
2025-06-05T04:28:41,835 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721835
2025-06-05T04:28:41,835 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:28:41,835 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]56432
2025-06-05T04:28:41,835 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,835 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,835 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:28:41,835 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:28:41,835 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:28:41,835 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:28:41,835 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:28:41,836 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721836
2025-06-05T04:28:41,836 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721836
2025-06-05T04:28:41,836 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:28:41,836 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721836
2025-06-05T04:28:41,836 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721836
2025-06-05T04:28:41,836 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:28:41,840 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=56441
2025-06-05T04:28:41,840 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:28:41,845 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:28:41,845 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]56441
2025-06-05T04:28:41,845 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:28:41,845 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:28:41,845 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,845 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:28:41,845 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:28:41,845 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:28:41,845 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:28:41,845 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721845
2025-06-05T04:28:41,845 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097721845
2025-06-05T04:28:41,846 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721846
2025-06-05T04:28:41,846 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097721846
2025-06-05T04:28:41,846 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:28:41,847 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:28:41,847 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:28:41,850 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:28:41,850 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:28:41,850 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:28:41,850 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:28:41,861 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:28:41,862 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:28:41,920 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:28:41,920 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:28:41,920 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:28:41,920 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:28:41,920 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:28:41,920 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:28:41,920 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:28:41,920 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:28:41,931 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:28:41,931 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:28:41,931 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:28:41,931 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:28:41,931 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:28:41,931 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:28:41,933 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:28:41,933 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:28:41,934 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:28:41,934 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,934 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,934 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,934 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,934 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,934 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,934 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:28:41,934 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:28:41,934 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,934 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,934 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,934 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,934 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:28:41,935 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2025-06-05T04:28:41,935 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:28:41,935 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:28:41,935 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,935 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,935 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,935 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,935 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,935 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:28:41,935 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:28:41,935 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:28:41,935 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,935 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:28:41,935 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,935 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,935 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:28:41,935 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:28:41,935 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:28:41,936 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:28:41,936 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,936 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:28:41,936 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,936 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:28:41,936 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,936 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,936 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,936 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:28:41,936 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:28:41,936 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2025-06-05T04:28:41,936 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2025-06-05T04:28:41,936 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2025-06-05T04:28:41,938 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:28:41,938 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:28:41,941 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:28:41,942 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:28:41,942 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:28:41,942 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,942 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:28:41,942 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:28:41,942 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,942 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:28:41,942 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,942 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:28:41,942 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:28:41,942 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:28:41,942 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,942 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:28:41,942 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,942 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:28:41,943 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2025-06-05T04:28:41,943 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2025-06-05T04:28:41,945 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:28:41,945 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:28:41,945 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:28:41,945 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:28:41,946 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:28:41,946 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:28:41,946 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:28:41,946 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:28:41,948 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:28:41,948 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:28:41,948 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:28:41,948 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:28:41,950 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:28:41,950 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:28:41,950 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:28:41,950 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:30:10,629 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,629 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,634 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,634 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,875 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,875 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,876 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,876 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,877 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,877 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,885 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:10,885 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:30:11,409 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=56975
2025-06-05T04:30:11,409 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:30:11,417 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:30:11,417 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]56975
2025-06-05T04:30:11,417 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:30:11,417 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:30:11,417 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,417 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,417 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:30:11,417 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:30:11,418 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:30:11,418 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811418
2025-06-05T04:30:11,418 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811418
2025-06-05T04:30:11,418 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811418
2025-06-05T04:30:11,418 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811418
2025-06-05T04:30:11,418 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:30:11,434 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:30:11,434 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:30:11,439 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=56980
2025-06-05T04:30:11,439 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:30:11,444 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:30:11,444 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]56980
2025-06-05T04:30:11,444 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:30:11,444 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:30:11,444 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,444 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,444 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:30:11,444 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:30:11,446 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:30:11,446 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811446
2025-06-05T04:30:11,446 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811446
2025-06-05T04:30:11,446 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811446
2025-06-05T04:30:11,446 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811446
2025-06-05T04:30:11,447 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:30:11,463 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:30:11,463 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:30:11,547 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:30:11,547 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:30:11,547 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:30:11,548 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:30:11,560 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:30:11,560 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:30:11,560 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:30:11,560 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:30:11,565 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:30:11,565 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:30:11,565 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:30:11,566 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:30:11,566 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:30:11,566 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,566 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,566 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,566 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:30:11,566 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:30:11,566 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:30:11,566 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,566 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,566 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,566 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:30:11,566 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:30:11,566 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:30:11,566 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:30:11,566 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:30:11,566 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,566 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:30:11,566 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,566 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,566 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:30:11,566 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:30:11,566 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:30:11,566 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2025-06-05T04:30:11,566 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2025-06-05T04:30:11,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:30:11,567 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds.
2025-06-05T04:30:11,567 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds.
2025-06-05T04:30:11,567 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:30:11,567 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:30:11,567 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:30:11,574 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:30:11,574 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:30:11,575 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:30:11,575 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:30:11,667 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=57007
2025-06-05T04:30:11,667 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:30:11,671 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:30:11,672 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]57007
2025-06-05T04:30:11,672 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:30:11,672 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:30:11,672 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,672 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,672 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:30:11,672 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:30:11,672 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:30:11,672 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811672
2025-06-05T04:30:11,672 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811672
2025-06-05T04:30:11,672 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811672
2025-06-05T04:30:11,672 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811672
2025-06-05T04:30:11,673 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:30:11,673 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=56985
2025-06-05T04:30:11,673 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:30:11,675 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=56992
2025-06-05T04:30:11,676 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:30:11,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:30:11,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]56985
2025-06-05T04:30:11,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:30:11,678 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:30:11,678 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,678 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,678 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:30:11,678 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:30:11,679 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:30:11,679 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811679
2025-06-05T04:30:11,679 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811679
2025-06-05T04:30:11,679 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811679
2025-06-05T04:30:11,679 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811679
2025-06-05T04:30:11,679 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:30:11,680 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:30:11,680 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]56992
2025-06-05T04:30:11,680 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:30:11,681 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:30:11,681 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,681 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,681 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:30:11,681 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:30:11,681 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:30:11,681 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811681
2025-06-05T04:30:11,681 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811681
2025-06-05T04:30:11,681 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811681
2025-06-05T04:30:11,681 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811681
2025-06-05T04:30:11,681 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:30:11,687 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:30:11,688 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:30:11,693 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:30:11,693 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:30:11,700 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:30:11,700 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:30:11,730 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=56988
2025-06-05T04:30:11,730 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:30:11,734 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:30:11,734 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]56988
2025-06-05T04:30:11,734 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:30:11,734 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:30:11,734 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,734 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:30:11,734 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:30:11,734 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:30:11,735 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:30:11,735 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811735
2025-06-05T04:30:11,735 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097811735
2025-06-05T04:30:11,735 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811735
2025-06-05T04:30:11,735 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097811735
2025-06-05T04:30:11,735 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:30:11,749 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:30:11,749 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:30:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:30:11,759 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:30:11,759 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:30:11,759 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:30:11,759 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:30:11,759 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:30:11,770 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:30:11,770 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:30:11,770 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:30:11,770 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:30:11,770 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:30:11,770 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:30:11,772 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:30:11,772 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:30:11,772 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:30:11,772 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:30:11,773 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:30:11,773 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:30:11,773 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,773 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,773 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:30:11,773 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,773 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:30:11,773 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,773 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,773 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,773 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,773 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,773 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,773 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,774 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:30:11,774 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:30:11,774 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:30:11,774 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:30:11,774 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,774 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,774 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,774 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,774 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,774 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,774 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,774 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,774 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds.
2025-06-05T04:30:11,774 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:30:11,774 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,774 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:30:11,774 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds.
2025-06-05T04:30:11,774 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,774 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds.
2025-06-05T04:30:11,774 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:30:11,774 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,774 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:30:11,774 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,774 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds.
2025-06-05T04:30:11,774 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:30:11,774 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,774 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:30:11,774 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,774 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:30:11,774 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:30:11,774 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:30:11,774 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:30:11,775 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:30:11,775 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,775 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:30:11,775 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds.
2025-06-05T04:30:11,775 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds.
2025-06-05T04:30:11,783 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:30:11,783 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:30:11,783 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:30:11,783 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:30:11,784 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:30:11,784 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:30:11,784 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:30:11,784 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:30:11,784 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:30:11,784 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:30:11,784 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:30:11,784 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:30:11,825 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:30:11,825 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:30:11,836 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:30:11,836 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:30:11,838 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,838 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:30:11,838 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:30:11,839 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:30:11,839 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:30:11,839 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:30:11,839 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:30:11,839 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:30:11,839 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,839 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:30:11,839 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:30:11,839 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,839 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:30:11,839 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:30:11,839 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:30:11,839 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,839 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:30:11,839 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,839 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:30:11,839 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 144 seconds.
2025-06-05T04:30:11,839 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 144 seconds.
2025-06-05T04:30:11,847 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:30:11,847 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:30:11,847 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:30:11,847 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:32:35,624 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,624 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,624 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,624 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,830 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,830 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,830 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,830 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,830 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,830 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,895 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:35,895 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:32:36,421 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=57750
2025-06-05T04:32:36,421 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:32:36,425 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:32:36,425 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]57750
2025-06-05T04:32:36,425 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:32:36,425 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,425 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,425 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:32:36,425 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:32:36,425 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:32:36,426 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:32:36,426 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956426
2025-06-05T04:32:36,426 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956426
2025-06-05T04:32:36,426 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956426
2025-06-05T04:32:36,426 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956426
2025-06-05T04:32:36,427 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:32:36,442 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:32:36,442 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:32:36,443 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=57751
2025-06-05T04:32:36,443 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:32:36,447 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:32:36,448 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]57751
2025-06-05T04:32:36,448 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:32:36,448 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:32:36,448 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,448 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,448 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:32:36,448 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:32:36,450 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:32:36,450 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956450
2025-06-05T04:32:36,450 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956450
2025-06-05T04:32:36,450 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956450
2025-06-05T04:32:36,450 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956450
2025-06-05T04:32:36,450 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:32:36,469 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:32:36,469 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:32:36,530 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:32:36,530 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:32:36,530 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:32:36,530 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:32:36,543 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:32:36,543 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:32:36,545 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:32:36,545 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:32:36,550 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,550 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,550 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:32:36,550 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,550 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:32:36,550 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:32:36,550 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,550 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:32:36,550 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/894607b110ec4315a426792dd379bf21/onnx_handler.py", line 16, in initialize
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:32:36,550 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:32:36,550 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,551 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:32:36,551 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:32:36,551 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/onnx_handler.py", line 16, in initialize
2025-06-05T04:32:36,550 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,551 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:32:36,551 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:32:36,551 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:32:36,551 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed:Load model /tmp/models/894607b110ec4315a426792dd379bf21/model.onnx failed. File doesn't exist
2025-06-05T04:32:36,551 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,551 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:32:36,551 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,551 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,551 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,551 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:32:36,551 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,551 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,551 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:32:36,551 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:32:36,551 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:32:36,551 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed:Load model /tmp/models/37c3f6aa19c14f69a6baea1ed8aee6b2/model.onnx failed. File doesn't exist
2025-06-05T04:32:36,551 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,551 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,551 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,551 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,562 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:32:36,562 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:32:36,562 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:32:36,562 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:32:36,563 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:32:36,564 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:32:36,564 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:32:36,563 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:32:36,621 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=57763
2025-06-05T04:32:36,622 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:32:36,626 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:32:36,626 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]57763
2025-06-05T04:32:36,626 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:32:36,626 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:32:36,626 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,626 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,626 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:32:36,626 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:32:36,627 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:32:36,627 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956627
2025-06-05T04:32:36,627 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956627
2025-06-05T04:32:36,627 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956627
2025-06-05T04:32:36,627 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956627
2025-06-05T04:32:36,628 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:32:36,635 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=57762
2025-06-05T04:32:36,636 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:32:36,639 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=57761
2025-06-05T04:32:36,640 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:32:36,640 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:32:36,640 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]57762
2025-06-05T04:32:36,640 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:32:36,640 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:32:36,641 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,641 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,641 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:32:36,641 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:32:36,641 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:32:36,641 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956641
2025-06-05T04:32:36,641 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956641
2025-06-05T04:32:36,642 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956641
2025-06-05T04:32:36,642 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956641
2025-06-05T04:32:36,642 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:32:36,645 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:32:36,646 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:32:36,648 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:32:36,648 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]57761
2025-06-05T04:32:36,648 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:32:36,648 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:32:36,648 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,648 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,648 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:32:36,648 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:32:36,648 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:32:36,649 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956649
2025-06-05T04:32:36,649 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956649
2025-06-05T04:32:36,649 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956649
2025-06-05T04:32:36,649 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956649
2025-06-05T04:32:36,649 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:32:36,656 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:32:36,657 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:32:36,663 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:32:36,663 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:32:36,712 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=57781
2025-06-05T04:32:36,712 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:32:36,716 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:32:36,716 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]57781
2025-06-05T04:32:36,716 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:32:36,716 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:32:36,716 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,716 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:32:36,716 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:32:36,716 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:32:36,717 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:32:36,717 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956717
2025-06-05T04:32:36,717 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097956717
2025-06-05T04:32:36,717 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956717
2025-06-05T04:32:36,717 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097956717
2025-06-05T04:32:36,717 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:32:36,720 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:32:36,720 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:32:36,720 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:32:36,720 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:32:36,720 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:32:36,720 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:32:36,730 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:32:36,730 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:32:36,731 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:32:36,731 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:32:36,731 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:32:36,731 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:32:36,731 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:32:36,731 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:32:36,735 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/e17873f41caa4416b8858cfd9fbbcba3/onnx_handler.py", line 16, in initialize
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:32:36,736 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:32:36,736 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:32:36,736 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed:Load model /tmp/models/e17873f41caa4416b8858cfd9fbbcba3/model.onnx failed. File doesn't exist
2025-06-05T04:32:36,736 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,736 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,736 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,736 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,736 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:32:36,736 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:32:36,736 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,736 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,736 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,736 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:32:36,737 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,737 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:32:36,738 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:32:36,737 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,738 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/e33ef62464614b5591afb68136eac6df/onnx_handler.py", line 16, in initialize
2025-06-05T04:32:36,738 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:32:36,738 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:32:36,738 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:32:36,738 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:32:36,738 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:32:36,738 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed:Load model /tmp/models/e33ef62464614b5591afb68136eac6df/model.onnx failed. File doesn't exist
2025-06-05T04:32:36,738 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,738 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,738 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,738 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,738 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:32:36,738 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:32:36,738 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,738 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,738 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,738 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,744 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:32:36,744 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:32:36,744 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:32:36,744 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:32:36,744 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/3df31b23cd754e81ae89acf8501bf003/onnx_handler.py", line 16, in initialize
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:32:36,745 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed:Load model /tmp/models/3df31b23cd754e81ae89acf8501bf003/model.onnx failed. File doesn't exist
2025-06-05T04:32:36,745 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,745 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,745 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:32:36,745 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:32:36,745 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:32:36,745 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:32:36,745 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,745 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,745 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,745 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,745 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:32:36,745 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:32:36,745 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,745 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,745 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,745 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,746 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:32:36,746 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:32:36,746 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:32:36,746 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:32:36,753 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:32:36,753 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:32:36,753 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:32:36,753 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:32:36,857 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:32:36,857 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:32:36,868 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:32:36,868 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fc9698834de243038ef5dbf39afe0e9a/onnx_handler.py", line 16, in initialize
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(model_path, providers=providers)
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 472, in __init__
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self._create_inference_session(providers, provider_options, disabled_optimizers)
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 550, in _create_inference_session
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
2025-06-05T04:32:36,872 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.NoSuchFile: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed:Load model /tmp/models/fc9698834de243038ef5dbf39afe0e9a/model.onnx failed. File doesn't exist
2025-06-05T04:32:36,872 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,872 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:32:36,872 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,872 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:32:36,872 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,872 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:32:36,872 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:32:36,872 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:32:36,872 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,872 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:32:36,873 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,873 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:32:36,880 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:32:36,880 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:32:36,880 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:32:36,880 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T06:29:04,481 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T06:29:04,481 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T06:29:04,483 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T06:29:04,483 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T06:29:04,525 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T06:29:04,525 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T06:28:53,405 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T06:28:53,405 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T06:28:53,413 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T06:28:53,413 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T06:28:53,425 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T06:28:53,425 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T06:28:53,528 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T06:28:53,528 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T06:28:53,529 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T06:28:53,529 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T06:28:53,529 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T06:28:53,529 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T06:28:53,529 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T06:28:53,529 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T06:28:53,533 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T06:28:53,533 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T06:28:53,534 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,534 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,606 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T06:28:53,606 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T06:28:53,606 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T06:28:53,606 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T06:28:53,606 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T06:28:53,606 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T06:28:53,607 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T06:28:53,607 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T06:28:53,607 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T06:28:53,607 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T06:28:53,608 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,608 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,681 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T06:28:53,681 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T06:28:53,681 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T06:28:53,681 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T06:28:53,681 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T06:28:53,681 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T06:28:53,682 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T06:28:53,682 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T06:28:53,682 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T06:28:53,682 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T06:28:53,682 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,682 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,752 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T06:28:53,752 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T06:28:53,752 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T06:28:53,752 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T06:28:53,752 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T06:28:53,752 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T06:28:53,752 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T06:28:53,752 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T06:28:53,753 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T06:28:53,753 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T06:28:53,754 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,754 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,824 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T06:28:53,824 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T06:28:53,825 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T06:28:53,825 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T06:28:53,825 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T06:28:53,825 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T06:28:53,825 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T06:28:53,825 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T06:28:53,825 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T06:28:53,825 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T06:28:53,826 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,826 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,895 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T06:28:53,895 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T06:28:53,895 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T06:28:53,895 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T06:28:53,895 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T06:28:53,895 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T06:28:53,896 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T06:28:53,896 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T06:28:53,897 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,897 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T06:28:53,900 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T06:28:53,900 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T06:28:53,984 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T06:28:53,984 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T06:28:53,985 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T06:28:53,985 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T06:28:53,993 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T06:28:53,993 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T06:28:53,993 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T06:28:53,993 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T06:28:53,994 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T06:28:53,994 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T06:28:54,556 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=81382
2025-06-05T06:28:54,559 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T06:28:54,561 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T06:28:54,561 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]81382
2025-06-05T06:28:54,561 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T06:28:54,563 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T06:28:54,563 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,563 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=81386
2025-06-05T06:28:54,566 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T06:28:54,567 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T06:28:54,567 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T06:28:54,571 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T06:28:54,572 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]81386
2025-06-05T06:28:54,572 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T06:28:54,572 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T06:28:54,573 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,571 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=81391
2025-06-05T06:28:54,573 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,573 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T06:28:54,573 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T06:28:54,573 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T06:28:54,574 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T06:28:54,575 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T06:28:54,576 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934576
2025-06-05T06:28:54,576 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934576
2025-06-05T06:28:54,576 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934576
2025-06-05T06:28:54,576 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934576
2025-06-05T06:28:54,577 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T06:28:54,577 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]81391
2025-06-05T06:28:54,577 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T06:28:54,577 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,577 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,577 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T06:28:54,577 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T06:28:54,577 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T06:28:54,578 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934578
2025-06-05T06:28:54,578 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934578
2025-06-05T06:28:54,578 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934578
2025-06-05T06:28:54,578 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934578
2025-06-05T06:28:54,578 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934578
2025-06-05T06:28:54,578 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T06:28:54,578 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934578
2025-06-05T06:28:54,578 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934578
2025-06-05T06:28:54,578 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934578
2025-06-05T06:28:54,580 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=81396
2025-06-05T06:28:54,581 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T06:28:54,585 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T06:28:54,585 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]81396
2025-06-05T06:28:54,586 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,585 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T06:28:54,586 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,586 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T06:28:54,586 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T06:28:54,586 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T06:28:54,590 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934590
2025-06-05T06:28:54,590 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T06:28:54,590 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934590
2025-06-05T06:28:54,592 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934592
2025-06-05T06:28:54,592 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934592
2025-06-05T06:28:54,601 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T06:28:54,605 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T06:28:54,608 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T06:28:54,614 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T06:28:54,634 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T06:28:54,634 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T06:28:54,634 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T06:28:54,634 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T06:28:54,634 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T06:28:54,634 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T06:28:54,634 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T06:28:54,635 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T06:28:54,679 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=81401
2025-06-05T06:28:54,680 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T06:28:54,683 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T06:28:54,684 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]81401
2025-06-05T06:28:54,684 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T06:28:54,684 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,684 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T06:28:54,684 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,684 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T06:28:54,684 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T06:28:54,685 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934685
2025-06-05T06:28:54,685 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T06:28:54,685 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934685
2025-06-05T06:28:54,685 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934685
2025-06-05T06:28:54,685 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934685
2025-06-05T06:28:54,696 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T06:28:54,712 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T06:28:54,713 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T06:28:54,729 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=81406
2025-06-05T06:28:54,729 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T06:28:54,734 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T06:28:54,734 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]81406
2025-06-05T06:28:54,734 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T06:28:54,734 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T06:28:54,734 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,734 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T06:28:54,734 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T06:28:54,734 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T06:28:54,735 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934735
2025-06-05T06:28:54,735 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T06:28:54,735 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749104934735
2025-06-05T06:28:54,735 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934735
2025-06-05T06:28:54,735 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749104934735
2025-06-05T06:28:54,746 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T06:28:54,750 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T06:28:54,750 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T06:28:54,750 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T06:28:54,750 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T06:28:54,750 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T06:28:54,750 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T06:28:54,750 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T06:28:54,750 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T06:28:54,763 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T06:28:54,764 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T06:28:54,767 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T06:28:54,767 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T06:28:54,767 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T06:28:54,767 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T06:28:54,767 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T06:28:54,767 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T06:28:54,767 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T06:28:54,767 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T06:28:54,769 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T06:28:54,769 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T06:28:54,785 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T06:28:54,785 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T06:28:54,808 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - [1;31m2025-06-05 06:28:54.807819208 [E:onnxruntime:Default, provider_bridge_ort.cc:2195 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.12: cannot open shared object file: No such file or directory
2025-06-05T06:28:54,808 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - [m
2025-06-05T06:28:54,809 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - [0;93m2025-06-05 06:28:54.807899552 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1055 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.[m
2025-06-05T06:28:54,808 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - [1;31m2025-06-05 06:28:54.808392755 [E:onnxruntime:Default, provider_bridge_ort.cc:2195 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.12: cannot open shared object file: No such file or directory
2025-06-05T06:28:54,809 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - [m
2025-06-05T06:28:54,809 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - [0;93m2025-06-05 06:28:54.808416124 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1055 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.[m
2025-06-05T06:28:54,808 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - [1;31m2025-06-05 06:28:54.808323011 [E:onnxruntime:Default, provider_bridge_ort.cc:2195 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.12: cannot open shared object file: No such file or directory
2025-06-05T06:28:54,809 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - [m
2025-06-05T06:28:54,810 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - [0;93m2025-06-05 06:28:54.808411070 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1055 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.[m
2025-06-05T06:28:54,815 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - [1;31m2025-06-05 06:28:54.815080402 [E:onnxruntime:Default, provider_bridge_ort.cc:2195 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.12: cannot open shared object file: No such file or directory
2025-06-05T06:28:54,815 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - [m
2025-06-05T06:28:54,815 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - [0;93m2025-06-05 06:28:54.815123498 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1055 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.[m
2025-06-05T06:28:54,843 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - [1;31m2025-06-05 06:28:54.843509857 [E:onnxruntime:Default, provider_bridge_ort.cc:2195 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.12: cannot open shared object file: No such file or directory
2025-06-05T06:28:54,844 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - [m
2025-06-05T06:28:54,844 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - [0;93m2025-06-05 06:28:54.843557525 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1055 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.[m
2025-06-05T06:28:54,845 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T06:28:54,845 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T06:28:54,860 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T06:28:54,860 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T06:28:54,881 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 303
2025-06-05T06:28:54,881 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 303
2025-06-05T06:28:54,881 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 303
2025-06-05T06:28:54,881 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 303
2025-06-05T06:28:54,882 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,882 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,882 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,882 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,882 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1275.0|#WorkerName:W-9001-thieudong_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,882 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1350.0|#WorkerName:W-9000-xuoc_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,883 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,883 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,883 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 305
2025-06-05T06:28:54,883 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 305
2025-06-05T06:28:54,883 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,883 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,883 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1201.0|#WorkerName:W-9002-vetlom_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,884 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,886 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 294
2025-06-05T06:28:54,886 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 294
2025-06-05T06:28:54,886 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,886 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,886 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1133.0|#WorkerName:W-9003-bamdinh_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,886 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,897 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - [1;31m2025-06-05 06:28:54.897658715 [E:onnxruntime:Default, provider_bridge_ort.cc:2195 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.12: cannot open shared object file: No such file or directory
2025-06-05T06:28:54,897 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - [m
2025-06-05T06:28:54,898 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - [0;93m2025-06-05 06:28:54.897685840 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1055 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.[m
2025-06-05T06:28:54,910 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 225
2025-06-05T06:28:54,910 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 225
2025-06-05T06:28:54,910 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,910 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,910 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1085.0|#WorkerName:W-9004-nganmach_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,911 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,944 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 208
2025-06-05T06:28:54,944 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 208
2025-06-05T06:28:54,945 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,945 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T06:28:54,945 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1049.0|#WorkerName:W-9005-divatma_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:54,945 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104934
2025-06-05T06:28:59,072 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:58532 "GET / HTTP/1.1" 405 4
2025-06-05T06:28:59,073 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749104939
2025-06-05T09:00:35,686 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:00:35,686 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:00:35,688 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:00:35,688 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:00:35,723 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:00:35,723 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:00:35,956 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:00:35,956 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:00:35,966 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:00:35,966 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:00:35,978 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T09:00:35,978 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T09:00:36,091 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:00:36,091 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:00:36,092 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:00:36,092 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:00:36,092 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:00:36,092 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:00:36,092 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:00:36,092 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:00:36,096 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T09:00:36,096 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T09:00:36,097 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,097 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,168 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:00:36,168 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:00:36,169 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:00:36,169 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:00:36,169 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:00:36,169 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:00:36,169 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:00:36,169 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:00:36,169 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T09:00:36,169 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T09:00:36,170 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,170 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,242 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:00:36,242 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:00:36,242 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:00:36,242 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:00:36,242 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:00:36,242 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:00:36,243 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:00:36,243 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:00:36,243 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T09:00:36,243 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T09:00:36,243 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,243 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,313 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:00:36,313 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:00:36,313 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:00:36,313 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:00:36,313 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:00:36,313 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:00:36,313 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:00:36,313 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:00:36,314 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T09:00:36,314 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T09:00:36,314 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,314 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,386 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:00:36,386 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:00:36,386 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:00:36,386 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:00:36,386 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:00:36,386 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:00:36,386 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:00:36,386 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:00:36,386 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T09:00:36,386 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T09:00:36,387 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,387 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,456 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:00:36,456 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:00:36,456 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:00:36,456 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:00:36,457 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:00:36,457 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:00:36,457 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:00:36,457 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:00:36,457 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,457 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:00:36,461 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:00:36,461 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:00:37,379 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=117471
2025-06-05T09:00:37,380 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T09:00:37,379 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=117466
2025-06-05T09:00:37,379 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=117476
2025-06-05T09:00:37,380 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T09:00:37,380 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T09:00:37,379 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=117443
2025-06-05T09:00:37,379 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=117447
2025-06-05T09:00:37,380 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T09:00:37,380 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T09:00:37,383 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:00:37,383 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]117447
2025-06-05T09:00:37,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:00:37,383 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:00:37,383 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]117466
2025-06-05T09:00:37,384 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:00:37,383 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:00:37,384 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:00:37,384 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,384 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,384 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:00:37,384 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,384 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,384 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]117443
2025-06-05T09:00:37,384 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,384 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:00:37,384 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,384 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:00:37,385 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:00:37,385 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]117471
2025-06-05T09:00:37,385 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:00:37,385 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,385 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,385 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:00:37,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:00:37,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]117476
2025-06-05T09:00:37,385 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:00:37,385 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,386 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:00:37,385 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,387 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:00:37,387 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:00:37,387 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:00:37,387 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:00:37,387 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:00:37,387 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:00:37,387 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:00:37,387 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:00:37,387 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:00:37,387 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:00:37,390 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=117461
2025-06-05T09:00:37,390 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T09:00:37,393 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T09:00:37,393 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T09:00:37,393 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T09:00:37,393 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T09:00:37,393 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T09:00:37,395 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,395 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037395
2025-06-05T09:00:37,397 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037397
2025-06-05T09:00:37,398 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037398
2025-06-05T09:00:37,398 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037398
2025-06-05T09:00:37,398 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037398
2025-06-05T09:00:37,397 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037397
2025-06-05T09:00:37,398 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037398
2025-06-05T09:00:37,398 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037398
2025-06-05T09:00:37,398 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037398
2025-06-05T09:00:37,398 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037398
2025-06-05T09:00:37,398 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037398
2025-06-05T09:00:37,398 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:00:37,398 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]117461
2025-06-05T09:00:37,398 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:00:37,399 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,399 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:00:37,399 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:00:37,399 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:00:37,399 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:00:37,400 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T09:00:37,400 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037400
2025-06-05T09:00:37,400 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114037400
2025-06-05T09:00:37,400 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037400
2025-06-05T09:00:37,400 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114037400
2025-06-05T09:00:37,427 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T09:00:37,427 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T09:00:37,427 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T09:00:37,428 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T09:00:37,428 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T09:00:37,429 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T09:00:37,463 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:00:37,463 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:00:37,464 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:00:37,464 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:00:37,464 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:00:37,464 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:00:37,464 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:00:37,464 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:00:37,464 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:00:37,464 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:00:37,464 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:00:37,464 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:00:37,620 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:00:37,621 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:00:37,621 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:00:37,622 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:00:37,625 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:00:37,628 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:00:37,648 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:00:37,649 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:00:37,648 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:00:37,648 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:00:37,648 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:00:37,648 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:00:37,649 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:00:37,648 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:00:37,649 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:00:37,649 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:00:37,649 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:00:37,649 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:00:37,779 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 379
2025-06-05T09:00:37,779 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 381
2025-06-05T09:00:37,779 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 379
2025-06-05T09:00:37,779 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 381
2025-06-05T09:00:37,779 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 381
2025-06-05T09:00:37,780 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,779 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 381
2025-06-05T09:00:37,779 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,780 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,780 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,780 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,779 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,780 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1685.0|#WorkerName:W-9000-xuoc_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,780 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1323.0|#WorkerName:W-9005-divatma_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,780 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1537.0|#WorkerName:W-9002-vetlom_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,781 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,781 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,780 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,782 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 384
2025-06-05T09:00:37,782 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 384
2025-06-05T09:00:37,782 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,782 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,782 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1613.0|#WorkerName:W-9001-thieudong_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,782 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,788 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 390
2025-06-05T09:00:37,788 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 390
2025-06-05T09:00:37,788 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,788 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,788 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1475.0|#WorkerName:W-9003-bamdinh_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,788 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,791 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 393
2025-06-05T09:00:37,791 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 393
2025-06-05T09:00:37,791 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,791 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:00:37,791 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1405.0|#WorkerName:W-9004-nganmach_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:37,791 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114037
2025-06-05T09:00:38,539 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-06-05T09:00:38,539 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-06-05T09:02:35,736 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:02:35,736 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:02:35,738 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:02:35,738 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:02:35,776 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:02:35,776 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:02:35,993 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:02:35,993 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:02:36,003 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:02:36,003 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:02:36,014 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T09:02:36,014 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T09:02:36,123 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:02:36,123 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:02:36,123 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:02:36,123 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:02:36,123 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:02:36,123 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:02:36,123 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:02:36,123 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:02:36,127 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T09:02:36,127 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T09:02:36,128 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,128 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,198 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:02:36,198 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:02:36,198 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:02:36,198 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:02:36,198 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:02:36,198 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:02:36,199 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:02:36,199 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:02:36,199 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T09:02:36,199 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T09:02:36,200 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,200 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,271 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:02:36,271 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:02:36,271 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:02:36,271 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:02:36,271 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:02:36,271 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:02:36,272 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:02:36,272 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:02:36,272 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T09:02:36,272 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T09:02:36,272 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,272 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,342 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:02:36,342 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:02:36,342 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:02:36,342 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:02:36,343 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:02:36,343 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:02:36,343 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:02:36,343 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:02:36,343 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T09:02:36,343 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T09:02:36,344 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,344 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,414 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:02:36,414 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:02:36,415 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:02:36,415 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:02:36,415 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:02:36,415 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:02:36,415 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:02:36,415 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:02:36,416 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T09:02:36,416 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T09:02:36,416 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,416 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,486 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:02:36,486 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:02:36,486 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:02:36,486 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:02:36,487 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:02:36,487 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:02:36,487 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:02:36,487 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:02:36,487 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,487 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:02:36,491 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:02:36,491 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:02:37,185 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=118418
2025-06-05T09:02:37,186 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=118408
2025-06-05T09:02:37,186 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=118413
2025-06-05T09:02:37,187 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T09:02:37,187 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T09:02:37,187 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T09:02:37,190 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:02:37,190 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:02:37,190 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]118408
2025-06-05T09:02:37,190 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:02:37,190 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]118413
2025-06-05T09:02:37,191 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:02:37,191 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:02:37,190 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=118404
2025-06-05T09:02:37,191 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:02:37,191 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T09:02:37,191 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,191 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,191 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,191 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,193 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:02:37,193 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]118418
2025-06-05T09:02:37,193 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:02:37,193 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,193 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:02:37,193 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,195 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:02:37,195 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:02:37,195 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:02:37,195 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:02:37,195 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:02:37,195 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:02:37,197 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:02:37,197 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]118404
2025-06-05T09:02:37,197 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:02:37,198 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,197 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:02:37,198 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,198 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:02:37,198 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:02:37,200 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T09:02:37,200 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T09:02:37,200 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T09:02:37,200 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T09:02:37,202 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157202
2025-06-05T09:02:37,202 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157202
2025-06-05T09:02:37,202 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157202
2025-06-05T09:02:37,202 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157202
2025-06-05T09:02:37,202 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157202
2025-06-05T09:02:37,202 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157202
2025-06-05T09:02:37,202 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157202
2025-06-05T09:02:37,202 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157202
2025-06-05T09:02:37,204 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157204
2025-06-05T09:02:37,204 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157204
2025-06-05T09:02:37,204 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157204
2025-06-05T09:02:37,204 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157204
2025-06-05T09:02:37,204 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157204
2025-06-05T09:02:37,204 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157204
2025-06-05T09:02:37,204 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157204
2025-06-05T09:02:37,204 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157204
2025-06-05T09:02:37,238 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T09:02:37,238 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T09:02:37,239 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T09:02:37,238 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T09:02:37,254 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=118423
2025-06-05T09:02:37,254 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T09:02:37,258 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:02:37,259 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]118423
2025-06-05T09:02:37,259 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:02:37,259 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,259 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:02:37,259 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,259 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:02:37,259 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:02:37,260 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T09:02:37,260 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157260
2025-06-05T09:02:37,260 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157260
2025-06-05T09:02:37,260 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157260
2025-06-05T09:02:37,260 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157260
2025-06-05T09:02:37,272 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T09:02:37,272 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:02:37,272 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:02:37,272 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:02:37,272 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:02:37,273 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:02:37,273 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:02:37,272 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:02:37,273 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:02:37,288 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:02:37,288 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:02:37,340 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=118428
2025-06-05T09:02:37,340 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T09:02:37,344 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:02:37,344 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]118428
2025-06-05T09:02:37,344 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:02:37,344 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,344 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:02:37,344 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:02:37,344 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:02:37,344 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:02:37,345 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157345
2025-06-05T09:02:37,345 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114157345
2025-06-05T09:02:37,345 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T09:02:37,345 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157345
2025-06-05T09:02:37,345 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114157345
2025-06-05T09:02:37,359 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T09:02:37,374 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:02:37,374 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:02:37,436 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:02:37,437 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:02:37,439 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:02:37,440 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:02:37,440 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:02:37,453 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:02:37,453 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:02:37,453 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:02:37,453 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:02:37,453 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:02:37,454 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:02:37,454 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:02:37,454 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:02:37,454 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:02:37,454 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:02:37,496 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:02:37,509 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:02:37,509 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:02:37,574 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 370
2025-06-05T09:02:37,574 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 314
2025-06-05T09:02:37,574 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 370
2025-06-05T09:02:37,574 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 355
2025-06-05T09:02:37,574 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 314
2025-06-05T09:02:37,574 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 355
2025-06-05T09:02:37,575 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,575 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,575 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,575 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,575 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,575 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,575 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1160.0|#WorkerName:W-9004-nganmach_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,575 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1376.0|#WorkerName:W-9001-thieudong_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,575 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1232.0|#WorkerName:W-9003-bamdinh_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,575 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,575 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,576 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:18.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,579 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 360
2025-06-05T09:02:37,579 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 360
2025-06-05T09:02:37,579 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,579 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,579 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1453.0|#WorkerName:W-9000-xuoc_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,579 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:17.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,583 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 379
2025-06-05T09:02:37,583 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 379
2025-06-05T09:02:37,583 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,583 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,583 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1311.0|#WorkerName:W-9002-vetlom_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,583 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,608 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 263
2025-06-05T09:02:37,608 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 263
2025-06-05T09:02:37,608 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,608 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:02:37,608 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1121.0|#WorkerName:W-9005-divatma_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:37,608 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:0.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114157
2025-06-05T09:02:38,622 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-06-05T09:02:38,622 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-06-05T09:03:19,811 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:03:19,811 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:03:19,812 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:03:19,812 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:03:19,847 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:03:19,847 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:03:20,036 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:03:20,036 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:03:20,041 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:03:20,041 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:03:20,050 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T09:03:20,050 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T09:03:20,167 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:03:20,167 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:03:20,168 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:03:20,168 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:03:20,168 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:03:20,168 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:03:20,168 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:03:20,168 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:03:20,172 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T09:03:20,172 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T09:03:20,173 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,173 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,254 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:03:20,254 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:03:20,254 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:03:20,254 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:03:20,254 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:03:20,254 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:03:20,255 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:03:20,255 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:03:20,255 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T09:03:20,255 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T09:03:20,256 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,256 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,342 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:03:20,342 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:03:20,342 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:03:20,342 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:03:20,342 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:03:20,342 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:03:20,342 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:03:20,342 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:03:20,343 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T09:03:20,343 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T09:03:20,343 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,343 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,423 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:03:20,423 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:03:20,424 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:03:20,424 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:03:20,424 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:03:20,424 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:03:20,424 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:03:20,424 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:03:20,425 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T09:03:20,425 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T09:03:20,425 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,425 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,499 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:03:20,499 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:03:20,499 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:03:20,499 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:03:20,499 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:03:20,499 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:03:20,499 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:03:20,499 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:03:20,500 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T09:03:20,500 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T09:03:20,500 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,500 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,576 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:03:20,576 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:03:20,576 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:03:20,576 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:03:20,577 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:03:20,577 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:03:20,577 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:03:20,577 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:03:20,578 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,578 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:03:20,580 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:03:20,580 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:03:20,642 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T09:03:20,642 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T09:03:20,643 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T09:03:20,643 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T09:03:20,653 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T09:03:20,653 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T09:03:20,654 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T09:03:20,654 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T09:03:20,654 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T09:03:20,654 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T09:03:21,168 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=119070
2025-06-05T09:03:21,169 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T09:03:21,175 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:03:21,175 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]119070
2025-06-05T09:03:21,176 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,176 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,176 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:03:21,176 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:03:21,178 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:03:21,178 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:03:21,185 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T09:03:21,187 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201187
2025-06-05T09:03:21,187 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201187
2025-06-05T09:03:21,188 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201188
2025-06-05T09:03:21,188 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201188
2025-06-05T09:03:21,197 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=119074
2025-06-05T09:03:21,197 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T09:03:21,202 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:03:21,202 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]119074
2025-06-05T09:03:21,203 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:03:21,203 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,203 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,203 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T09:03:21,203 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:03:21,203 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:03:21,203 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:03:21,205 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201205
2025-06-05T09:03:21,205 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201205
2025-06-05T09:03:21,205 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201205
2025-06-05T09:03:21,205 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T09:03:21,205 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201205
2025-06-05T09:03:21,215 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T09:03:21,219 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:03:21,220 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:03:21,231 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:03:21,231 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:03:21,242 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=119082
2025-06-05T09:03:21,242 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T09:03:21,246 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:03:21,247 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]119082
2025-06-05T09:03:21,247 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:03:21,247 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,247 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:03:21,247 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,247 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:03:21,247 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:03:21,249 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T09:03:21,249 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201249
2025-06-05T09:03:21,249 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201249
2025-06-05T09:03:21,250 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201250
2025-06-05T09:03:21,250 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201250
2025-06-05T09:03:21,261 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T09:03:21,289 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:03:21,290 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:03:21,308 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=119087
2025-06-05T09:03:21,309 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T09:03:21,312 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:03:21,312 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]119087
2025-06-05T09:03:21,313 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:03:21,313 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:03:21,313 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,313 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,313 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:03:21,313 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:03:21,314 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201314
2025-06-05T09:03:21,314 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T09:03:21,314 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201314
2025-06-05T09:03:21,314 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201314
2025-06-05T09:03:21,314 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201314
2025-06-05T09:03:21,326 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T09:03:21,341 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:03:21,341 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:03:21,354 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:03:21,357 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:03:21,367 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:03:21,367 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:03:21,371 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:03:21,371 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:03:21,411 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:03:21,426 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:03:21,426 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:03:21,447 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=119092
2025-06-05T09:03:21,448 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T09:03:21,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:03:21,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]119092
2025-06-05T09:03:21,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:03:21,452 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:03:21,452 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,452 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,452 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:03:21,452 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:03:21,454 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T09:03:21,454 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201454
2025-06-05T09:03:21,454 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201454
2025-06-05T09:03:21,454 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201454
2025-06-05T09:03:21,454 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201454
2025-06-05T09:03:21,457 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:03:21,466 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 277
2025-06-05T09:03:21,466 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 277
2025-06-05T09:03:21,466 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,466 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T09:03:21,466 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,467 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1295.0|#WorkerName:W-9000-xuoc_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,467 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,470 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:03:21,471 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:03:21,473 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 268
2025-06-05T09:03:21,473 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 268
2025-06-05T09:03:21,473 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,473 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,473 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1218.0|#WorkerName:W-9001-thieudong_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,474 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,483 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:03:21,484 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:03:21,511 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 261
2025-06-05T09:03:21,511 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 261
2025-06-05T09:03:21,511 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,511 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,511 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1169.0|#WorkerName:W-9002-vetlom_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,511 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,516 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=119111
2025-06-05T09:03:21,517 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T09:03:21,521 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:03:21,522 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]119111
2025-06-05T09:03:21,522 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:03:21,522 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:03:21,522 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,522 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:03:21,522 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:03:21,522 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:03:21,523 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201523
2025-06-05T09:03:21,523 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749114201523
2025-06-05T09:03:21,523 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201523
2025-06-05T09:03:21,523 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114201523
2025-06-05T09:03:21,523 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T09:03:21,534 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T09:03:21,544 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 229
2025-06-05T09:03:21,544 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 229
2025-06-05T09:03:21,544 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,544 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,544 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1120.0|#WorkerName:W-9003-bamdinh_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,544 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,549 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:03:21,550 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:03:21,602 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:03:21,614 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:03:21,614 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:03:21,656 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:03:21,667 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:03:21,667 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:03:21,706 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 252
2025-06-05T09:03:21,706 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 252
2025-06-05T09:03:21,707 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,707 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,707 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1207.0|#WorkerName:W-9004-nganmach_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,707 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,739 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 216
2025-06-05T09:03:21,739 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 216
2025-06-05T09:03:21,740 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,740 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:03:21,740 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1162.0|#WorkerName:W-9005-divatma_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:21,740 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114201
2025-06-05T09:03:24,409 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:38930 "GET / HTTP/1.1" 405 2
2025-06-05T09:03:24,410 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114204
2025-06-05T09:10:06,529 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749114606
2025-06-05T09:10:06,532 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749114606532
2025-06-05T09:10:06,532 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749114606532
2025-06-05T09:10:06,533 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114606533
2025-06-05T09:10:06,533 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114606533
2025-06-05T09:10:06,534 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend received inference at: 1749114606
2025-06-05T09:10:06,603 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-06-05T09:10:06,603 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 134, in predict
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py", line 428, in handle
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/e13982913fb44a56aa444b76c17cb597/onnx_handler.py", line 27, in preprocess
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - cv2.error: OpenCV(4.11.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:993: error: (-215:Assertion failed) !buf.empty() in function 'imdecode_'
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - 
2025-06-05T09:10:06,604 [INFO ] W-9003-bamdinh_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:47768 "POST /predictions/bamdinh HTTP/1.1" 503 77
2025-06-05T09:10:06,605 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114606
2025-06-05T09:10:06,605 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 162136, Inference time ns: 73667789
2025-06-05T09:10:06,605 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 162136, Inference time ns: 73667789
2025-06-05T09:10:06,605 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2025-06-05T09:10:06,605 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2025-06-05T09:10:06,606 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114606
2025-06-05T09:11:46,859 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749114706
2025-06-05T09:11:46,859 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749114706859
2025-06-05T09:11:46,859 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749114706859
2025-06-05T09:11:46,860 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114706860
2025-06-05T09:11:46,860 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114706860
2025-06-05T09:11:46,861 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend received inference at: 1749114706
2025-06-05T09:11:46,864 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-06-05T09:11:46,864 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T09:11:46,864 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 134, in predict
2025-06-05T09:11:46,864 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py", line 428, in handle
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/e13982913fb44a56aa444b76c17cb597/onnx_handler.py", line 28, in preprocess
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     img = cv2.resize(img, (self.imgsz, self.imgsz))
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - cv2.error: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:57098 "POST /predictions/bamdinh HTTP/1.1" 503 6
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - 
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114706
2025-06-05T09:11:46,865 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 89623, Inference time ns: 5599307
2025-06-05T09:11:46,865 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 89623, Inference time ns: 5599307
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4
2025-06-05T09:11:46,865 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114706
2025-06-05T09:14:29,730 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749114869
2025-06-05T09:14:29,731 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749114869731
2025-06-05T09:14:29,731 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749114869731
2025-06-05T09:14:29,731 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114869731
2025-06-05T09:14:29,731 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749114869731
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend received inference at: 1749114869
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 134, in predict
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py", line 428, in handle
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/e13982913fb44a56aa444b76c17cb597/onnx_handler.py", line 28, in preprocess
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     img = cv2.resize(img, (self.imgsz, self.imgsz))
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - cv2.error: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - 
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:47464 "POST /predictions/bamdinh HTTP/1.1" 503 5
2025-06-05T09:14:29,732 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114869
2025-06-05T09:14:29,733 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 76751, Inference time ns: 1994456
2025-06-05T09:14:29,733 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 76751, Inference time ns: 1994456
2025-06-05T09:14:29,733 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2025-06-05T09:14:29,733 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2025-06-05T09:14:29,733 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114869
2025-06-05T09:14:47,521 [INFO ] epollEventLoopGroup-3-6 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:60788 "GET /models/bamdinh HTTP/1.1" 200 3
2025-06-05T09:14:47,521 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749114887
2025-06-05T09:18:08,399 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:18:08,399 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:18:08,456 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:18:08,456 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:18:08,456 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:18:08,456 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:18:08,493 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:18:08,493 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:18:08,664 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: logs/config/20250605091727419-shutdown.cfg
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:18:08,664 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: logs/config/20250605091727419-shutdown.cfg
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:18:08,672 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20250605091727419-shutdown.cfg",
  "modelCount": 6,
  "created": 1749115047419,
  "models": {
    "divatma": {
      "1.0": {
        "defaultVersion": true,
        "marName": "divatma.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "vetlom": {
      "1.0": {
        "defaultVersion": true,
        "marName": "vetlom.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "nganmach": {
      "1.0": {
        "defaultVersion": true,
        "marName": "nganmach.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "xuoc": {
      "1.0": {
        "defaultVersion": true,
        "marName": "xuoc.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "bamdinh": {
      "1.0": {
        "defaultVersion": true,
        "marName": "bamdinh.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "thieudong": {
      "1.0": {
        "defaultVersion": true,
        "marName": "thieudong.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    }
  }
}
2025-06-05T09:18:08,672 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20250605091727419-shutdown.cfg",
  "modelCount": 6,
  "created": 1749115047419,
  "models": {
    "divatma": {
      "1.0": {
        "defaultVersion": true,
        "marName": "divatma.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "vetlom": {
      "1.0": {
        "defaultVersion": true,
        "marName": "vetlom.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "nganmach": {
      "1.0": {
        "defaultVersion": true,
        "marName": "nganmach.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "xuoc": {
      "1.0": {
        "defaultVersion": true,
        "marName": "xuoc.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "bamdinh": {
      "1.0": {
        "defaultVersion": true,
        "marName": "bamdinh.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "thieudong": {
      "1.0": {
        "defaultVersion": true,
        "marName": "thieudong.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    }
  }
}
2025-06-05T09:18:08,676 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20250605091727419-shutdown.cfg
2025-06-05T09:18:08,676 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20250605091727419-shutdown.cfg
2025-06-05T09:18:08,677 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20250605091727419-shutdown.cfg validated successfully
2025-06-05T09:18:08,677 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20250605091727419-shutdown.cfg validated successfully
2025-06-05T09:18:08,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:18:08,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:18:08,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:18:08,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:18:08,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:18:08,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:18:08,782 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:18:08,782 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:18:08,782 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:18:08,782 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:18:08,787 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:08,787 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:08,857 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:18:08,857 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:18:08,857 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:18:08,857 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:18:08,857 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:18:08,857 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:18:08,858 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:18:08,858 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:18:08,858 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:18:08,858 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:18:08,859 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:08,859 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:08,933 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:18:08,933 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:18:08,933 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:18:08,933 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:18:08,933 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:18:08,933 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:18:08,933 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:18:08,933 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:18:08,934 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:18:08,934 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:18:08,934 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:08,934 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:09,006 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:18:09,006 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:18:09,006 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:18:09,006 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:18:09,007 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:18:09,007 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:18:09,007 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:18:09,007 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:18:09,007 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:18:09,007 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:18:09,008 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:09,008 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:09,082 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:18:09,082 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:18:09,083 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:18:09,083 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:18:09,083 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:18:09,083 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:18:09,083 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:18:09,083 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:18:09,083 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:18:09,083 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:18:09,085 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:09,085 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:09,175 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:18:09,175 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:18:09,176 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:18:09,176 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:18:09,176 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:18:09,176 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:18:09,176 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:18:09,176 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:18:09,176 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:18:09,176 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:18:09,191 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:09,191 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:18:09,194 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:18:09,194 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:18:09,256 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T09:18:09,256 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T09:18:09,257 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T09:18:09,257 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T09:18:09,257 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T09:18:09,257 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T09:18:09,258 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T09:18:09,258 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T09:18:09,258 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T09:18:09,258 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T09:18:09,804 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=123043
2025-06-05T09:18:09,805 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T09:18:09,806 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=123052
2025-06-05T09:18:09,807 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T09:18:09,809 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:18:09,809 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]123043
2025-06-05T09:18:09,810 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:18:09,810 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:18:09,810 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,810 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,812 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:18:09,812 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]123052
2025-06-05T09:18:09,813 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:18:09,813 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:18:09,813 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:18:09,813 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:18:09,813 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,813 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,814 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:18:09,814 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:18:09,818 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T09:18:09,818 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T09:18:09,819 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089819
2025-06-05T09:18:09,819 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089819
2025-06-05T09:18:09,819 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089819
2025-06-05T09:18:09,819 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089819
2025-06-05T09:18:09,821 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089820
2025-06-05T09:18:09,820 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089820
2025-06-05T09:18:09,821 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089820
2025-06-05T09:18:09,820 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089820
2025-06-05T09:18:09,824 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=123066
2025-06-05T09:18:09,825 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T09:18:09,830 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:18:09,830 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]123066
2025-06-05T09:18:09,830 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,830 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,830 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:18:09,831 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:18:09,831 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:18:09,831 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:18:09,832 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=123047
2025-06-05T09:18:09,833 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T09:18:09,835 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089835
2025-06-05T09:18:09,835 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T09:18:09,835 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089835
2025-06-05T09:18:09,836 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089836
2025-06-05T09:18:09,836 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089836
2025-06-05T09:18:09,853 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:18:09,858 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T09:18:09,858 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T09:18:09,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]123047
2025-06-05T09:18:09,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:18:09,858 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:18:09,859 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,859 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,859 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:18:09,859 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:18:09,860 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T09:18:09,862 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089862
2025-06-05T09:18:09,862 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089862
2025-06-05T09:18:09,862 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089862
2025-06-05T09:18:09,862 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T09:18:09,862 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089862
2025-06-05T09:18:09,880 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T09:18:09,896 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:18:09,896 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:18:09,896 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:18:09,896 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:18:09,896 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:18:09,896 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:18:09,897 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:18:09,897 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:18:09,947 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=123071
2025-06-05T09:18:09,947 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T09:18:09,951 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:18:09,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]123071
2025-06-05T09:18:09,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:18:09,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:18:09,952 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,952 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,952 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:18:09,952 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:18:09,953 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089953
2025-06-05T09:18:09,953 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089953
2025-06-05T09:18:09,953 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089953
2025-06-05T09:18:09,953 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T09:18:09,953 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089953
2025-06-05T09:18:09,975 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T09:18:09,975 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=123130
2025-06-05T09:18:09,975 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T09:18:09,979 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:18:09,979 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]123130
2025-06-05T09:18:09,979 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:18:09,979 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,979 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:18:09,979 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:18:09,979 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:18:09,979 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:18:09,980 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T09:18:09,980 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089980
2025-06-05T09:18:09,980 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115089980
2025-06-05T09:18:09,980 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089980
2025-06-05T09:18:09,980 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115089980
2025-06-05T09:18:09,996 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:18:09,997 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T09:18:09,997 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:18:10,012 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:18:10,012 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:18:10,079 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:18:10,080 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:18:10,082 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:18:10,083 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:18:10,096 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:18:10,096 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:18:10,097 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:18:10,097 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:18:10,097 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:18:10,098 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:18:10,099 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:18:10,099 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:18:10,120 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:18:10,150 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:18:10,151 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:18:10,162 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:18:10,179 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:18:10,179 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:18:10,212 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 391
2025-06-05T09:18:10,212 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 391
2025-06-05T09:18:10,212 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,212 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,212 [INFO ] W-9002-nganmach_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1278.0|#WorkerName:W-9002-nganmach_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,213 [INFO ] W-9002-nganmach_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,229 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 393
2025-06-05T09:18:10,229 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 393
2025-06-05T09:18:10,230 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,230 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,230 [INFO ] W-9003-xuoc_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1223.0|#WorkerName:W-9003-xuoc_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,230 [INFO ] W-9003-xuoc_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,239 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 377
2025-06-05T09:18:10,239 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 377
2025-06-05T09:18:10,239 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,239 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,240 [INFO ] W-9001-vetlom_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1381.0|#WorkerName:W-9001-vetlom_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,240 [INFO ] W-9001-vetlom_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,240 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 419
2025-06-05T09:18:10,240 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 419
2025-06-05T09:18:10,240 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,240 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,240 [INFO ] W-9000-divatma_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1455.0|#WorkerName:W-9000-divatma_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,240 [INFO ] W-9000-divatma_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,247 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 294
2025-06-05T09:18:10,247 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 294
2025-06-05T09:18:10,247 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,247 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,247 [INFO ] W-9004-bamdinh_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1164.0|#WorkerName:W-9004-bamdinh_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,247 [INFO ] W-9004-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:0.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,271 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 288
2025-06-05T09:18:10,271 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 288
2025-06-05T09:18:10,271 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,271 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:18:10,272 [INFO ] W-9005-thieudong_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1094.0|#WorkerName:W-9005-thieudong_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:10,272 [INFO ] W-9005-thieudong_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115090
2025-06-05T09:18:19,985 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:44684 "GET / HTTP/1.1" 405 3
2025-06-05T09:18:19,985 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115099
2025-06-05T09:19:32,893 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749115172
2025-06-05T09:19:32,895 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749115172895
2025-06-05T09:19:32,895 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749115172895
2025-06-05T09:19:32,895 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115172895
2025-06-05T09:19:32,895 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115172895
2025-06-05T09:19:32,897 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend received inference at: 1749115172
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Invoking custom service failed.
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/service.py", line 134, in predict
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py", line 428, in handle
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/12b797488c0349b1a03c11fa2875754d/onnx_handler.py", line 28, in preprocess
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     img = cv2.resize(img, (self.imgsz, self.imgsz))
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - cv2.error: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'
2025-06-05T09:19:32,952 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - 
2025-06-05T09:19:32,953 [INFO ] W-9004-bamdinh_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:47614 "POST /predictions/bamdinh HTTP/1.1" 503 62
2025-06-05T09:19:32,953 [INFO ] W-9004-bamdinh_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115172
2025-06-05T09:19:32,954 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 139070, Inference time ns: 58730010
2025-06-05T09:19:32,954 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 139070, Inference time ns: 58730010
2025-06-05T09:19:32,954 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2025-06-05T09:19:32,954 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2025-06-05T09:19:32,954 [INFO ] W-9004-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115172
2025-06-05T09:30:02,498 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:30:02,498 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T09:30:02,501 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:30:02,501 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T09:30:02,553 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:30:02,553 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T09:30:02,761 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:30:02,761 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko/torchserve
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Metrics dir: /root/Code/TestCycletimeMeiko/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: true
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T09:30:02,773 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:30:02,773 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T09:30:02,784 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T09:30:02,784 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T09:30:02,896 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:30:02,896 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T09:30:02,897 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:30:02,897 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T09:30:02,897 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:30:02,897 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T09:30:02,897 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:30:02,897 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T09:30:02,901 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T09:30:02,901 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T09:30:02,901 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:02,901 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:02,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:30:02,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T09:30:02,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:30:02,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T09:30:02,973 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:30:02,973 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T09:30:02,974 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:30:02,974 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T09:30:02,974 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T09:30:02,974 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T09:30:02,975 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:02,975 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,045 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:30:03,045 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T09:30:03,045 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:30:03,045 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T09:30:03,045 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:30:03,045 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T09:30:03,045 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:30:03,045 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T09:30:03,046 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T09:30:03,046 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T09:30:03,046 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,046 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,116 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:30:03,116 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T09:30:03,116 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:30:03,116 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T09:30:03,117 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:30:03,117 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T09:30:03,117 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:30:03,117 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T09:30:03,117 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T09:30:03,117 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T09:30:03,118 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,118 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,187 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:30:03,187 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T09:30:03,187 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:30:03,187 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T09:30:03,187 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:30:03,187 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T09:30:03,187 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:30:03,187 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T09:30:03,188 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T09:30:03,188 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T09:30:03,188 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,188 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:30:03,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T09:30:03,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:30:03,262 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T09:30:03,262 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:30:03,262 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T09:30:03,263 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:30:03,263 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T09:30:03,270 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:30:03,270 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T09:30:03,269 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,269 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T09:30:03,352 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T09:30:03,352 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-06-05T09:30:03,352 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T09:30:03,352 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T09:30:03,353 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T09:30:03,353 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2025-06-05T09:30:03,353 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T09:30:03,353 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T09:30:03,354 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T09:30:03,354 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2025-06-05T09:30:03,952 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=126368
2025-06-05T09:30:03,952 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=126364
2025-06-05T09:30:03,953 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T09:30:03,953 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T09:30:03,952 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=126390
2025-06-05T09:30:03,953 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T09:30:03,957 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:30:03,958 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]126390
2025-06-05T09:30:03,958 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:30:03,958 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:30:03,958 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:30:03,958 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]126368
2025-06-05T09:30:03,958 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:30:03,958 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:30:03,958 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:30:03,958 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:03,958 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:03,958 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:03,958 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:03,959 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]126364
2025-06-05T09:30:03,959 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:30:03,959 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:30:03,959 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:03,959 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:03,961 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:30:03,961 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:30:03,962 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:30:03,961 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T09:30:03,961 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T09:30:03,962 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T09:30:03,967 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T09:30:03,967 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T09:30:03,967 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T09:30:03,968 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=126382
2025-06-05T09:30:03,968 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T09:30:03,969 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115803969
2025-06-05T09:30:03,969 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115803969
2025-06-05T09:30:03,969 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115803969
2025-06-05T09:30:03,969 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115803969
2025-06-05T09:30:03,969 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115803969
2025-06-05T09:30:03,969 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115803969
2025-06-05T09:30:03,971 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115803971
2025-06-05T09:30:03,971 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115803971
2025-06-05T09:30:03,971 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115803971
2025-06-05T09:30:03,971 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115803971
2025-06-05T09:30:03,971 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115803971
2025-06-05T09:30:03,971 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115803971
2025-06-05T09:30:03,973 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:30:03,973 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]126382
2025-06-05T09:30:03,973 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:03,973 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:30:03,973 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:03,974 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:30:03,974 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:30:03,974 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T09:30:03,975 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115803975
2025-06-05T09:30:03,975 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115803975
2025-06-05T09:30:03,975 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T09:30:03,975 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115803975
2025-06-05T09:30:03,975 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115803975
2025-06-05T09:30:03,997 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T09:30:03,998 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T09:30:03,997 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=126398
2025-06-05T09:30:03,998 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T09:30:04,002 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T09:30:04,003 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T09:30:04,003 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:30:04,003 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]126398
2025-06-05T09:30:04,004 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:04,004 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:30:04,004 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:04,004 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:30:04,004 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:30:04,004 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T09:30:04,005 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115804005
2025-06-05T09:30:04,005 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T09:30:04,005 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115804005
2025-06-05T09:30:04,005 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115804005
2025-06-05T09:30:04,005 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115804005
2025-06-05T09:30:04,026 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T09:30:04,041 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:30:04,041 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:30:04,041 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:30:04,042 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:30:04,042 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:30:04,042 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:30:04,042 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:30:04,042 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:30:04,042 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:30:04,042 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:30:04,129 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=126457
2025-06-05T09:30:04,130 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T09:30:04,133 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T09:30:04,133 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]126457
2025-06-05T09:30:04,133 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T09:30:04,133 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:04,134 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T09:30:04,133 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T09:30:04,134 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:30:04,134 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T09:30:04,135 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115804135
2025-06-05T09:30:04,135 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T09:30:04,135 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749115804135
2025-06-05T09:30:04,135 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115804135
2025-06-05T09:30:04,135 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115804135
2025-06-05T09:30:04,153 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T09:30:04,168 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T09:30:04,169 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T09:30:04,218 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:30:04,219 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:30:04,221 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:30:04,228 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:30:04,233 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:30:04,234 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:30:04,234 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:30:04,234 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:30:04,234 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:30:04,235 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:30:04,235 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:30:04,243 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:30:04,244 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:30:04,274 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:30:04,274 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:30:04,320 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T09:30:04,333 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T09:30:04,333 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T09:30:04,342 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 371
2025-06-05T09:30:04,342 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 371
2025-06-05T09:30:04,343 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,343 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,344 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1226.0|#WorkerName:W-9003-bamdinh_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,345 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,348 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 343
2025-06-05T09:30:04,348 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 377
2025-06-05T09:30:04,348 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 343
2025-06-05T09:30:04,348 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 377
2025-06-05T09:30:04,348 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,348 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,348 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,348 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,348 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1161.0|#WorkerName:W-9004-nganmach_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,348 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1448.0|#WorkerName:W-9000-xuoc_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,349 [INFO ] W-9004-nganmach_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,349 [INFO ] W-9000-xuoc_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,359 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 384
2025-06-05T09:30:04,359 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 384
2025-06-05T09:30:04,359 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,359 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,359 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1313.0|#WorkerName:W-9002-vetlom_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,359 [INFO ] W-9002-vetlom_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:0.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,379 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 408
2025-06-05T09:30:04,379 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 408
2025-06-05T09:30:04,379 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,379 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,379 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1405.0|#WorkerName:W-9001-thieudong_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,380 [INFO ] W-9001-thieudong_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,404 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 264
2025-06-05T09:30:04,404 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 264
2025-06-05T09:30:04,404 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,404 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-06-05T09:30:04,404 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1141.0|#WorkerName:W-9005-divatma_1.0,Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:04,405 [INFO ] W-9005-divatma_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:6.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115804
2025-06-05T09:30:09,862 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:49562 "GET / HTTP/1.1" 405 2
2025-06-05T09:30:09,863 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115809
2025-06-05T09:31:15,612 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749115875
2025-06-05T09:31:15,614 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749115875613
2025-06-05T09:31:15,614 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749115875613
2025-06-05T09:31:15,614 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115875614
2025-06-05T09:31:15,614 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115875614
2025-06-05T09:31:15,615 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend received inference at: 1749115875
2025-06-05T09:31:15,733 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:117.05|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749115875,5a7ed79d-4a24-4287-a569-c91e216629e7, pattern=[METRICS]
2025-06-05T09:31:15,733 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:117.05|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749115875,5a7ed79d-4a24-4287-a569-c91e216629e7, pattern=[METRICS]
2025-06-05T09:31:15,735 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_METRICS - HandlerTime.ms:117.05|#ModelName:bamdinh,Level:Model|#hostname:DESKTOP-G57G3K8,requestID:5a7ed79d-4a24-4287-a569-c91e216629e7,timestamp:1749115875
2025-06-05T09:31:15,740 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:117.29|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749115875,5a7ed79d-4a24-4287-a569-c91e216629e7, pattern=[METRICS]
2025-06-05T09:31:15,740 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:117.29|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749115875,5a7ed79d-4a24-4287-a569-c91e216629e7, pattern=[METRICS]
2025-06-05T09:31:15,740 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_METRICS - PredictionTime.ms:117.29|#ModelName:bamdinh,Level:Model|#hostname:DESKTOP-G57G3K8,requestID:5a7ed79d-4a24-4287-a569-c91e216629e7,timestamp:1749115875
2025-06-05T09:31:15,760 [INFO ] W-9003-bamdinh_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:48844 "POST /predictions/bamdinh HTTP/1.1" 200 150
2025-06-05T09:31:15,761 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115875
2025-06-05T09:31:15,765 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:146122.794|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749115875
2025-06-05T09:31:15,765 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:114.469|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749115875
2025-06-05T09:31:15,765 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 114469, Backend time ns: 151371600
2025-06-05T09:31:15,765 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 114469, Backend time ns: 151371600
2025-06-05T09:31:15,765 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115875
2025-06-05T09:31:15,765 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 145
2025-06-05T09:31:15,765 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 145
2025-06-05T09:31:15,766 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:8.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115875
2025-06-05T09:31:52,374 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749115912
2025-06-05T09:31:52,374 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749115912374
2025-06-05T09:31:52,374 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749115912374
2025-06-05T09:31:52,374 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115912374
2025-06-05T09:31:52,374 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749115912374
2025-06-05T09:31:52,375 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend received inference at: 1749115912
2025-06-05T09:31:52,423 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:47.25|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749115912,12fbcbe7-4eeb-48b2-a086-ba946d6611b6, pattern=[METRICS]
2025-06-05T09:31:52,423 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:47.25|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749115912,12fbcbe7-4eeb-48b2-a086-ba946d6611b6, pattern=[METRICS]
2025-06-05T09:31:52,423 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_METRICS - HandlerTime.ms:47.25|#ModelName:bamdinh,Level:Model|#hostname:DESKTOP-G57G3K8,requestID:12fbcbe7-4eeb-48b2-a086-ba946d6611b6,timestamp:1749115912
2025-06-05T09:31:52,423 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:47.46|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749115912,12fbcbe7-4eeb-48b2-a086-ba946d6611b6, pattern=[METRICS]
2025-06-05T09:31:52,423 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:47.46|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749115912,12fbcbe7-4eeb-48b2-a086-ba946d6611b6, pattern=[METRICS]
2025-06-05T09:31:52,423 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_METRICS - PredictionTime.ms:47.46|#ModelName:bamdinh,Level:Model|#hostname:DESKTOP-G57G3K8,requestID:12fbcbe7-4eeb-48b2-a086-ba946d6611b6,timestamp:1749115912
2025-06-05T09:32:03,770 [INFO ] W-9003-bamdinh_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:57806 "POST /predictions/bamdinh HTTP/1.1" 200 11397
2025-06-05T09:32:03,771 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115923
2025-06-05T09:32:03,771 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:87278.573|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749115923
2025-06-05T09:32:03,771 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:205.327|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749115923
2025-06-05T09:32:03,771 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 205327, Backend time ns: 88146651
2025-06-05T09:32:03,771 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 205327, Backend time ns: 88146651
2025-06-05T09:32:03,771 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115923
2025-06-05T09:32:03,771 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 11396
2025-06-05T09:32:03,771 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 11396
2025-06-05T09:32:03,771 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749115923
2025-06-05T09:34:09,725 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749116049
2025-06-05T09:34:09,725 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749116049725
2025-06-05T09:34:09,725 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1749116049725
2025-06-05T09:34:09,725 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749116049725
2025-06-05T09:34:09,725 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749116049725
2025-06-05T09:34:09,726 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend received inference at: 1749116049
2025-06-05T09:34:09,757 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:30.11|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749116049,28fea39d-af64-477a-929a-45820f2288b9, pattern=[METRICS]
2025-06-05T09:34:09,757 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:30.11|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749116049,28fea39d-af64-477a-929a-45820f2288b9, pattern=[METRICS]
2025-06-05T09:34:09,757 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_METRICS - HandlerTime.ms:30.11|#ModelName:bamdinh,Level:Model|#hostname:DESKTOP-G57G3K8,requestID:28fea39d-af64-477a-929a-45820f2288b9,timestamp:1749116049
2025-06-05T09:34:09,757 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:30.29|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749116049,28fea39d-af64-477a-929a-45820f2288b9, pattern=[METRICS]
2025-06-05T09:34:09,757 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:30.29|#ModelName:bamdinh,Level:Model|#type:GAUGE|#hostname:DESKTOP-G57G3K8,1749116049,28fea39d-af64-477a-929a-45820f2288b9, pattern=[METRICS]
2025-06-05T09:34:09,757 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_METRICS - PredictionTime.ms:30.29|#ModelName:bamdinh,Level:Model|#hostname:DESKTOP-G57G3K8,requestID:28fea39d-af64-477a-929a-45820f2288b9,timestamp:1749116049
2025-06-05T09:34:09,795 [INFO ] W-9003-bamdinh_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:35558 "POST /predictions/bamdinh HTTP/1.1" 200 70
2025-06-05T09:34:09,796 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749116049
2025-06-05T09:34:09,796 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:69810.22|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749116049
2025-06-05T09:34:09,796 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:86.839|#model_name:bamdinh,model_version:default|#hostname:DESKTOP-G57G3K8,timestamp:1749116049
2025-06-05T09:34:09,796 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 86839, Backend time ns: 70701779
2025-06-05T09:34:09,796 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 86839, Backend time ns: 70701779
2025-06-05T09:34:09,796 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749116049
2025-06-05T09:34:09,796 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2025-06-05T09:34:09,796 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2025-06-05T09:34:09,796 [INFO ] W-9003-bamdinh_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:DESKTOP-G57G3K8,timestamp:1749116049
