2025-06-05T04:17:23,973 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:17:23,973 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:17:24,028 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:17:24,028 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:17:24,029 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:17:24,029 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:17:24,056 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:17:24,056 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:17:12,955 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/logs
Metrics dir: /root/Code/TestCycletimeMeiko/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:17:12,955 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/logs
Metrics dir: /root/Code/TestCycletimeMeiko/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:17:12,960 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T04:17:12,960 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: xuoc.mar
2025-06-05T04:17:13,059 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:17:13,059 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:17:13,059 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:17:13,059 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:17:13,060 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:17:13,060 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:17:13,060 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:17:13,060 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:17:13,063 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T04:17:13,063 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: thieudong.mar
2025-06-05T04:17:13,063 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,063 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,137 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:17:13,137 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:17:13,137 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:17:13,137 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:17:13,137 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:17:13,137 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:17:13,137 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:17:13,137 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:17:13,138 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T04:17:13,138 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: vetlom.mar
2025-06-05T04:17:13,139 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,139 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,222 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:17:13,222 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:17:13,222 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:17:13,222 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:17:13,222 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:17:13,222 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:17:13,223 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:17:13,223 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:17:13,223 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T04:17:13,223 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: bamdinh.mar
2025-06-05T04:17:13,224 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,224 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,293 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:17:13,293 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:17:13,293 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:17:13,293 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:17:13,294 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:17:13,294 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:17:13,294 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:17:13,294 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:17:13,294 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T04:17:13,294 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: nganmach.mar
2025-06-05T04:17:13,294 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,294 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,371 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:17:13,371 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:17:13,371 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:17:13,371 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:17:13,371 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:17:13,371 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:17:13,371 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:17:13,371 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:17:13,372 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T04:17:13,372 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,372 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,372 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: divatma.mar
2025-06-05T04:17:13,498 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:17:13,498 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:17:13,498 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:17:13,498 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:17:13,498 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:17:13,498 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:17:13,498 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:17:13,498 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:17:13,500 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,500 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:13,502 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:17:13,502 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:17:13,955 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=43574
2025-06-05T04:17:13,955 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=43570
2025-06-05T04:17:13,956 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:17:13,956 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:17:13,963 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:13,964 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - [PID]43574
2025-06-05T04:17:13,964 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:13,964 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:13,965 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - [PID]43570
2025-06-05T04:17:13,965 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:13,965 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:13,965 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:13,965 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:13,965 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:13,965 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:13,968 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:13,968 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:13,968 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:13,968 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:13,968 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:13,978 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:17:13,979 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:17:13,980 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097033980
2025-06-05T04:17:13,980 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097033980
2025-06-05T04:17:13,981 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097033981
2025-06-05T04:17:13,981 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097033981
2025-06-05T04:17:13,981 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097033981
2025-06-05T04:17:13,981 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097033981
2025-06-05T04:17:13,985 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097033985
2025-06-05T04:17:13,985 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097033985
2025-06-05T04:17:14,010 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:17:14,014 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:17:14,040 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:14,040 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:14,041 [WARN ] W-9000-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:14,041 [WARN ] W-9001-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:14,046 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=43579
2025-06-05T04:17:14,046 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:17:14,050 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:14,050 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - [PID]43579
2025-06-05T04:17:14,050 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:14,050 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:14,050 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:14,051 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:14,050 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:14,051 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:14,052 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097034052
2025-06-05T04:17:14,052 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097034052
2025-06-05T04:17:14,052 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:17:14,052 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097034052
2025-06-05T04:17:14,052 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097034052
2025-06-05T04:17:14,068 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:17:14,086 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:14,086 [WARN ] W-9002-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:14,143 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:14,143 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:14,143 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:14,144 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:14,147 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:14,148 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:14,159 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:14,159 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:14,159 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:14,159 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:14,160 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:14,160 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:14,160 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:14,160 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:14,160 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:14,160 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:14,160 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:14,160 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:14,160 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:14,161 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:14,161 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:14,161 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:14,161 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,161 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:14,161 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,161 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:14,161 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,161 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:14,161 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,161 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:14,162 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,162 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:14,162 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,162 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,162 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,162 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:14,162 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:14,162 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:14,162 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:14,162 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:14,162 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/24b684d45bec4fe7a3ace0083e627f40/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:14,162 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:14,162 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:14,163 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:14,163 [INFO ] W-9001-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:14,163 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/cf7ae8e275a6426b9174e56aef5e1e0a/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:14,163 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:14,163 [INFO ] W-9000-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:14,164 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:14,165 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,165 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:14,165 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,166 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:14,166 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,166 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:14,166 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,166 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:14,166 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:14,166 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:14,166 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:14,167 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:14,167 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:14,167 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:14,167 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:14,167 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:14,167 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:14,167 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/b882f648329d4014912fc7b8ada1196c/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:14,168 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:14,168 [INFO ] W-9002-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:14,162 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,162 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,166 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,162 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,162 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,166 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,171 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:14,171 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:14,171 [WARN ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:14,171 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:14,171 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,171 [WARN ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:14,171 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,171 [WARN ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:14,171 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,171 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,171 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,171 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,171 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034171
2025-06-05T04:17:14,171 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034171
2025-06-05T04:17:14,171 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034171
2025-06-05T04:17:14,171 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034171
2025-06-05T04:17:14,171 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034171
2025-06-05T04:17:14,171 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034171
2025-06-05T04:17:14,173 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:17:14,173 [INFO ] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:17:14,173 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:17:14,173 [INFO ] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:17:14,173 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:17:14,173 [INFO ] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:17:14,182 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:17:14,182 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:17:14,182 [INFO ] W-9001-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stderr
2025-06-05T04:17:14,182 [INFO ] W-9001-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-thieudong_1.0-stdout
2025-06-05T04:17:14,182 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:17:14,182 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:17:14,182 [INFO ] W-9000-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stderr
2025-06-05T04:17:14,182 [INFO ] W-9000-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-xuoc_1.0-stdout
2025-06-05T04:17:14,183 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:17:14,183 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:17:14,183 [INFO ] W-9002-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stderr
2025-06-05T04:17:14,183 [INFO ] W-9002-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-vetlom_1.0-stdout
2025-06-05T04:17:14,184 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=43584
2025-06-05T04:17:14,184 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:17:14,188 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:14,188 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - [PID]43584
2025-06-05T04:17:14,189 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:14,189 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:14,189 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:14,189 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:14,189 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:14,189 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:14,190 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097034190
2025-06-05T04:17:14,190 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:17:14,190 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097034190
2025-06-05T04:17:14,190 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097034190
2025-06-05T04:17:14,190 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097034190
2025-06-05T04:17:14,197 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:17:14,212 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:14,213 [WARN ] W-9003-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:14,220 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=43617
2025-06-05T04:17:14,220 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:17:14,223 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:14,224 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - [PID]43617
2025-06-05T04:17:14,224 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:14,224 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:14,224 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:14,224 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:14,224 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:14,224 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:14,225 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097034225
2025-06-05T04:17:14,225 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097034225
2025-06-05T04:17:14,225 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:17:14,225 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097034225
2025-06-05T04:17:14,225 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097034225
2025-06-05T04:17:14,233 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:17:14,250 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:14,251 [WARN ] W-9004-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:14,299 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:14,299 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:14,299 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:14,299 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:14,310 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:14,310 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:14,310 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:14,310 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:14,310 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:14,310 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:14,310 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:14,310 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:14,310 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:14,310 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:14,310 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:14,310 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:14,310 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:14,310 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:14,310 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,310 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,310 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:14,310 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:14,311 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:14,311 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,311 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,311 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/7db0b87bda7c4a6fa0ca04fb9f0ce043/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:14,311 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:14,311 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,311 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,311 [INFO ] W-9004-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:14,311 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:14,311 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,311 [WARN ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:14,311 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,311 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034311
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/0ff71da13d244a93aae74c25bd85dd09/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:14,311 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034311
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:14,311 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,311 [INFO ] W-9003-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:14,312 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:14,312 [WARN ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:14,312 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,312 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,312 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034312
2025-06-05T04:17:14,312 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034312
2025-06-05T04:17:14,312 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:17:14,312 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:17:14,312 [INFO ] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:17:14,312 [INFO ] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:17:14,319 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:17:14,319 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:17:14,319 [INFO ] W-9003-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stderr
2025-06-05T04:17:14,319 [INFO ] W-9003-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-bamdinh_1.0-stdout
2025-06-05T04:17:14,322 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:17:14,322 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:17:14,322 [INFO ] W-9004-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stderr
2025-06-05T04:17:14,322 [INFO ] W-9004-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-nganmach_1.0-stdout
2025-06-05T04:17:14,324 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=43652
2025-06-05T04:17:14,325 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:17:14,328 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:14,329 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - [PID]43652
2025-06-05T04:17:14,329 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:14,329 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:14,329 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:14,329 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:14,329 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:14,329 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:14,330 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:17:14,330 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097034330
2025-06-05T04:17:14,330 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097034330
2025-06-05T04:17:14,330 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097034330
2025-06-05T04:17:14,330 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097034330
2025-06-05T04:17:14,338 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:17:14,354 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:14,354 [WARN ] W-9005-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:14,430 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:14,430 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:14,441 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:14,441 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:14,441 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,441 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:14,441 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:14,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:14,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/c71238b850a84ee2ab337858ffce2574/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:14,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:14,442 [INFO ] W-9005-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:14,442 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,442 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:14,442 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:14,442 [WARN ] W-9005-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:14,442 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,442 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:14,442 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034442
2025-06-05T04:17:14,442 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097034442
2025-06-05T04:17:14,443 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:17:14,443 [INFO ] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:17:14,450 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:17:14,450 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:17:14,450 [INFO ] W-9005-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stdout
2025-06-05T04:17:14,450 [INFO ] W-9005-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-divatma_1.0-stderr
2025-06-05T04:17:15,175 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,175 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,175 [DEBUG] W-9000-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,175 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,175 [DEBUG] W-9002-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,175 [DEBUG] W-9001-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,312 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,312 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,312 [DEBUG] W-9003-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,312 [DEBUG] W-9004-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,443 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,443 [DEBUG] W-9005-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:15,587 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-06-05T04:17:15,587 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-06-05T04:17:44,773 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:17:44,773 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-06-05T04:17:44,829 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:17:44,829 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-06-05T04:17:44,829 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:17:44,829 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-06-05T04:17:44,858 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:17:44,858 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-06-05T04:17:45,069 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: logs/config/20250605041715589-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/logs
Metrics dir: /root/Code/TestCycletimeMeiko/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:17:45,069 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages
Current directory: /root/Code/TestCycletimeMeiko
Temp directory: /tmp
Metrics config path: /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 28
Max heap size: 3968 M
Python executable: /root/miniconda3/envs/ts_yolo12/bin/python
Config file: logs/config/20250605041715589-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
Initial Models: all
Log dir: /root/Code/TestCycletimeMeiko/logs
Metrics dir: /root/Code/TestCycletimeMeiko/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /root/Code/TestCycletimeMeiko/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2025-06-05T04:17:45,075 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20250605041715589-shutdown.cfg",
  "modelCount": 6,
  "created": 1749097035590,
  "models": {
    "divatma": {
      "1.0": {
        "defaultVersion": true,
        "marName": "divatma.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "vetlom": {
      "1.0": {
        "defaultVersion": true,
        "marName": "vetlom.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "nganmach": {
      "1.0": {
        "defaultVersion": true,
        "marName": "nganmach.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "xuoc": {
      "1.0": {
        "defaultVersion": true,
        "marName": "xuoc.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "bamdinh": {
      "1.0": {
        "defaultVersion": true,
        "marName": "bamdinh.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "thieudong": {
      "1.0": {
        "defaultVersion": true,
        "marName": "thieudong.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    }
  }
}
2025-06-05T04:17:45,075 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20250605041715589-shutdown.cfg",
  "modelCount": 6,
  "created": 1749097035590,
  "models": {
    "divatma": {
      "1.0": {
        "defaultVersion": true,
        "marName": "divatma.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "vetlom": {
      "1.0": {
        "defaultVersion": true,
        "marName": "vetlom.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "nganmach": {
      "1.0": {
        "defaultVersion": true,
        "marName": "nganmach.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "xuoc": {
      "1.0": {
        "defaultVersion": true,
        "marName": "xuoc.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "bamdinh": {
      "1.0": {
        "defaultVersion": true,
        "marName": "bamdinh.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    },
    "thieudong": {
      "1.0": {
        "defaultVersion": true,
        "marName": "thieudong.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120,
        "runtimeType": "python"
      }
    }
  }
}
2025-06-05T04:17:45,080 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20250605041715589-shutdown.cfg
2025-06-05T04:17:45,080 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20250605041715589-shutdown.cfg
2025-06-05T04:17:45,080 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20250605041715589-shutdown.cfg validated successfully
2025-06-05T04:17:45,080 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20250605041715589-shutdown.cfg validated successfully
2025-06-05T04:17:45,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:17:45,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model divatma
2025-06-05T04:17:45,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:17:45,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:17:45,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:17:45,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model divatma
2025-06-05T04:17:45,189 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:17:45,189 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model divatma loaded.
2025-06-05T04:17:45,190 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:17:45,190 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: divatma, count: 1
2025-06-05T04:17:45,194 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,194 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,265 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:17:45,265 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model vetlom
2025-06-05T04:17:45,265 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:17:45,265 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:17:45,266 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:17:45,266 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model vetlom
2025-06-05T04:17:45,266 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:17:45,266 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model vetlom loaded.
2025-06-05T04:17:45,266 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:17:45,266 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: vetlom, count: 1
2025-06-05T04:17:45,267 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,267 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,334 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:17:45,334 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model nganmach
2025-06-05T04:17:45,334 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:17:45,334 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:17:45,335 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:17:45,335 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model nganmach
2025-06-05T04:17:45,335 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:17:45,335 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model nganmach loaded.
2025-06-05T04:17:45,335 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:17:45,335 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: nganmach, count: 1
2025-06-05T04:17:45,336 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,336 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,407 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:17:45,407 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model xuoc
2025-06-05T04:17:45,407 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:17:45,407 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:17:45,407 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:17:45,407 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model xuoc
2025-06-05T04:17:45,407 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:17:45,407 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model xuoc loaded.
2025-06-05T04:17:45,408 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:17:45,408 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: xuoc, count: 1
2025-06-05T04:17:45,408 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,408 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,489 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:17:45,489 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model bamdinh
2025-06-05T04:17:45,489 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:17:45,489 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:17:45,490 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:17:45,490 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model bamdinh
2025-06-05T04:17:45,490 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:17:45,490 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model bamdinh loaded.
2025-06-05T04:17:45,490 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:17:45,490 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: bamdinh, count: 1
2025-06-05T04:17:45,513 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,513 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,601 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:17:45,601 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model thieudong
2025-06-05T04:17:45,602 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:17:45,602 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:17:45,602 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:17:45,602 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model thieudong
2025-06-05T04:17:45,602 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:17:45,602 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model thieudong loaded.
2025-06-05T04:17:45,602 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:17:45,602 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: thieudong, count: 1
2025-06-05T04:17:45,617 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:17:45,615 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,617 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-06-05T04:17:45,615 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:45,670 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-06-05T04:17:45,670 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-06-05T04:17:45,671 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T04:17:45,671 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-06-05T04:17:45,671 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-06-05T04:17:45,671 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-06-05T04:17:45,672 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T04:17:45,672 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-06-05T04:17:45,672 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-06-05T04:17:45,672 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-06-05T04:17:45,922 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-06-05T04:17:45,922 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-06-05T04:17:45,972 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-06-05T04:17:45,972 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-06-05T04:17:46,014 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=44205
2025-06-05T04:17:46,015 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:17:46,017 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:46,017 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]44205
2025-06-05T04:17:46,017 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:46,018 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:46,018 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,018 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,021 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:46,021 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:46,025 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:17:46,026 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066026
2025-06-05T04:17:46,026 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066026
2025-06-05T04:17:46,029 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066029
2025-06-05T04:17:46,029 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066029
2025-06-05T04:17:46,050 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:17:46,079 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:46,080 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:46,139 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=44209
2025-06-05T04:17:46,139 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:17:46,143 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:46,143 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]44209
2025-06-05T04:17:46,143 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:46,143 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,143 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,143 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:46,144 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:46,144 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:46,145 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066145
2025-06-05T04:17:46,145 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066145
2025-06-05T04:17:46,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:17:46,145 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066145
2025-06-05T04:17:46,145 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066145
2025-06-05T04:17:46,165 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:17:46,177 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:46,177 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:46,181 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:46,182 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:46,190 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:46,191 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:46,191 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:46,191 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:46,191 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:46,191 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:46,191 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:46,191 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:46,192 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:46,192 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:46,192 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:46,193 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,193 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,193 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,193 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,203 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:46,203 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:46,204 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,204 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,204 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066204
2025-06-05T04:17:46,204 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066204
2025-06-05T04:17:46,205 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:17:46,205 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:17:46,213 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:46,213 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:46,213 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:46,213 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:46,267 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=44214
2025-06-05T04:17:46,267 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:17:46,270 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:46,270 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:46,270 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:46,271 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]44214
2025-06-05T04:17:46,271 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:46,271 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:46,271 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,271 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,271 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:46,271 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:46,272 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:17:46,272 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066272
2025-06-05T04:17:46,272 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066272
2025-06-05T04:17:46,272 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066272
2025-06-05T04:17:46,272 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066272
2025-06-05T04:17:46,279 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:17:46,282 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:46,282 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:46,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:46,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:46,283 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,283 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,283 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:46,283 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,283 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,285 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:46,285 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,285 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:46,285 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,285 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:46,285 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:46,285 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:46,285 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:46,285 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:46,285 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:46,285 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:46,285 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,285 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,285 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:46,286 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066286
2025-06-05T04:17:46,286 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:46,286 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066286
2025-06-05T04:17:46,286 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:46,286 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:46,286 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:46,286 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:17:46,286 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:17:46,295 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:46,295 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:46,295 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:46,295 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:46,295 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=44219
2025-06-05T04:17:46,296 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:17:46,299 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:46,299 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]44219
2025-06-05T04:17:46,299 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:46,299 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:46,299 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,299 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,299 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:46,299 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:46,301 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:17:46,301 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066301
2025-06-05T04:17:46,301 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066301
2025-06-05T04:17:46,301 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066301
2025-06-05T04:17:46,301 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066301
2025-06-05T04:17:46,302 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:46,302 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:46,306 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:17:46,323 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:46,323 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:46,357 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=44256
2025-06-05T04:17:46,357 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:17:46,361 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:46,361 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]44256
2025-06-05T04:17:46,361 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:46,361 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,361 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,361 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:46,361 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:46,361 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:46,362 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:17:46,363 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066363
2025-06-05T04:17:46,363 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066363
2025-06-05T04:17:46,363 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066363
2025-06-05T04:17:46,363 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066363
2025-06-05T04:17:46,377 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:17:46,388 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:46,388 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:46,388 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:46,388 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:46,394 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:46,394 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:46,399 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:46,399 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:46,399 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:46,399 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:46,399 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:46,399 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:46,399 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:46,399 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:46,399 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,400 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:46,400 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:46,400 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:46,400 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:46,400 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:46,399 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,400 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:46,400 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:46,399 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,400 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:46,400 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:46,400 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:46,400 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:46,400 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:46,400 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:46,400 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,399 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,400 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,400 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:46,401 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,400 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:46,401 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,401 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:46,401 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:46,401 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:46,401 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,401 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,401 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:46,401 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:46,401 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:46,401 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,401 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:46,401 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,401 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:46,401 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:46,401 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:46,401 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:46,401 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:46,401 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:46,401 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:46,401 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,401 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,401 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,402 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066402
2025-06-05T04:17:46,401 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,402 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066402
2025-06-05T04:17:46,402 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066402
2025-06-05T04:17:46,401 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:46,402 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066402
2025-06-05T04:17:46,402 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:17:46,402 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:17:46,402 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:17:46,402 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:17:46,410 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:46,410 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:46,410 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:46,410 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:46,410 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:46,410 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:46,410 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:46,410 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:46,448 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=44292
2025-06-05T04:17:46,448 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:17:46,452 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:46,452 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]44292
2025-06-05T04:17:46,452 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:46,452 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:46,452 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,452 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change null -> WORKER_STARTED
2025-06-05T04:17:46,452 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:46,452 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:46,453 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:17:46,453 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066453
2025-06-05T04:17:46,453 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097066453
2025-06-05T04:17:46,453 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066453
2025-06-05T04:17:46,453 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097066453
2025-06-05T04:17:46,458 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:17:46,474 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:46,474 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:46,480 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:46,480 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:46,491 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,491 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:46,491 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:46,492 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:46,492 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,492 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,492 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,492 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:46,492 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:46,492 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,492 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066492
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066492
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:17:46,492 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:17:46,501 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:46,501 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:46,501 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:46,501 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:46,533 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:46,533 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:46,543 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:46,544 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,544 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:46,544 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:46,544 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:46,544 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:46,545 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:46,545 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:46,545 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:46,544 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,544 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:46,545 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:46,545 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:46,545 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,545 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:46,545 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066545
2025-06-05T04:17:46,545 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1749097066545
2025-06-05T04:17:46,545 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:17:46,545 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:17:46,553 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:46,553 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:46,553 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:46,553 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:47,206 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,206 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,287 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,287 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,402 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,402 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,402 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,402 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,493 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,493 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,546 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:47,546 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:59,159 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=44470
2025-06-05T04:17:59,159 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:17:59,166 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:59,166 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]44470
2025-06-05T04:17:59,166 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:59,166 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:59,166 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:59,166 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:59,166 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:59,167 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:59,168 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:17:59,169 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097079169
2025-06-05T04:17:59,169 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097079169
2025-06-05T04:17:59,169 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097079169
2025-06-05T04:17:59,169 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097079169
2025-06-05T04:17:59,203 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:17:59,210 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:59,212 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:48,030 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:48,031 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:48,041 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:48,042 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:48,042 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:48,042 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:48,042 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,042 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:48,042 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,043 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,042 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:48,043 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:48,043 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:48,043 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:48,043 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:48,043 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:48,043 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,049 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:48,049 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:48,049 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:48,049 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:48,049 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:48,049 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:48,049 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,049 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,049 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:48,049 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:48,049 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,049 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,049 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,049 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,050 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:17:48,050 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-06-05T04:17:48,061 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:48,061 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:48,061 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:48,061 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:48,127 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=44473
2025-06-05T04:17:48,127 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:17:48,134 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:48,134 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]44473
2025-06-05T04:17:48,134 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:48,134 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,134 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:48,134 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,135 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:48,135 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:48,136 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:17:48,136 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068136
2025-06-05T04:17:48,136 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068136
2025-06-05T04:17:48,136 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068136
2025-06-05T04:17:48,136 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068136
2025-06-05T04:17:48,146 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:17:48,172 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:48,173 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:48,233 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=44476
2025-06-05T04:17:48,233 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:17:48,236 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=44477
2025-06-05T04:17:48,236 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:17:48,237 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:48,237 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]44476
2025-06-05T04:17:48,237 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:48,237 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:48,237 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,237 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,237 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:48,237 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:48,238 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:17:48,238 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068238
2025-06-05T04:17:48,238 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068238
2025-06-05T04:17:48,238 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068238
2025-06-05T04:17:48,238 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068238
2025-06-05T04:17:48,242 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:48,243 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]44477
2025-06-05T04:17:48,243 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:48,243 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:48,243 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,243 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,244 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:48,244 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:48,248 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068248
2025-06-05T04:17:48,248 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:17:48,248 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068248
2025-06-05T04:17:48,249 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:17:48,249 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068249
2025-06-05T04:17:48,249 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068249
2025-06-05T04:17:48,263 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:17:48,265 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:48,266 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:48,277 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:48,277 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:48,278 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:48,278 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:48,287 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=44509
2025-06-05T04:17:48,287 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:48,288 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:48,289 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:48,289 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:48,290 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,290 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,290 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,290 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,290 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:48,290 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]44509
2025-06-05T04:17:48,290 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,290 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:48,291 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,291 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,290 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,291 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:48,291 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:48,291 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:48,291 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:48,291 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:48,291 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,291 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,291 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,291 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,292 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:17:48,292 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2025-06-05T04:17:48,292 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:17:48,292 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068292
2025-06-05T04:17:48,292 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068292
2025-06-05T04:17:48,293 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068293
2025-06-05T04:17:48,293 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068293
2025-06-05T04:17:48,305 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:17:48,305 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:48,305 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:48,305 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:48,305 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:48,324 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:48,324 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:48,359 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:48,359 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:48,359 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:48,359 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:48,370 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:48,370 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:48,370 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:48,371 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,371 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:48,371 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:48,371 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:48,371 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,371 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:48,372 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:48,372 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:48,372 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:48,372 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:48,372 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:48,372 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:48,372 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,372 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,372 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,372 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,372 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,372 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,372 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,372 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:48,372 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:48,372 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,372 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,372 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,372 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,372 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,372 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:48,372 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:48,372 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,372 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,372 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,372 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,373 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:17:48,373 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:17:48,373 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2025-06-05T04:17:48,373 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2025-06-05T04:17:48,376 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:48,376 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:48,382 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:48,382 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:48,382 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:48,382 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:48,383 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=44512
2025-06-05T04:17:48,383 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:17:48,384 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:48,384 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:48,384 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:48,384 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:48,387 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:48,387 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]44512
2025-06-05T04:17:48,387 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:48,387 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:48,387 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,387 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:48,387 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:48,387 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:48,388 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:17:48,388 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068388
2025-06-05T04:17:48,388 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097068388
2025-06-05T04:17:48,388 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068388
2025-06-05T04:17:48,388 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097068388
2025-06-05T04:17:48,391 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:48,391 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:48,392 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:48,392 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,392 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,392 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,392 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,393 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,393 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,393 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:48,393 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:48,393 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,393 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,393 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,393 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,393 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:17:48,393 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2025-06-05T04:17:48,395 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:17:48,402 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:48,402 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:48,402 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:48,402 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:48,411 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:48,411 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:48,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:48,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:48,496 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:48,496 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:48,496 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,496 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:48,496 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:48,497 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:48,496 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,496 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:48,497 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:48,497 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:48,497 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,497 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:48,497 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,497 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:48,497 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:17:48,497 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2025-06-05T04:17:48,504 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:48,504 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:48,504 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:48,504 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:49,052 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,052 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,293 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,293 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,373 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,373 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,373 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,373 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,394 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,394 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,498 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,498 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:49,753 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=44678
2025-06-05T04:17:49,753 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:17:49,760 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:49,760 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]44678
2025-06-05T04:17:49,761 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:49,761 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:49,761 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:49,761 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:49,761 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:49,761 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:49,764 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:17:49,764 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097069764
2025-06-05T04:17:49,764 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097069764
2025-06-05T04:17:49,765 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097069765
2025-06-05T04:17:49,765 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097069765
2025-06-05T04:17:49,802 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:17:49,822 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:49,822 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:49,904 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:49,904 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:49,917 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:49,918 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:49,918 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:49,918 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:49,918 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:49,918 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:49,918 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:49,918 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:49,918 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:49,918 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:49,918 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:49,918 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:49,918 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:49,918 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:49,919 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:49,919 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:49,919 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:49,919 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:49,919 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:49,919 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:49,919 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-06-05T04:17:49,919 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-06-05T04:17:49,928 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:49,928 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:49,928 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:49,928 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:50,059 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=44708
2025-06-05T04:17:50,060 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:17:50,063 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:50,063 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]44708
2025-06-05T04:17:50,063 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:50,063 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:50,063 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,063 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,064 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:50,064 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:50,064 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:17:50,064 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070064
2025-06-05T04:17:50,064 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070064
2025-06-05T04:17:50,064 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070064
2025-06-05T04:17:50,064 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070064
2025-06-05T04:17:50,076 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:17:50,097 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=44711
2025-06-05T04:17:50,097 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:17:50,101 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:50,101 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]44711
2025-06-05T04:17:50,101 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:50,101 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:50,101 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,101 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,101 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:50,101 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:50,102 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070102
2025-06-05T04:17:50,102 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070102
2025-06-05T04:17:50,102 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:50,102 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070102
2025-06-05T04:17:50,102 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:17:50,102 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:50,102 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070102
2025-06-05T04:17:50,117 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:17:50,130 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=44712
2025-06-05T04:17:50,131 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:17:50,132 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:50,132 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:50,134 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:50,134 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]44712
2025-06-05T04:17:50,134 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:50,134 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:50,134 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,134 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,135 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:50,135 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:50,135 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:17:50,135 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070135
2025-06-05T04:17:50,135 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070135
2025-06-05T04:17:50,136 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070136
2025-06-05T04:17:50,136 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070136
2025-06-05T04:17:50,153 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:17:50,157 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=44717
2025-06-05T04:17:50,157 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:17:50,161 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:50,161 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]44717
2025-06-05T04:17:50,161 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:50,161 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:50,161 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,161 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,161 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:50,161 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:50,162 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:17:50,162 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070162
2025-06-05T04:17:50,162 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070162
2025-06-05T04:17:50,162 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070162
2025-06-05T04:17:50,162 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070162
2025-06-05T04:17:50,170 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:50,170 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:50,175 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:17:50,191 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:50,191 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:50,198 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:50,198 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:50,199 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:50,199 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:50,210 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:50,210 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:50,210 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:50,210 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:50,210 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:50,211 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:50,211 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:50,211 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:50,211 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,211 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,211 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:50,211 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:50,211 [INFO ] epollEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,211 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:50,211 [INFO ] epollEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,211 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:50,211 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:50,211 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,211 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:50,211 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,211 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:50,211 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,211 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:50,211 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,211 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:50,211 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:50,211 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,211 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:50,211 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:50,211 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:50,212 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:50,212 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:50,212 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,212 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:50,212 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,212 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,212 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:50,212 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,212 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,212 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,212 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,212 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,212 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2025-06-05T04:17:50,212 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2025-06-05T04:17:50,212 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2025-06-05T04:17:50,221 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:50,221 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:50,221 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:50,221 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:50,221 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:50,221 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:50,221 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:50,221 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:50,222 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:50,222 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:50,233 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:50,234 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:50,234 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:50,234 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:50,234 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:50,234 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:50,234 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:50,234 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:50,234 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,234 [INFO ] epollEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,234 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,234 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,234 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,234 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,235 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:50,235 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:50,235 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,235 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,235 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,235 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,235 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2025-06-05T04:17:50,235 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2025-06-05T04:17:50,240 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:50,240 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:50,242 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:50,242 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:50,242 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:50,242 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:50,251 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:50,252 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:50,251 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,251 [INFO ] epollEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,252 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,252 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,252 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,252 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,252 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:50,252 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:50,252 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,252 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,252 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,252 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,253 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2025-06-05T04:17:50,253 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2025-06-05T04:17:50,259 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:50,259 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:50,259 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:50,259 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:50,349 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=44720
2025-06-05T04:17:50,350 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:17:50,353 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:50,353 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]44720
2025-06-05T04:17:50,353 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:50,353 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:50,353 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,353 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:50,353 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:50,353 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:50,354 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070354
2025-06-05T04:17:50,354 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:17:50,354 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097070354
2025-06-05T04:17:50,354 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070354
2025-06-05T04:17:50,354 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097070354
2025-06-05T04:17:50,359 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:17:50,374 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:50,374 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:50,433 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:50,433 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:50,443 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:50,444 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:50,444 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,444 [INFO ] epollEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:50,444 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,444 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:50,444 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,444 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:50,445 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:50,445 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:50,445 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,445 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:50,445 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,445 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:50,445 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2025-06-05T04:17:50,445 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2025-06-05T04:17:50,453 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:50,453 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:50,453 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:50,453 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:51,920 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:51,920 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,213 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,213 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,213 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,213 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,235 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,235 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,253 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,253 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,446 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,446 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:52,557 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=44896
2025-06-05T04:17:52,558 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:17:52,584 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:52,584 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]44896
2025-06-05T04:17:52,584 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:52,585 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:52,585 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:52,585 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:52,585 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:52,585 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:52,586 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:17:52,586 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097072586
2025-06-05T04:17:52,586 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097072586
2025-06-05T04:17:52,586 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097072586
2025-06-05T04:17:52,586 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097072586
2025-06-05T04:17:52,609 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:17:52,627 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:52,627 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:52,718 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:52,718 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:52,729 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:52,729 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:52,730 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:52,730 [INFO ] epollEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:52,730 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:52,731 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:52,731 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:52,731 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:52,731 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:52,731 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:52,731 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:52,731 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:52,731 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:52,731 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:52,731 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:52,732 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-06-05T04:17:52,732 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-06-05T04:17:52,740 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:52,740 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:52,740 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:52,740 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:04,193 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=44926
2025-06-05T04:18:04,193 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:18:04,199 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:04,200 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]44926
2025-06-05T04:18:04,200 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:04,200 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:04,200 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:04,200 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:04,200 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:04,200 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:04,201 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:18:04,201 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097084201
2025-06-05T04:18:04,201 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097084201
2025-06-05T04:18:04,201 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097084201
2025-06-05T04:18:04,201 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097084201
2025-06-05T04:18:04,208 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=44932
2025-06-05T04:18:04,208 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:18:04,210 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:04,211 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]44932
2025-06-05T04:18:04,211 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:04,211 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:04,211 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:04,211 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:04,211 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:04,211 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:04,212 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:18:04,212 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097084212
2025-06-05T04:18:04,212 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097084212
2025-06-05T04:18:04,213 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097084213
2025-06-05T04:18:04,213 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097084213
2025-06-05T04:18:04,219 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:18:04,227 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=44927
2025-06-05T04:18:04,228 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:18:04,228 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:04,228 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=44935
2025-06-05T04:18:04,228 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]44927
2025-06-05T04:18:04,228 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:04,228 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:04,228 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:18:04,228 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:04,228 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:04,228 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:18:04,228 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:04,228 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:04,229 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:18:04,229 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097084229
2025-06-05T04:18:04,229 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097084229
2025-06-05T04:18:04,229 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097084229
2025-06-05T04:18:04,229 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097084229
2025-06-05T04:18:04,232 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:04,232 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]44935
2025-06-05T04:18:04,232 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:04,232 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:04,232 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:04,232 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:04,232 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:04,232 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:04,235 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097084235
2025-06-05T04:18:04,235 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097084235
2025-06-05T04:18:04,235 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:18:04,235 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097084235
2025-06-05T04:18:04,235 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097084235
2025-06-05T04:18:04,241 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:04,241 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:04,249 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:18:04,249 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:04,249 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:04,256 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:17:52,969 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:52,975 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:52,975 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:52,975 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:53,027 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:53,027 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:53,027 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:53,028 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:53,028 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:53,028 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:53,029 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:53,029 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:53,039 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:53,039 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:53,040 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:53,040 [INFO ] epollEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,040 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:53,040 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:53,040 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:53,040 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:53,040 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:53,040 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,040 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:53,041 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:53,040 [INFO ] epollEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,041 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:53,041 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:53,041 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,041 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:53,041 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,041 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:53,041 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:53,040 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,041 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,040 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,041 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:53,041 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:53,041 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:53,041 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:53,041 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:53,041 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:53,041 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:53,041 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,041 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,041 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:53,041 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:53,041 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:53,041 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,041 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,041 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,041 [INFO ] epollEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,041 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:53,041 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,041 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,041 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:53,041 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:53,042 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:53,042 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:53,041 [INFO ] epollEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,041 [INFO ] epollEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,042 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:53,042 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,042 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2025-06-05T04:17:53,042 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,042 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,042 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,042 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:53,042 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2025-06-05T04:17:53,041 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,042 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,042 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,042 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,042 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,042 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:53,042 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:53,042 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,042 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:53,042 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:53,042 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:53,042 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2025-06-05T04:17:53,042 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:53,042 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,042 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2025-06-05T04:17:53,042 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,042 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:53,042 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,042 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,043 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:53,043 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:53,043 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,042 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,043 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,043 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:53,043 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,043 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:53,043 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:53,043 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2025-06-05T04:17:53,043 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2025-06-05T04:17:53,043 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:53,043 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:53,043 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:53,043 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:53,044 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:53,044 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:53,044 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:53,052 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:53,052 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:53,052 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:53,052 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:53,052 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:53,052 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:53,053 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:53,053 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:53,053 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:53,053 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:53,054 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:53,054 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:53,171 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=44938
2025-06-05T04:17:53,171 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:17:53,175 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:53,175 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]44938
2025-06-05T04:17:53,175 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:53,175 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:53,175 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:53,175 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:53,175 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:53,175 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:53,176 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097073176
2025-06-05T04:17:53,176 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:17:53,176 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097073176
2025-06-05T04:17:53,176 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097073176
2025-06-05T04:17:53,176 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097073176
2025-06-05T04:17:53,186 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:17:53,202 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:53,202 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:53,275 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:53,275 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:53,286 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:53,286 [INFO ] epollEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,286 [INFO ] epollEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:53,287 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,287 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:53,287 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,287 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:53,287 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:53,287 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:53,287 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,287 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:53,287 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,287 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:53,287 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2025-06-05T04:17:53,287 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2025-06-05T04:17:53,295 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:53,295 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:53,295 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:53,295 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:55,707 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:55,707 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,042 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,042 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,043 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,043 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,043 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,043 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,043 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,043 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,288 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,288 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:17:56,326 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=45111
2025-06-05T04:17:56,326 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:17:56,331 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:56,332 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]45111
2025-06-05T04:17:56,332 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,332 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:56,332 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:56,332 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,332 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:56,332 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:17:56,333 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:17:56,333 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076333
2025-06-05T04:17:56,333 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076333
2025-06-05T04:17:56,333 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076333
2025-06-05T04:17:56,333 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076333
2025-06-05T04:17:56,395 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:17:56,422 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:56,423 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:56,516 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:56,516 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:56,530 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:56,531 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:56,531 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:56,531 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:56,531 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:56,531 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:56,531 [INFO ] epollEventLoopGroup-5-25 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,531 [INFO ] epollEventLoopGroup-5-25 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,532 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,532 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,532 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,532 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,533 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:56,533 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:17:56,533 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,533 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,533 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,533 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,534 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-06-05T04:17:56,534 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-06-05T04:17:56,542 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:56,542 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:56,542 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:17:56,542 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:17:56,775 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=45144
2025-06-05T04:17:56,775 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:17:56,778 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:56,778 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]45144
2025-06-05T04:17:56,778 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:56,779 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:56,779 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,779 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,779 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:56,779 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:17:56,780 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:17:56,780 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076780
2025-06-05T04:17:56,780 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076780
2025-06-05T04:17:56,780 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076780
2025-06-05T04:17:56,780 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076780
2025-06-05T04:17:56,782 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=45142
2025-06-05T04:17:56,782 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:17:56,786 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:56,786 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:17:56,786 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]45142
2025-06-05T04:17:56,786 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:56,786 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:56,787 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,787 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,787 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:56,787 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:17:56,788 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:17:56,788 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076788
2025-06-05T04:17:56,788 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076788
2025-06-05T04:17:56,788 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076788
2025-06-05T04:17:56,788 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076788
2025-06-05T04:17:56,790 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=45141
2025-06-05T04:17:56,790 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:17:56,794 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:56,794 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]45141
2025-06-05T04:17:56,794 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:56,794 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:56,795 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,795 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,795 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:56,795 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:17:56,797 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076797
2025-06-05T04:17:56,797 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:17:56,797 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076797
2025-06-05T04:17:56,797 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076797
2025-06-05T04:17:56,797 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076797
2025-06-05T04:17:56,798 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:17:56,808 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:17:56,808 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:56,808 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:56,813 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=45145
2025-06-05T04:17:56,813 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:17:56,815 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:56,816 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:56,816 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:56,817 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]45145
2025-06-05T04:17:56,817 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:56,817 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:56,818 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,818 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:56,818 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:56,818 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:17:56,819 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:17:56,819 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076819
2025-06-05T04:17:56,819 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097076819
2025-06-05T04:17:56,819 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076819
2025-06-05T04:17:56,819 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097076819
2025-06-05T04:17:56,827 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:56,827 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:56,830 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:17:56,846 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:56,846 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:56,891 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:56,891 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:56,891 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:56,891 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:56,891 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:56,891 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:56,899 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:56,899 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:56,903 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:56,904 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:56,904 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:56,904 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:56,904 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:56,904 [INFO ] epollEventLoopGroup-5-28 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:56,904 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:56,904 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:56,904 [INFO ] epollEventLoopGroup-5-28 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:56,905 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:56,905 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:56,905 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:56,905 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:56,905 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:56,905 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:56,905 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:56,905 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,905 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,905 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,905 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,905 [INFO ] epollEventLoopGroup-5-27 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] epollEventLoopGroup-5-26 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:56,905 [INFO ] epollEventLoopGroup-5-27 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] epollEventLoopGroup-5-26 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2025-06-05T04:17:56,906 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,906 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:56,906 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,905 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2025-06-05T04:17:56,906 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,906 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:56,906 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,906 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,906 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,906 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:56,906 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,906 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:17:56,906 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:56,906 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:17:56,906 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,906 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,906 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,906 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,906 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,906 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,906 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,906 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,906 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2025-06-05T04:17:56,906 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2025-06-05T04:17:56,906 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2025-06-05T04:17:56,906 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2025-06-05T04:17:56,912 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:56,913 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:56,914 [INFO ] epollEventLoopGroup-5-29 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,914 [INFO ] epollEventLoopGroup-5-29 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:56,915 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,915 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:56,915 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,915 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:56,916 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:56,916 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:17:56,916 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,916 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:56,916 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,916 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:56,916 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2025-06-05T04:17:56,916 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2025-06-05T04:17:56,917 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:56,917 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:17:56,917 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:56,917 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:17:56,918 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:56,918 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:17:56,918 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:56,918 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:17:56,919 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:56,919 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:56,919 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:17:56,919 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:17:56,925 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:56,925 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:56,925 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:17:56,925 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:17:57,023 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=45153
2025-06-05T04:17:57,024 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:17:57,027 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:17:57,027 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]45153
2025-06-05T04:17:57,027 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:17:57,027 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:17:57,027 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:57,027 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:17:57,027 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:57,027 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:17:57,028 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:17:57,028 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097077028
2025-06-05T04:17:57,028 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097077028
2025-06-05T04:17:57,028 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097077028
2025-06-05T04:17:57,028 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097077028
2025-06-05T04:17:57,033 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:17:57,049 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:17:57,049 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:17:57,114 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:17:57,114 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:17:57,125 [INFO ] epollEventLoopGroup-5-30 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:17:57,125 [INFO ] epollEventLoopGroup-5-30 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:17:57,125 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:17:57,125 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:57,125 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:17:57,126 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:57,126 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:17:57,126 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:57,126 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:17:57,126 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:57,126 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:17:57,126 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:57,126 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:17:57,126 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2025-06-05T04:17:57,126 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2025-06-05T04:17:57,135 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:57,135 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:17:57,135 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:17:57,135 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:18:01,547 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,547 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,920 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,920 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,920 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,920 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,921 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,921 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,928 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:01,928 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:02,138 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:02,138 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:02,189 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=45348
2025-06-05T04:18:02,189 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:18:02,196 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:02,196 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]45348
2025-06-05T04:18:02,196 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:02,196 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:02,196 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,196 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,196 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:18:02,196 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:18:02,198 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:18:02,198 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082198
2025-06-05T04:18:02,198 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082198
2025-06-05T04:18:02,198 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082198
2025-06-05T04:18:02,198 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082198
2025-06-05T04:18:02,236 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:18:02,283 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:02,283 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:02,405 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:02,405 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:02,421 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:02,421 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:02,421 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:02,422 [INFO ] epollEventLoopGroup-5-31 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:02,422 [INFO ] epollEventLoopGroup-5-31 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:02,422 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:02,423 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:02,423 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:02,423 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:02,423 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:02,423 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:02,423 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:02,423 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:02,423 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:02,423 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:18:02,423 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:18:02,423 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:02,423 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:02,423 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:02,423 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:02,424 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-06-05T04:18:02,424 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-06-05T04:18:02,436 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:02,436 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:18:02,436 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:02,436 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:18:02,684 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=45392
2025-06-05T04:18:02,684 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:18:02,688 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:02,688 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]45392
2025-06-05T04:18:02,688 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:02,688 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:02,688 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,688 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,688 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:02,688 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:02,689 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:18:02,690 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082690
2025-06-05T04:18:02,690 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082690
2025-06-05T04:18:02,690 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082690
2025-06-05T04:18:02,690 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082690
2025-06-05T04:18:02,695 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:18:02,701 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=45393
2025-06-05T04:18:02,701 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:18:02,706 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:02,706 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]45393
2025-06-05T04:18:02,706 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:02,706 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:02,706 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,706 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,706 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:02,706 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:02,707 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:18:02,707 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082707
2025-06-05T04:18:02,707 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082707
2025-06-05T04:18:02,707 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082707
2025-06-05T04:18:02,707 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082707
2025-06-05T04:18:02,718 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=45394
2025-06-05T04:18:02,718 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:18:02,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:02,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]45394
2025-06-05T04:18:02,725 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:02,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:02,725 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:02,725 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,725 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:02,725 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,725 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:02,725 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:02,727 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:18:02,728 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082728
2025-06-05T04:18:02,727 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:18:02,728 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082728
2025-06-05T04:18:02,728 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082728
2025-06-05T04:18:02,728 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082728
2025-06-05T04:18:02,740 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=45401
2025-06-05T04:18:02,741 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:18:02,743 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:18:02,746 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:02,746 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]45401
2025-06-05T04:18:02,747 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:02,747 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:02,747 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,747 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:02,747 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:02,747 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:02,749 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:18:02,749 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082749
2025-06-05T04:18:02,749 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097082749
2025-06-05T04:18:02,749 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082749
2025-06-05T04:18:02,749 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097082749
2025-06-05T04:18:02,755 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:02,755 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:14,052 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:14,053 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:14,054 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:18:14,074 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:14,075 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:14,125 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:14,125 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:14,125 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:14,125 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:14,126 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:14,126 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:14,131 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:14,131 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:14,138 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:14,138 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:14,138 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:14,138 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:14,138 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:14,139 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:14,139 [INFO ] epollEventLoopGroup-5-33 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:14,139 [INFO ] epollEventLoopGroup-5-34 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:14,139 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:14,139 [INFO ] epollEventLoopGroup-5-33 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:14,139 [INFO ] epollEventLoopGroup-5-34 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:14,139 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:14,139 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:14,139 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:14,139 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:14,139 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:14,139 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:14,139 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:14,139 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:14,139 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:14,139 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:14,140 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:14,140 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:18:14,140 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:18:14,140 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:14,140 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:18:14,140 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:14,140 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:14,140 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:14,140 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:14,140 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:18:14,140 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:14,140 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:14,140 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:14,140 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:14,140 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:14,140 [INFO ] epollEventLoopGroup-5-32 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:14,140 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2025-06-05T04:18:14,140 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2025-06-05T04:18:14,140 [INFO ] epollEventLoopGroup-5-32 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:14,140 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2025-06-05T04:18:14,140 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:14,140 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2025-06-05T04:18:14,140 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:14,140 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:14,141 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:14,141 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:14,141 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:14,141 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:14,141 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:14,141 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:14,141 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:14,141 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:18:14,141 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:18:14,141 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:14,141 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:14,141 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:14,141 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:14,141 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2025-06-05T04:18:14,141 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:14,145 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:14,146 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:14,146 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:14,146 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:14,146 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:14,146 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:14,146 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:14,146 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:14,146 [INFO ] epollEventLoopGroup-5-35 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:14,146 [INFO ] epollEventLoopGroup-5-35 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:14,146 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:14,146 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:14,146 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:14,146 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:14,147 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:18:14,147 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:18:14,147 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:14,147 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:14,147 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:14,147 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:14,148 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2025-06-05T04:18:14,148 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2025-06-05T04:18:14,150 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:18:14,150 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:18:14,150 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:18:14,150 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:18:14,151 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:18:14,151 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:18:14,151 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:18:14,151 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:18:14,152 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:18:14,152 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:18:14,152 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:18:14,152 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:18:14,156 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:18:14,156 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:18:14,156 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:18:14,156 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:18:14,247 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=45404
2025-06-05T04:18:14,247 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:18:14,251 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:14,251 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]45404
2025-06-05T04:18:14,251 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:14,251 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:14,251 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:14,251 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:14,251 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:18:14,251 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:18:14,252 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:18:14,252 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097094252
2025-06-05T04:18:14,252 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097094252
2025-06-05T04:18:14,252 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097094252
2025-06-05T04:18:14,252 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097094252
2025-06-05T04:18:14,268 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:18:02,988 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:02,988 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:03,079 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:03,079 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:03,090 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:03,090 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:03,091 [INFO ] epollEventLoopGroup-5-36 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:03,091 [INFO ] epollEventLoopGroup-5-36 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:03,091 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:03,091 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:03,091 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:03,091 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:03,091 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:03,091 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:18:03,091 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:18:03,091 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:03,091 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:03,091 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:03,091 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:03,092 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2025-06-05T04:18:03,092 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2025-06-05T04:18:03,099 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:18:03,099 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:18:03,099 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:18:03,099 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:18:10,426 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,426 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,854 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,854 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,854 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,854 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,855 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,855 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,862 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:10,862 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:11,022 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=45606
2025-06-05T04:18:11,022 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:18:11,026 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:11,026 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]45606
2025-06-05T04:18:11,026 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:11,026 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:11,026 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,026 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,026 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:18:11,026 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:18:11,027 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:18:11,027 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091027
2025-06-05T04:18:11,027 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091027
2025-06-05T04:18:11,027 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091027
2025-06-05T04:18:11,027 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091027
2025-06-05T04:18:11,048 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:18:11,074 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:11,074 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:11,102 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:11,102 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:11,186 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:11,186 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:11,250 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:11,250 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:11,251 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:11,251 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:11,251 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:11,251 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:11,251 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:11,251 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:11,251 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:11,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:11,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:11,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:11,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:11,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:11,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:11,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:11,252 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:11,253 [INFO ] epollEventLoopGroup-5-37 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,253 [INFO ] epollEventLoopGroup-5-37 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,253 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,254 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:18:11,254 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:18:11,254 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,254 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,254 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,254 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,254 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-06-05T04:18:11,254 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-06-05T04:18:11,266 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:18:11,266 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:11,266 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:18:11,266 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:11,572 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=45637
2025-06-05T04:18:11,572 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:18:11,579 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:11,579 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]45637
2025-06-05T04:18:11,579 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:11,579 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,579 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:11,579 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,580 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:11,580 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:11,581 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:18:11,581 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091581
2025-06-05T04:18:11,581 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091581
2025-06-05T04:18:11,581 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091581
2025-06-05T04:18:11,581 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091581
2025-06-05T04:18:11,598 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=45638
2025-06-05T04:18:11,599 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:18:11,604 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:11,604 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]45638
2025-06-05T04:18:11,604 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:11,604 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:11,605 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,605 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,605 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:11,605 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:11,605 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:18:11,606 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091606
2025-06-05T04:18:11,606 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091606
2025-06-05T04:18:11,606 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:18:11,606 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091606
2025-06-05T04:18:11,606 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091606
2025-06-05T04:18:11,625 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=45645
2025-06-05T04:18:11,625 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:18:11,625 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:11,625 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]45645
2025-06-05T04:18:11,625 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:11,625 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:11,626 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,626 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,626 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:18:11,626 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:11,626 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:11,627 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:18:11,627 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091627
2025-06-05T04:18:11,627 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091627
2025-06-05T04:18:11,627 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091627
2025-06-05T04:18:11,627 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091627
2025-06-05T04:18:11,634 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:11,635 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:11,641 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=45636
2025-06-05T04:18:11,641 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:18:11,649 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:11,650 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]45636
2025-06-05T04:18:11,650 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:11,650 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:11,650 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,650 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:11,650 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:11,650 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:18:11,650 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:11,651 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:11,651 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:11,652 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:18:11,652 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091652
2025-06-05T04:18:11,652 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097091652
2025-06-05T04:18:11,652 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091652
2025-06-05T04:18:11,652 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097091652
2025-06-05T04:18:11,674 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:11,675 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:11,675 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:18:11,695 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:11,695 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:11,733 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:11,733 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:11,734 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:11,734 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:11,733 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:11,734 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:11,748 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:11,748 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:11,748 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:11,748 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:11,748 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:11,749 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:11,748 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:11,749 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:11,749 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:11,749 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:11,749 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:11,749 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:11,749 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:11,749 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:11,749 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:11,749 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:11,749 [INFO ] epollEventLoopGroup-5-39 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,749 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:11,749 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:11,749 [INFO ] epollEventLoopGroup-5-39 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,749 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:11,749 [INFO ] epollEventLoopGroup-5-40 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,749 [INFO ] epollEventLoopGroup-5-38 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,749 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,749 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,749 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:11,749 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:11,749 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:11,749 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:11,749 [INFO ] epollEventLoopGroup-5-38 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,749 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:11,749 [INFO ] epollEventLoopGroup-5-40 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,750 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,750 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,749 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:11,750 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:11,750 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:11,750 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,750 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,749 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,750 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,750 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,750 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:11,749 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,750 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,750 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,750 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:11,750 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:18:11,750 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:18:11,750 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:18:11,750 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:11,750 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:18:11,750 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:11,750 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:18:11,750 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:18:11,750 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,750 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,750 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,750 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,750 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:11,750 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,750 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,750 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,750 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,750 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:11,750 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:11,750 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,750 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:11,750 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,750 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,750 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:11,750 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,750 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:11,750 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:11,750 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2025-06-05T04:18:11,750 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2025-06-05T04:18:11,751 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2025-06-05T04:18:11,751 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2025-06-05T04:18:11,750 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:11,751 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:11,751 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:11,751 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2025-06-05T04:18:11,751 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:18:11,751 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2025-06-05T04:18:11,751 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:18:11,759 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:18:11,759 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:18:11,759 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:18:11,759 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:18:11,760 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:18:11,760 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:18:11,761 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:18:11,761 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:18:11,761 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:18:11,761 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:18:11,763 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:11,763 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:11,777 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:11,778 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:11,778 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:11,778 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:11,778 [INFO ] epollEventLoopGroup-5-41 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,778 [INFO ] epollEventLoopGroup-5-41 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:11,778 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,778 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:11,778 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,778 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:11,779 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:18:11,779 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:18:11,779 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,779 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:11,779 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,779 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:11,779 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2025-06-05T04:18:11,779 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2025-06-05T04:18:11,787 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:18:11,787 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:18:11,787 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:18:11,787 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:18:12,003 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=45651
2025-06-05T04:18:12,003 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:18:12,007 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:12,008 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]45651
2025-06-05T04:18:12,008 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:12,008 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:12,008 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:12,008 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:12,008 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:18:12,008 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:18:12,009 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:18:12,009 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097092009
2025-06-05T04:18:12,009 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097092009
2025-06-05T04:18:12,009 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097092009
2025-06-05T04:18:12,009 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097092009
2025-06-05T04:18:12,010 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:18:12,029 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:12,029 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:12,095 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:12,095 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:12,111 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:12,111 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:12,112 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:12,112 [INFO ] epollEventLoopGroup-5-42 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:12,112 [INFO ] epollEventLoopGroup-5-42 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:12,113 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:12,113 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:12,113 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:12,113 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:12,113 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:18:12,113 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:18:12,113 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:12,113 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:12,113 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:12,113 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:12,114 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2025-06-05T04:18:12,114 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2025-06-05T04:18:12,122 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:18:12,122 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:18:12,122 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:18:12,122 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:18:24,262 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,262 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,758 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,758 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,758 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,758 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,758 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,758 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,786 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,786 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:24,887 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=45878
2025-06-05T04:18:24,887 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:18:24,891 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:24,891 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]45878
2025-06-05T04:18:24,891 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:24,891 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:24,891 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:24,891 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:24,891 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:18:24,891 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:18:24,892 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:18:24,892 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097104892
2025-06-05T04:18:24,892 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097104892
2025-06-05T04:18:24,892 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097104892
2025-06-05T04:18:24,892 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097104892
2025-06-05T04:18:24,893 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:18:24,913 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:24,914 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:25,015 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:25,015 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:25,028 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:25,028 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:25,029 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:25,029 [INFO ] epollEventLoopGroup-5-43 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,029 [INFO ] epollEventLoopGroup-5-43 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,029 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,029 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,030 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,030 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,030 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:18:25,030 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:18:25,030 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,030 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,030 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,030 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,030 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-06-05T04:18:25,030 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-06-05T04:18:25,039 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:18:25,039 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:25,039 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:18:25,039 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:25,121 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:25,121 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:25,436 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=45908
2025-06-05T04:18:25,436 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:18:25,440 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:25,440 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]45908
2025-06-05T04:18:25,440 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:25,441 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:25,441 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,441 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,441 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:25,441 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:25,441 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:18:25,441 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105441
2025-06-05T04:18:25,441 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105441
2025-06-05T04:18:25,442 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105442
2025-06-05T04:18:25,442 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105442
2025-06-05T04:18:25,442 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:18:25,459 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:25,459 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:25,497 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=45909
2025-06-05T04:18:25,498 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:18:25,506 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:25,506 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]45909
2025-06-05T04:18:25,506 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:25,507 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,507 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,507 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:25,507 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:25,507 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:25,509 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:18:25,510 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105510
2025-06-05T04:18:25,510 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105510
2025-06-05T04:18:25,510 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105510
2025-06-05T04:18:25,510 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105510
2025-06-05T04:18:25,510 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:18:25,532 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:25,532 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:25,551 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=45910
2025-06-05T04:18:25,552 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:18:25,556 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:25,556 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]45910
2025-06-05T04:18:25,556 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:25,556 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,556 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,556 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:25,556 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:25,556 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:25,557 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105557
2025-06-05T04:18:25,557 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105557
2025-06-05T04:18:25,557 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105557
2025-06-05T04:18:25,557 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105557
2025-06-05T04:18:25,557 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:18:25,558 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:18:25,560 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:25,560 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:25,572 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:25,572 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:25,573 [INFO ] epollEventLoopGroup-5-44 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:25,573 [INFO ] epollEventLoopGroup-5-44 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:25,573 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:25,573 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:25,573 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:25,573 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,574 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:25,573 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,574 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:25,574 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:25,574 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:18:25,574 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:25,574 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:18:25,574 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:25,574 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:25,574 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,574 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,574 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,574 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,574 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2025-06-05T04:18:25,574 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2025-06-05T04:18:25,582 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:25,582 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:25,583 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:18:25,583 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:18:25,583 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:18:25,583 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:18:25,592 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:25,592 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:25,603 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:25,603 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:25,604 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:25,604 [INFO ] epollEventLoopGroup-5-45 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,604 [INFO ] epollEventLoopGroup-5-45 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,604 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,604 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,604 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,604 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,605 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:18:25,605 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:18:25,605 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,605 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,605 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,605 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,605 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2025-06-05T04:18:25,605 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2025-06-05T04:18:25,607 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=45917
2025-06-05T04:18:25,607 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:18:25,611 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:25,611 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]45917
2025-06-05T04:18:25,611 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:25,611 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,611 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:25,611 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,611 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:25,611 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:25,612 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:18:25,612 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:18:25,612 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:18:25,612 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:18:25,612 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:18:25,612 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105612
2025-06-05T04:18:25,612 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105612
2025-06-05T04:18:25,613 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105613
2025-06-05T04:18:25,613 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105613
2025-06-05T04:18:25,613 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:18:25,628 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:25,628 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:25,663 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:25,663 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:25,673 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:25,673 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:25,673 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:25,673 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:25,674 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:25,674 [INFO ] epollEventLoopGroup-5-46 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,674 [INFO ] epollEventLoopGroup-5-46 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,674 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,674 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,674 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,674 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,674 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:18:25,674 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:18:25,675 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,675 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,675 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,675 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,675 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2025-06-05T04:18:25,675 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2025-06-05T04:18:25,682 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:18:25,682 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:18:25,682 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:18:25,682 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:18:25,684 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:25,684 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:25,685 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:25,685 [INFO ] epollEventLoopGroup-5-47 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,685 [INFO ] epollEventLoopGroup-5-47 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,685 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,685 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,685 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,685 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,685 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:18:25,685 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:18:25,685 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,685 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,685 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,685 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,686 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2025-06-05T04:18:25,686 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2025-06-05T04:18:25,693 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:18:25,693 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:18:25,693 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:18:25,693 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:18:25,820 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=46005
2025-06-05T04:18:25,820 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:18:25,824 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:25,824 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]46005
2025-06-05T04:18:25,824 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:25,824 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:25,824 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,824 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:25,824 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:18:25,824 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:18:25,825 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:18:25,825 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105825
2025-06-05T04:18:25,825 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097105825
2025-06-05T04:18:25,825 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105825
2025-06-05T04:18:25,825 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097105825
2025-06-05T04:18:25,825 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:18:25,840 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:25,841 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:25,900 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:25,900 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:25,911 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:25,911 [INFO ] epollEventLoopGroup-5-48 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,911 [INFO ] epollEventLoopGroup-5-48 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:25,912 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,912 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:25,912 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,912 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:25,912 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:18:25,912 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:18:25,912 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,912 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:25,912 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,912 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:25,912 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2025-06-05T04:18:25,912 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2025-06-05T04:18:25,920 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:18:25,920 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:18:25,920 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:18:25,920 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:18:45,863 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-06-05T04:18:45,863 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-06-05T04:18:46,022 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,022 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,565 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,565 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,590 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=46172
2025-06-05T04:18:46,590 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:18:46,594 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:46,594 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]46172
2025-06-05T04:18:46,594 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:46,594 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:46,594 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:46,594 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:46,594 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:18:46,594 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:18:46,595 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:18:46,595 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097126595
2025-06-05T04:18:46,595 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097126595
2025-06-05T04:18:46,595 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097126595
2025-06-05T04:18:46,595 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097126595
2025-06-05T04:18:46,596 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:18:46,596 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,596 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,612 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:46,612 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:46,666 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,666 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,677 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,677 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,681 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:46,681 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:46,695 [INFO ] epollEventLoopGroup-5-49 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:46,695 [INFO ] epollEventLoopGroup-5-49 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:46,695 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:46,696 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:46,696 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:46,696 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:46,696 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:46,696 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:46,696 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:46,696 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:46,696 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:46,696 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:18:46,696 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:18:46,696 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:46,696 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:46,696 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:46,696 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:46,697 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-06-05T04:18:46,697 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-06-05T04:18:46,705 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:18:46,705 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:46,705 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:18:46,705 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:18:46,903 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:46,903 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:18:47,396 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=46208
2025-06-05T04:18:47,396 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:18:47,399 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:47,399 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]46208
2025-06-05T04:18:47,400 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:47,400 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:47,400 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,400 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,400 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:47,400 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:18:47,401 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127401
2025-06-05T04:18:47,401 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127401
2025-06-05T04:18:47,401 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:18:47,401 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127401
2025-06-05T04:18:47,401 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127401
2025-06-05T04:18:47,401 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:18:47,418 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=46212
2025-06-05T04:18:47,418 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:18:47,422 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:47,422 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]46212
2025-06-05T04:18:47,422 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:47,422 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:47,422 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,422 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,422 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:47,422 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:18:47,423 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:18:47,423 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127423
2025-06-05T04:18:47,423 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127423
2025-06-05T04:18:47,423 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127423
2025-06-05T04:18:47,423 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127423
2025-06-05T04:18:47,424 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:18:47,429 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:47,429 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:47,441 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:47,441 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:47,444 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=46217
2025-06-05T04:18:47,444 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:18:47,448 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:47,448 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]46217
2025-06-05T04:18:47,448 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:47,449 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:47,449 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,449 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,449 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:47,449 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:18:47,451 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:18:47,452 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127452
2025-06-05T04:18:47,452 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127452
2025-06-05T04:18:47,452 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127452
2025-06-05T04:18:47,452 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127452
2025-06-05T04:18:47,452 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=46220
2025-06-05T04:18:47,452 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:18:47,452 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:18:47,456 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:47,456 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]46220
2025-06-05T04:18:47,456 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:47,456 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,456 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,456 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:47,456 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:47,456 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:18:47,457 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:18:47,457 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127457
2025-06-05T04:18:47,457 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127457
2025-06-05T04:18:47,457 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127457
2025-06-05T04:18:47,457 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127457
2025-06-05T04:18:47,458 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:18:47,470 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:47,470 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:47,475 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:47,475 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:47,523 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:47,523 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:47,523 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:47,523 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:47,525 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:47,525 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:47,527 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:47,527 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:47,536 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:47,536 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:47,536 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:47,536 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:47,536 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:47,536 [INFO ] epollEventLoopGroup-5-51 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,536 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:47,536 [INFO ] epollEventLoopGroup-5-50 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:47,536 [INFO ] epollEventLoopGroup-5-50 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:47,536 [INFO ] epollEventLoopGroup-5-51 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,536 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:47,536 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:47,537 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:47,537 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:47,537 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,537 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,537 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:47,537 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:18:47,537 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:18:47,537 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:47,537 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,537 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,537 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:47,537 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:47,537 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:47,537 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,537 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:47,537 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:18:47,537 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,537 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,537 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,537 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2025-06-05T04:18:47,537 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:47,537 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:47,538 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:18:47,538 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:18:47,538 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:47,538 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:47,539 [INFO ] epollEventLoopGroup-5-52 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:47,539 [INFO ] epollEventLoopGroup-5-52 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,539 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:47,539 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,539 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:47,539 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,539 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,540 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:18:47,540 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:18:47,540 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,540 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,540 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,540 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,540 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2025-06-05T04:18:47,540 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:47,541 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:47,541 [INFO ] epollEventLoopGroup-5-53 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,541 [INFO ] epollEventLoopGroup-5-53 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,542 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,542 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,542 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,542 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,542 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:18:47,542 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:18:47,542 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,542 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,542 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,542 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,542 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2025-06-05T04:18:47,542 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2025-06-05T04:18:47,548 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:18:47,548 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:18:47,549 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:18:47,549 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:18:47,549 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:18:47,549 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:18:47,549 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:18:47,549 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:18:47,549 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:18:47,549 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:18:47,550 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:18:47,550 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:18:47,550 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:18:47,550 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:18:47,681 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=46286
2025-06-05T04:18:47,681 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:18:47,685 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:18:47,685 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]46286
2025-06-05T04:18:47,685 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:18:47,685 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:18:47,685 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,685 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:18:47,685 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:18:47,685 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:18:47,686 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:18:47,686 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127686
2025-06-05T04:18:47,686 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097127686
2025-06-05T04:18:47,686 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127686
2025-06-05T04:18:47,686 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097127686
2025-06-05T04:18:47,686 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:18:47,702 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:18:47,702 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:18:47,760 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:18:47,760 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:18:47,771 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:18:47,771 [INFO ] epollEventLoopGroup-5-54 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,771 [INFO ] epollEventLoopGroup-5-54 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:18:47,772 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,772 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:18:47,772 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,772 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:18:47,772 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:18:47,772 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:18:47,772 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,772 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:18:47,772 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,772 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:18:47,772 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2025-06-05T04:18:47,772 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2025-06-05T04:18:47,780 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:18:47,780 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:18:47,780 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:18:47,780 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:19:20,690 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:20,690 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,268 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=46513
2025-06-05T04:19:21,268 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:19:21,272 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:19:21,272 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]46513
2025-06-05T04:19:21,272 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:19:21,272 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:19:21,272 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:21,272 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:21,272 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:19:21,272 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:19:21,273 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:19:21,273 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097161273
2025-06-05T04:19:21,273 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097161273
2025-06-05T04:19:21,273 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097161273
2025-06-05T04:19:21,273 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097161273
2025-06-05T04:19:21,274 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:19:21,289 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:19:21,290 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:19:21,359 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:19:21,359 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:19:21,371 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:19:21,372 [INFO ] epollEventLoopGroup-5-55 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:21,372 [INFO ] epollEventLoopGroup-5-55 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:21,372 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:21,372 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:21,372 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:21,372 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:21,372 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:19:21,372 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:19:21,372 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:21,372 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:21,372 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:21,372 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:21,373 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-06-05T04:19:21,373 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-06-05T04:19:21,379 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:19:21,379 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:19:21,379 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:19:21,379 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:19:21,531 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,531 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,531 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,531 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,533 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,533 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,536 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,536 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,766 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:21,766 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:19:22,248 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=46555
2025-06-05T04:19:22,248 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:19:22,251 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:19:22,251 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]46555
2025-06-05T04:19:22,252 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:19:22,252 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:19:22,252 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,252 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,252 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:19:22,252 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:19:22,252 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:19:22,253 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162253
2025-06-05T04:19:22,253 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162253
2025-06-05T04:19:22,253 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162253
2025-06-05T04:19:22,253 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162253
2025-06-05T04:19:22,253 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:19:22,262 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=46549
2025-06-05T04:19:22,263 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:19:22,267 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:19:22,267 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]46549
2025-06-05T04:19:22,267 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:19:22,267 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:19:22,267 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,267 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,267 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:19:22,267 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:19:22,268 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:19:22,268 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162268
2025-06-05T04:19:22,268 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162268
2025-06-05T04:19:22,268 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162268
2025-06-05T04:19:22,268 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162268
2025-06-05T04:19:22,269 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:19:22,269 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=46558
2025-06-05T04:19:22,269 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:19:22,270 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:19:22,270 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:19:22,272 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=46550
2025-06-05T04:19:22,272 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:19:22,273 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:19:22,273 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]46558
2025-06-05T04:19:22,274 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:19:22,274 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,274 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:19:22,274 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,274 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:19:22,274 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:19:22,274 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:19:22,275 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162275
2025-06-05T04:19:22,275 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162275
2025-06-05T04:19:22,275 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162275
2025-06-05T04:19:22,275 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162275
2025-06-05T04:19:22,275 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:19:22,277 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:19:22,277 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]46550
2025-06-05T04:19:22,277 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:19:22,277 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:19:22,277 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,277 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,277 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:19:22,277 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:19:22,278 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:19:22,278 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162278
2025-06-05T04:19:22,278 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162278
2025-06-05T04:19:22,278 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162278
2025-06-05T04:19:22,278 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162278
2025-06-05T04:19:22,278 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:19:22,288 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:19:22,288 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:19:22,293 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:19:22,293 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:19:22,296 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:19:22,296 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:19:22,363 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:19:22,363 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:19:22,363 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:19:22,363 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:19:22,363 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:19:22,363 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:19:22,363 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:19:22,363 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:19:22,378 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:19:22,378 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:19:22,378 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:19:22,378 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:19:22,378 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:19:22,378 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:19:22,379 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:19:22,379 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:19:22,379 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:19:22,379 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:19:22,379 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:19:22,379 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:19:22,379 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:19:22,379 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,379 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,379 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:19:22,379 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,379 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:19:22,379 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:19:22,379 [INFO ] epollEventLoopGroup-5-56 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,379 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:19:22,379 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,379 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:19:22,379 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:19:22,379 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,379 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:19:22,379 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,379 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:19:22,379 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:19:22,380 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:19:22,380 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,380 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:19:22,380 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:19:22,380 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:19:22,379 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,380 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:19:22,379 [INFO ] epollEventLoopGroup-5-56 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,380 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,380 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:19:22,379 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,380 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,380 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:19:22,380 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,380 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:19:22,380 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:19:22,380 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,380 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:19:22,380 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:19:22,380 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:19:22,380 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,380 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:19:22,380 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,380 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,380 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,380 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,380 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:19:22,380 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:19:22,380 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:19:22,380 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,380 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:19:22,380 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,380 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,380 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,380 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,380 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:19:22,380 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,380 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:19:22,380 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:19:22,380 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2025-06-05T04:19:22,380 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:19:22,380 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:19:22,381 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:19:22,381 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:19:22,380 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:19:22,381 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2025-06-05T04:19:22,381 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:19:22,381 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:19:22,380 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:19:22,381 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,381 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:19:22,381 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,381 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:19:22,381 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,381 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:19:22,381 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:19:22,381 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:19:22,382 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2025-06-05T04:19:22,382 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:19:22,382 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:19:22,383 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:19:22,383 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:19:22,383 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:19:22,383 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:19:22,383 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,383 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:19:22,383 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,383 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:19:22,383 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,383 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,383 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,383 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,383 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:19:22,383 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:19:22,383 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,383 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,383 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,383 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,383 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2025-06-05T04:19:22,383 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2025-06-05T04:19:22,392 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:19:22,392 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:19:22,392 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:19:22,392 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:19:22,393 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:19:22,393 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:19:22,394 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:19:22,394 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:19:22,394 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:19:22,394 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:19:22,395 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:19:22,395 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:19:22,599 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=46570
2025-06-05T04:19:22,600 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:19:22,604 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:19:22,604 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]46570
2025-06-05T04:19:22,604 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:19:22,604 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:19:22,604 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,604 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:19:22,604 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:19:22,604 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:19:22,605 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:19:22,605 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162605
2025-06-05T04:19:22,605 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097162605
2025-06-05T04:19:22,605 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162605
2025-06-05T04:19:22,605 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097162605
2025-06-05T04:19:22,606 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:19:22,625 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:19:22,626 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:19:22,698 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:19:22,698 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:19:22,711 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:19:22,712 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:19:22,712 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,712 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:19:22,712 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,712 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:19:22,712 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,712 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:19:22,713 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:19:22,713 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:19:22,713 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,713 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:19:22,713 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,713 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:19:22,713 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2025-06-05T04:19:22,713 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2025-06-05T04:19:22,721 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:19:22,721 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:19:22,721 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:19:22,721 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:19:45,872 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-06-05T04:19:45,872 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-06-05T04:20:16,379 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:16,379 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:16,986 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=47057
2025-06-05T04:20:16,986 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-06-05T04:20:16,990 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:20:16,990 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - [PID]47057
2025-06-05T04:20:16,990 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:20:16,990 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:20:16,990 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:16,990 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:16,990 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:20:16,990 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-06-05T04:20:16,991 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097216991
2025-06-05T04:20:16,991 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097216991
2025-06-05T04:20:16,991 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-06-05T04:20:16,992 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097216992
2025-06-05T04:20:16,992 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097216992
2025-06-05T04:20:16,993 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - model_name: divatma, batchSize: 1
2025-06-05T04:20:17,011 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:20:17,012 [WARN ] W-9000-divatma_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:20:17,088 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:20:17,088 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -   File "/tmp/models/0f974cb963224dbf9821bef7d8b5a7ee/onnx_handler.py", line 11, in initialize
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:20:17,100 [INFO ] W-9000-divatma_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:20:17,101 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:17,101 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:17,101 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:17,101 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:17,101 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:17,101 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:17,101 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:20:17,101 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: divatma, error: Worker died.
2025-06-05T04:20:17,101 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:17,101 [DEBUG] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-divatma_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:17,101 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:17,101 [WARN ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:17,101 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-06-05T04:20:17,101 [INFO ] W-9000-divatma_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-06-05T04:20:17,108 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:20:17,108 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:20:17,108 [INFO ] W-9000-divatma_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stderr
2025-06-05T04:20:17,108 [INFO ] W-9000-divatma_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-divatma_1.0-stdout
2025-06-05T04:20:17,387 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,388 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,387 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,388 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,388 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,388 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,390 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,390 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,719 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:17,719 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/root/miniconda3/envs/ts_yolo12/bin/python, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-06-05T04:20:29,367 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=47090
2025-06-05T04:20:29,367 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2025-06-05T04:20:29,373 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:20:29,374 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - [PID]47090
2025-06-05T04:20:29,374 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:20:29,374 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:29,374 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:29,374 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:20:29,374 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2025-06-05T04:20:29,374 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097229374
2025-06-05T04:20:29,374 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097229374
2025-06-05T04:20:29,375 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097229375
2025-06-05T04:20:29,375 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097229375
2025-06-05T04:20:29,376 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:20:29,376 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2025-06-05T04:20:29,376 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - model_name: vetlom, batchSize: 1
2025-06-05T04:20:29,388 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=47089
2025-06-05T04:20:29,389 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004
2025-06-05T04:20:29,393 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:20:29,393 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - [PID]47089
2025-06-05T04:20:29,393 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:20:29,393 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:20:29,393 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:29,393 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:29,393 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:20:29,393 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2025-06-05T04:20:29,394 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.
2025-06-05T04:20:29,394 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097229394
2025-06-05T04:20:29,394 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097229394
2025-06-05T04:20:29,394 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097229394
2025-06-05T04:20:29,394 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097229394
2025-06-05T04:20:29,395 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - model_name: bamdinh, batchSize: 1
2025-06-05T04:20:29,397 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:20:29,397 [WARN ] W-9001-vetlom_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:20:29,410 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:20:29,410 [WARN ] W-9004-bamdinh_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:20:29,413 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=47092
2025-06-05T04:20:29,413 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2025-06-05T04:20:29,414 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=47091
2025-06-05T04:20:29,414 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2025-06-05T04:20:29,417 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:20:29,417 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - [PID]47092
2025-06-05T04:20:29,417 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:20:29,417 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:20:29,417 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:29,417 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:29,417 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:20:29,417 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2025-06-05T04:20:29,418 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2025-06-05T04:20:29,418 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097229418
2025-06-05T04:20:29,418 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097229418
2025-06-05T04:20:29,418 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097229418
2025-06-05T04:20:29,418 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097229418
2025-06-05T04:20:29,418 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:20:29,418 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - [PID]47091
2025-06-05T04:20:29,419 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:20:29,419 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:20:29,419 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:29,419 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:29,419 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - model_name: xuoc, batchSize: 1
2025-06-05T04:20:29,419 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:20:29,419 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2025-06-05T04:20:29,420 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097229420
2025-06-05T04:20:29,420 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2025-06-05T04:20:29,420 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097229420
2025-06-05T04:20:29,420 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097229420
2025-06-05T04:20:29,420 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097229420
2025-06-05T04:20:29,421 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - model_name: nganmach, batchSize: 1
2025-06-05T04:20:29,435 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:20:29,435 [WARN ] W-9003-xuoc_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:20:29,437 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:20:29,437 [WARN ] W-9002-nganmach_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:20:29,490 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:20:29,490 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:20:29,490 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:20:29,490 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:20:29,490 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:20:29,490 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:20:29,490 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:20:29,490 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:20:29,502 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:20:29,502 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:20:29,502 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:20:29,502 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:29,502 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:29,502 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:29,502 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:29,502 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:20:29,502 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:20:29,502 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:29,502 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:29,502 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:29,502 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:20:29,502 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:20:29,502 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:20:29,502 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:20:29,503 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:20:29,503 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:20:29,502 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:29,502 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -   File "/tmp/models/63cbc4d224a44e8780b556e3364964bf/onnx_handler.py", line 11, in initialize
2025-06-05T04:20:29,503 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:29,503 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:20:29,502 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:29,503 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:29,503 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:29,503 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:20:29,503 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:20:29,503 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:20:29,503 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: bamdinh, error: Worker died.
2025-06-05T04:20:29,503 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:20:29,503 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:20:29,503 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:29,503 [DEBUG] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-bamdinh_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:29,503 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -   File "/tmp/models/7f7bf297ed6241de894ada107bbbdebb/onnx_handler.py", line 11, in initialize
2025-06-05T04:20:29,503 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:20:29,503 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:29,503 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:29,503 [WARN ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:29,503 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:29,503 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: xuoc, error: Worker died.
2025-06-05T04:20:29,503 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:20:29,503 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:20:29,503 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: vetlom, error: Worker died.
2025-06-05T04:20:29,503 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:29,503 [DEBUG] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-xuoc_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:29,503 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:29,503 [WARN ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2025-06-05T04:20:29,503 [INFO ] W-9004-bamdinh_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2025-06-05T04:20:29,503 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:20:29,503 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:20:29,503 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2025-06-05T04:20:29,503 [INFO ] W-9003-xuoc_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:20:29,503 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:20:29,503 [INFO ] W-9003-xuoc_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:20:29,503 [DEBUG] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-vetlom_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:29,504 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:20:29,504 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:20:29,504 [WARN ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:29,504 [INFO ] W-9003-xuoc_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stdout
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -   File "/tmp/models/56400c4e530e4bceb140aa8ea8b05223/onnx_handler.py", line 11, in initialize
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2025-06-05T04:20:29,504 [INFO ] W-9001-vetlom_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -   File "/tmp/models/fda126c85c5744a5a42639dec447b9c0/onnx_handler.py", line 11, in initialize
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:20:29,505 [INFO ] W-9002-nganmach_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:20:29,505 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:29,505 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:29,506 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:29,506 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:29,506 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:29,506 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:29,506 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:20:29,506 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: nganmach, error: Worker died.
2025-06-05T04:20:29,506 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:29,506 [DEBUG] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-nganmach_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:29,506 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:29,506 [WARN ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:29,506 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2025-06-05T04:20:29,506 [INFO ] W-9002-nganmach_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2025-06-05T04:20:29,512 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:20:29,512 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:20:29,512 [INFO ] W-9004-bamdinh_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stdout
2025-06-05T04:20:29,512 [INFO ] W-9004-bamdinh_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-bamdinh_1.0-stderr
2025-06-05T04:20:29,513 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:20:29,513 [INFO ] W-9003-xuoc_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-xuoc_1.0-stderr
2025-06-05T04:20:29,514 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:20:29,514 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:20:29,514 [INFO ] W-9001-vetlom_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stderr
2025-06-05T04:20:29,514 [INFO ] W-9001-vetlom_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-vetlom_1.0-stdout
2025-06-05T04:20:29,515 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:20:29,515 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:20:29,515 [INFO ] W-9002-nganmach_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stderr
2025-06-05T04:20:29,515 [INFO ] W-9002-nganmach_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-nganmach_1.0-stdout
2025-06-05T04:20:18,395 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=47211
2025-06-05T04:20:18,395 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005
2025-06-05T04:20:18,398 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Successfully loaded /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-06-05T04:20:18,399 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - [PID]47211
2025-06-05T04:20:18,399 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch worker started.
2025-06-05T04:20:18,399 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Python runtime: 3.10.18
2025-06-05T04:20:18,399 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:18,399 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-06-05T04:20:18,399 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:20:18,399 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2025-06-05T04:20:18,399 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.
2025-06-05T04:20:18,399 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097218399
2025-06-05T04:20:18,399 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1749097218399
2025-06-05T04:20:18,399 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097218399
2025-06-05T04:20:18,399 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1749097218399
2025-06-05T04:20:18,400 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - model_name: thieudong, batchSize: 1
2025-06-05T04:20:18,415 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG - /root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/torch_handler/base_handler.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
2025-06-05T04:20:18,415 [WARN ] W-9005-thieudong_1.0-stderr MODEL_LOG -   from pkg_resources import packaging
2025-06-05T04:20:18,475 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-06-05T04:20:18,475 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - ONNX enabled
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Backend worker process died.
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     worker.run_server()
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/model_loader.py", line 143, in load
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -   File "/tmp/models/b66c78abab22413fbc8dba1c106a08d1/onnx_handler.py", line 11, in initialize
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG -     self.session = ort.InferenceSession(ctx.model_uri, providers=providers)
2025-06-05T04:20:18,485 [INFO ] W-9005-thieudong_1.0-stdout MODEL_LOG - AttributeError: 'Context' object has no attribute 'model_uri'
2025-06-05T04:20:18,485 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:18,485 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2025-06-05T04:20:18,486 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:18,486 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-06-05T04:20:18,486 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:18,486 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1686) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2025-06-05T04:20:18,486 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:20:18,486 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: thieudong, error: Worker died.
2025-06-05T04:20:18,486 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:18,486 [DEBUG] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-thieudong_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-06-05T04:20:18,486 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:18,486 [WARN ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-06-05T04:20:18,486 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2025-06-05T04:20:18,486 [INFO ] W-9005-thieudong_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2025-06-05T04:20:18,493 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:20:18,493 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:20:18,493 [INFO ] W-9005-thieudong_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stderr
2025-06-05T04:20:18,493 [INFO ] W-9005-thieudong_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-thieudong_1.0-stdout
2025-06-05T04:20:45,878 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-06-05T04:20:45,878 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/root/miniconda3/envs/ts_yolo12/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

